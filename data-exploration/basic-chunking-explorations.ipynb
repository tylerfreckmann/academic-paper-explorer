{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('papers_embedded.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>573 \\n\\nBIT - SERIAL NEURAL  NETWORKS \\n\\nAlan...</td>\n",
       "      <td>7959.0</td>\n",
       "      <td>[0.007057549431920052, 0.022557897493243217, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 \\n\\nCONNECTIVITY VERSUS ENTROPY \\n\\nYaser  S...</td>\n",
       "      <td>5220.0</td>\n",
       "      <td>[0.017669761553406715, -0.02821267582476139, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>278 \\n\\nTHE HOPFIELD MODEL WITH MUL TI-LEVEL N...</td>\n",
       "      <td>4445.0</td>\n",
       "      <td>[0.019363459199666977, -0.004505184479057789, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>442 \\n\\nAlan  Lapedes \\nRobert  Farber \\n\\nThe...</td>\n",
       "      <td>7942.0</td>\n",
       "      <td>[0.006703744176775217, -0.002929536160081625, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>740 \\n\\nSPATIAL  ORGANIZATION  OF  NEURAL  NEn...</td>\n",
       "      <td>7980.0</td>\n",
       "      <td>[-0.019718386232852936, 0.021891098469495773, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  n_tokens  \\\n",
       "0  573 \\n\\nBIT - SERIAL NEURAL  NETWORKS \\n\\nAlan...    7959.0   \n",
       "1  1 \\n\\nCONNECTIVITY VERSUS ENTROPY \\n\\nYaser  S...    5220.0   \n",
       "2  278 \\n\\nTHE HOPFIELD MODEL WITH MUL TI-LEVEL N...    4445.0   \n",
       "3  442 \\n\\nAlan  Lapedes \\nRobert  Farber \\n\\nThe...    7942.0   \n",
       "4  740 \\n\\nSPATIAL  ORGANIZATION  OF  NEURAL  NEn...    7980.0   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.007057549431920052, 0.022557897493243217, -...  \n",
       "1  [0.017669761553406715, -0.02821267582476139, 0...  \n",
       "2  [0.019363459199666977, -0.004505184479057789, ...  \n",
       "3  [0.006703744176775217, -0.002929536160081625, ...  \n",
       "4  [-0.019718386232852936, 0.021891098469495773, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         0\n",
       "n_tokens     0\n",
       "embedding    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embedding'] = df['embedding'].apply(eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>573 \\n\\nBIT - SERIAL NEURAL  NETWORKS \\n\\nAlan...</td>\n",
       "      <td>7959.0</td>\n",
       "      <td>[0.007057549431920052, 0.022557897493243217, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 \\n\\nCONNECTIVITY VERSUS ENTROPY \\n\\nYaser  S...</td>\n",
       "      <td>5220.0</td>\n",
       "      <td>[0.017669761553406715, -0.02821267582476139, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>278 \\n\\nTHE HOPFIELD MODEL WITH MUL TI-LEVEL N...</td>\n",
       "      <td>4445.0</td>\n",
       "      <td>[0.019363459199666977, -0.004505184479057789, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>442 \\n\\nAlan  Lapedes \\nRobert  Farber \\n\\nThe...</td>\n",
       "      <td>7942.0</td>\n",
       "      <td>[0.006703744176775217, -0.002929536160081625, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>740 \\n\\nSPATIAL  ORGANIZATION  OF  NEURAL  NEn...</td>\n",
       "      <td>7980.0</td>\n",
       "      <td>[-0.019718386232852936, 0.021891098469495773, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  n_tokens  \\\n",
       "0  573 \\n\\nBIT - SERIAL NEURAL  NETWORKS \\n\\nAlan...    7959.0   \n",
       "1  1 \\n\\nCONNECTIVITY VERSUS ENTROPY \\n\\nYaser  S...    5220.0   \n",
       "2  278 \\n\\nTHE HOPFIELD MODEL WITH MUL TI-LEVEL N...    4445.0   \n",
       "3  442 \\n\\nAlan  Lapedes \\nRobert  Farber \\n\\nThe...    7942.0   \n",
       "4  740 \\n\\nSPATIAL  ORGANIZATION  OF  NEURAL  NEn...    7980.0   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.007057549431920052, 0.022557897493243217, -...  \n",
       "1  [0.017669761553406715, -0.02821267582476139, 0...  \n",
       "2  [0.019363459199666977, -0.004505184479057789, ...  \n",
       "3  [0.006703744176775217, -0.002929536160081625, ...  \n",
       "4  [-0.019718386232852936, 0.021891098469495773, ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text          object\n",
       "n_tokens     float64\n",
       "embedding     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_context(\n",
    "    question, max_len=8000\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a context for a question by finding the most similar context from the dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the embeddings for the question\n",
    "    q_embeddings = client.embeddings.create(input=question, model='text-embedding-3-small').data[0].embedding\n",
    "\n",
    "    # Get the distances from the embeddings\n",
    "    df['distances'] = df['embedding'].apply(lambda x: cosine_similarity(x, q_embeddings))\n",
    "\n",
    "\n",
    "    returns = []\n",
    "    cur_len = 0\n",
    "\n",
    "    # Sort by distance and add the text to the context until the context is too long\n",
    "    for _, row in df.sort_values('distances', ascending=False).iterrows():\n",
    "\n",
    "        # Add the length of the text to the current length\n",
    "        cur_len += row['n_tokens'] + 4\n",
    "\n",
    "        # If the context is too long, break\n",
    "        if cur_len > max_len:\n",
    "            break\n",
    "\n",
    "        # Else add it to the text that is being returned\n",
    "        returns.append(row[\"text\"])\n",
    "    \n",
    "    print(len(returns))\n",
    "\n",
    "    # Return the context\n",
    "    return \"\\n\\n###\\n\\n\".join(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "context = create_context(\"Can you summarize the main findings of 'CRoSS: Diffusion Model Makes Controllable, Robust and Secure Image Steganography'?\", max_len=64000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRoSS: Diffusion Model Makes\n",
      "Controllable, Robust and Secure Image Steganography\n",
      "Jiwen Yu1\n",
      "Xuanyu Zhang1\n",
      "Youmin Xu1,2\n",
      "Jian Zhang1†\n",
      "1 Peking University Shenzhen Graduate School\n",
      "2 Peng Cheng Laboratory\n",
      "Abstract\n",
      "Current image steganography techniques are mainly focused on cover-based meth-\n",
      "ods, which commonly have the risk of leaking secret images and poor robustness\n",
      "against degraded container images. Inspired by recent developments in diffu-\n",
      "sion models, we discovered that two properties of diffusion models, the ability to\n",
      "achieve translation between two images without training, and robustness to noisy\n",
      "data, can be used to improve security and natural robustness in image steganogra-\n",
      "phy tasks. For the choice of diffusion model, we selected Stable Diffusion, a type\n",
      "of conditional diffusion model, and fully utilized the latest tools from open-source\n",
      "communities, such as LoRAs and ControlNets, to improve the controllability and\n",
      "diversity of container images. In summary, we propose a novel image steganog-\n",
      "raphy framework, named Controllable, Robust and Secure Image Steganography\n",
      "(CRoSS), which has significant advantages in controllability, robustness, and secu-\n",
      "rity compared to cover-based image steganography methods. These benefits are\n",
      "obtained without additional training. To our knowledge, this is the first work to\n",
      "introduce diffusion models to the field of image steganography. In the experimental\n",
      "section, we conducted detailed experiments to demonstrate the advantages of our\n",
      "proposed CRoSS framework in controllability, robustness, and security. Code is\n",
      "available at https://github.com/vvictoryuki/CRoSS. 1\n",
      "Introduction\n",
      "With the explosive development of digital communication and AIGC (AI-generated content), the\n",
      "privacy, security, and protection of data have aroused significant concerns. As a widely studied\n",
      "technique, steganography [10] aims to hide messages like audio, image, and text into the container\n",
      "image in an undetected manner. In its reveal process, it is only possible for the receivers with pre-\n",
      "defined revealing operations to reconstruct secret information from the container image. It has a wide\n",
      "range of applications, such as copyright protection [4], digital watermarking [15], e-commerce [11],\n",
      "anti-visual detection [34], spoken language understanding [12, 13] and cloud computing [76]. For image steganography, traditional methods tend to transform the secret messages in the spatial\n",
      "or adaptive domains [27], such as fewer significant bits [9] or indistinguishable parts. With the\n",
      "development of deep neural networks, researchers begin to use auto-encoder networks [5, 6] or\n",
      "invertible neural networks (INN) [35, 26]to hide data, namely deep steganography. The essential targets of image steganography are security, reconstruction quality, and robustness [9,\n",
      "45, 77]. Since most previous methods use cover images to hide secret images, they tend to explicitly\n",
      "retain some secret information as artifacts or local details in the container image, which poses a risk\n",
      "of information leakage and reduces the security of transmission. Meanwhile, although previous\n",
      "works can maintain well reconstruction fidelity of the revealed images, they tend to train models in a\n",
      "noise-free simulation environment and can not withstand noise, compression artifacts, and non-linear\n",
      "transformations in practice, which severely hampers their practical values and robustness [30, 44, 25]. †Corresponding author. This work was supported by National Natural Science Foundation of China under\n",
      "Grant 62372016. 37th Conference on Neural Information Processing Systems (NeurIPS 2023). To address security and robustness concerns, researchers have shifted their focus toward coverless\n",
      "steganography. This approach aims to create a container image that bears no relation to the secret\n",
      "information, thereby enhancing its security. Current coverless steganography methods frequently\n",
      "employ frameworks such as CycleGAN [78] and encoder-decoder models [76], leveraging the\n",
      "principle of cycle consistency. However, the controllability of the container images generated by\n",
      "existing coverless methods remains limited. Their container images are only randomly sampled from\n",
      "the generative model and cannot be determined by the user. Moreover, existing approaches [47] tend\n",
      "to only involve hiding bits into container images, ignoring the more complex hiding of secret images. Overall, current methods, whether cover-based or coverless, have not been able to achieve good unity\n",
      "in terms of security, controllability, and robustness. Thus, our focus is to propose a new framework\n",
      "that can simultaneously improve existing methods in these three aspects. Recently, research on diffusion-based generative models [22, 55, 56] has been very popular, with\n",
      "various unique properties such as the ability to perform many tasks in a zero-shot manner [36, 28, 64,\n",
      "63, 72, 37, 20], strong control over the generation process [16, 49, 74, 40, 19, 50], natural robustness\n",
      "to noise in images [64, 28, 14, 65], and the ability to achieve image-to-image translation [75, 8, 20,\n",
      "39, 57, 14, 29, 37]. We were pleasantly surprised to find that these properties perfectly match the\n",
      "goals we mentioned above for image steganography: (1) Security: By utilizing the DDIM Inversion\n",
      "technique [54] for diffusion-based image translation, we ensure the invertibility of the translation\n",
      "process. This invertible translation process enables a coverless steganography framework, ensuring\n",
      "the security of the hidden image. (2) Controllability: The powerful control capabilities of conditional\n",
      "diffusion models make the container image highly controllable, and its visual quality is guaranteed\n",
      "by the generative prior of the diffusion model; (3) Robustness: Diffusion models are essentially\n",
      "Gaussian denoisers and have natural robustness to noise and perturbations. Even if the container\n",
      "image is degraded during transmission, we can still reveal the main content of the secret image. We believe that the fusion of diffusion models and image steganography is not simply a matter of\n",
      "mechanically combining them, but rather an elegant and instructive integration that takes into account\n",
      "the real concerns of image steganography. Based on these ideas, we propose the Controllable, Robust\n",
      "and Secure Image Steganography (CRoSS) framework, a new image steganography framework that\n",
      "aims to simultaneously achieve gains in security, controllability, and robustness. Our contributions can be summarized as follows:\n",
      "• We identify the limitations of existing image steganography methods and propose a unified goal of\n",
      "achieving security, controllability, and robustness. We also demonstrate that the diffusion model\n",
      "can seamlessly integrate with image steganography to achieve these goals using diffusion-based\n",
      "invertible image translation technique without requiring any additional training. • We propose a new image steganography framework: Controllable, Robust and Secure Image\n",
      "Steganography (CRoSS). To the best of our knowledge, this is the first attempt to apply the\n",
      "diffusion model to the field of image steganography and gain better performance. • We leveraged the progress of the rapidly growing Stable Diffusion community to propose variants\n",
      "of CRoSS using prompts, LoRAs, and ControlNets, enhancing its controllability and diversity. • We conducted comprehensive experiments focusing on the three targets of security, controllability,\n",
      "and robustness, demonstrating the advantages of CRoSS compared to existing methods. 2\n",
      "Related Work\n",
      "2.1\n",
      "Steganography Methods\n",
      "Cover-based Image Steganography. Unlike cryptography, steganography aims to hide secret\n",
      "data in a host to produce an information container. For image steganography, a cover image is\n",
      "required to hide the secret image in it [5]. Traditionally, spatial-based [24, 41, 43, 46] methods\n",
      "utilize the Least Significant Bits (LSB), pixel value differencing (PVD) [43], histogram shifting [60],\n",
      "multiple bit-planes [41] and palettes [24, 42] to hide images, which may arise statistical suspicion\n",
      "and are vulnerable to steganalysis methods. Adaptive methods [45, 31] decompose the steganography\n",
      "into embedding distortion minimization and data coding, which is indistinguishable by appearance\n",
      "but limited in capacity. Various transform-based schemes [10, 27] including JSteg [46] and DCT\n",
      "steganography [21] also fail to offer high payload capacity. Recently, various deep learning-based\n",
      "2\n",
      "\fschemes have been proposed to solve image steganography. Baluja [5] proposed the first deep-\n",
      "learning method to hide a full-size image into another image. Generative adversarial networks\n",
      "(GANs) [53] are introduced to synthesize container images. Probability map methods focus on\n",
      "generating various cost functions satisfying minimal-distortion embedding [45, 59]. [69] proposes\n",
      "a generator based on U-Net. [58] presents an adversarial scheme under distortion minimization. Three-player game methods like SteganoGAN [73] and HiDDeN [77] learn information embedding\n",
      "and recovery by auto-encoder architecture to adversarially resist steganalysis. Recent attempts [66] to\n",
      "introduce invertible neural networks (INN) into low-level inverse problems like denoising, rescaling,\n",
      "and colorization show impressive potential over auto-encoder, GAN [3], and other learning-based\n",
      "architectures. Recently, [35, 26] proposed designing the steganography model as an invertible neural\n",
      "network (INN) [17, 18] to perform image hiding and recovering with a single INN model. Coverless Steganography. Coverless steganography is an emerging technique in the field of\n",
      "information hiding, which aims to embed secret information within a medium without modifying the\n",
      "cover object [47]. Unlike traditional steganography methods that require a cover medium (e.g., an\n",
      "image or audio file) to be altered for hiding information, coverless steganography seeks to achieve\n",
      "secure communication without introducing any changes to the cover object [33]. This makes it more\n",
      "challenging for adversaries to detect the presence of hidden data, as there are no observable changes\n",
      "in the medium’s properties [38]. To the best of our knowledge, existing coverless steganography\n",
      "methods [34] still focus on hiding bits into container images, and few explorations involve hiding\n",
      "images without resorting to cover images. 2.2\n",
      "Diffusion Models\n",
      "Diffusion models [22, 55, 56] are a type of generative model that is trained to learn the target image\n",
      "distribution from a noise distribution. Recently, due to their powerful generative capabilities, diffusion\n",
      "models have been widely used in various image applications, including image generation [16, 48,\n",
      "51, 49], restoration [52, 28, 64], translation [14, 29, 37, 75], and more. Large-scale diffusion model\n",
      "communities have also emerged on the Internet, with the aim of promoting the development of\n",
      "AIGC(AI-generated content)-related fields by applying the latest advanced techniques. In these communities, the Stable Diffusion [49] community is currently one of the most popular and\n",
      "thriving ones, with a large number of open-source tools available for free, including model checkpoints\n",
      "finetuned on various specialized datasets. Additionally, various LoRAs [23] and ControlNets [74] are\n",
      "available in these communities for efficient control over the results generated by Stable Diffusion. LoRAs achieve control by efficiently modifying some network parameters in a low-rank way, while\n",
      "ControlNets introduce an additional network to modify the intermediate features of Stable Diffusion\n",
      "for control. These mentioned recent developments have enhanced our CRoSS framework. 3\n",
      "Method\n",
      "3.1\n",
      "Definition of Image Steganography\n",
      "Hide Process\n",
      "Reveal Process\n",
      "Transmission\n",
      "Figure 1: Illustration used to show the\n",
      "definition of image steganography. Before introducing our specific method, we first define the\n",
      "image steganography task as consisting of three images\n",
      "and two processes (as shown in Fig. 1): the three images\n",
      "refer to the secret image xsec, container image xcont, and\n",
      "revealed image xrev, while the two processes are the hide\n",
      "process and reveal process. The secret image xsec is the\n",
      "image we want to hide and is hidden in the container\n",
      "image xcont through the hide process. After transmission\n",
      "over the Internet, the container image xcont may become\n",
      "degraded, resulting in a degraded container image x′\n",
      "cont,\n",
      "from which we extract the revealed image xrev through\n",
      "the reveal process. Our goal is to make our proposed framework have the following properties: (1)\n",
      "Security: even if the container image xcont is intercepted by other receivers, the hidden secret image\n",
      "xsec cannot be leaked. (2) Controllability: the content in the container image xcont can be controlled\n",
      "by the user, and its visual quality is high. (3) Robustness: the reveal process can still generate\n",
      "semantically consistent results (xrev ≈xsec) even if there is deviation in the x′\n",
      "cont compared to the\n",
      "xcont (x′\n",
      "cont = d(xcont), d(·) denotes the degradation process). According to the above definition,\n",
      "3\n",
      "\fDiffusion\n",
      "Forward\n",
      "Translation\n",
      "（b）DDIM Inversion\n",
      "（a）Image Translation using Different Conditions\n",
      "Diffusion\n",
      "Backward\n",
      "Invertible?\n",
      "prompt：“cat”\n",
      "prompt：“tiger”\n",
      "Deterministic\n",
      "Forward\n",
      "Deterministic\n",
      "Backward\n",
      "Figure 2: In part (a), Conditional diffusion models can be used with different conditions to perform\n",
      "image translation. In this example, we use two different prompts (“cat\" and “tiger\") to translate a cat\n",
      "image into a tiger image. However, a critical challenge for coverless image steganography is whether\n",
      "we can reveal the original image from the translated image. The answer is yes, and we can use DDIM\n",
      "Inversion (shown in part (b)) to achieve dual-direction translation between the image distribution and\n",
      "noise distribution, allowing for invertible image translation. we can consider the hide process as a translation between the secret image xsec and the container\n",
      "image xcont, and the reveal process as the inverse process of the hide process. In Sec. 3.2, we will\n",
      "introduce how to use diffusion models to implement these ideas, and in Sec. 3.3, we will provide a\n",
      "detailed description of our proposed framework CRoSS for coverless image steganography. 3.2\n",
      "Invertible Image Translation using Diffusion Model\n",
      "Diffusion Model Defined by DDIM. A complete diffusion model process consists of two stages:\n",
      "the forward phase adds noise to a clean image, while the backward sampling phase denoises it step\n",
      "by step. In DDIM [54], the formula for the forward process is given by:\n",
      "xt = √αtxt−1 +\n",
      "√\n",
      "1 −αtϵ,\n",
      "ϵ ∼N(0, I),\n",
      "(1)\n",
      "where xt denotes the noisy image in the t-th step, ϵ denotes the randomly sampled Gaussian noise,\n",
      "αt is a predefined parameter and the range of time step t is [1, T]. The formula of DDIM for the\n",
      "backward sampling process is given by:\n",
      "xs = √¯αsfθ(xt, t) +\n",
      "p\n",
      "1 −¯αs −σ2sϵθ(xt, t) + σsϵ,\n",
      "fθ(xt, t) = xt −√1 −¯αtϵθ(xt, t)\n",
      "√¯αt\n",
      ", (2)\n",
      "where ϵ ∼N(0, I) is a randomly sampled Gaussian noise with σ2\n",
      "s as the noise variance, fθ(·, t) is a\n",
      "denoising function based on the pre-trained noise estimator ϵθ(·, t), and ¯αt = Qt\n",
      "i=1 αi. DDIM does\n",
      "not require the two steps in its sampling formula to be adjacent (i.e., t = s + 1). Therefore, s and\n",
      "t can be any two steps that satisfy s < t. This makes DDIM a popular algorithm for accelerating\n",
      "sampling. Furthermore, if σs in Eq.2 is set to 0, the DDIM sampling process becomes deterministic. In this case, the sampling result is solely determined by the initial value xT , which can be considered\n",
      "as a latent code. The sampling process can also be equivalently described as solving an Ordinary\n",
      "Differential Equation (ODE) using an ODE solver [54]. In our work, we choose deterministic DDIM\n",
      "to implement the diffusion model and use the following formula:\n",
      "x0 = ODESolve(xT ; ϵθ, T, 0)\n",
      "(3)\n",
      "to represent the process of sampling from xT to x0 using a pretrained noise estimator ϵθ. Image Translation using Diffusion Model. A large number of image translation methods [75, 8,\n",
      "20, 39, 57, 14, 29, 37] based on diffusion models have been proposed. In our method, we will adopt\n",
      "a simple approach. First, we assume that the diffusion models used in our work are all conditional\n",
      "diffusion models that support condition c as input to control the generated results. Taking the example\n",
      "shown in Fig. 2 (a), suppose we want to transform an image of a cat into an image of a tiger. We add\n",
      "noise to the cat image using the forward process (Eq. 1) to obtain the intermediate noise, and then\n",
      "control the backward sampling process (Eq. 2) from noise by inputting a condition (prompt=“tiger”),\n",
      "resulting in a new tiger image. In general, if the sampling condition is set to c, our conditional\n",
      "sampling process can be expressed based on Eq. 3 as follows:\n",
      "x0 = ODESolve(xT ; ϵθ, c, T, 0). (4)\n",
      "For image translation, there are two properties that need to be considered: the structural consistency\n",
      "of the two images before and after the translation, and whether the translation process is invertible. 4\n",
      "\fHide Process\n",
      "Reveal Process\n",
      "Private Key\n",
      "Public Key\n",
      "Private Key\n",
      "Public Key\n",
      "Social Media\n",
      "Internet\n",
      "Transmission\n",
      "Figure 3: Our coverless image steganography framework CRoSS. The diffusion model we choose is\n",
      "a conditional diffusion model, which supports conditional inputs to control the generation results. We\n",
      "choose the deterministic DDIM as the sampling strategy and use the two different conditions (kpri\n",
      "and kpub) given to the model as the private key and the public key. Algorithm 1 The Hide Process of CRoSS. Input: The secret image xsec which will be hidden, a pre-trained conditional diffusion model with\n",
      "noise estimator ϵθ, the number T of time steps for sampling and two different conditions kpri and\n",
      "kpub which serve as the private and public keys. Output: The container image xcont used to hide the secret image xsec. xnoise = ODESolve(xsec; ϵθ, kpri, 0, T)\n",
      "xcont = ODESolve(xnoise; ϵθ, kpub, T, 0)\n",
      "return xcont\n",
      "Structural consistency is crucial for most applications related to image translation, but for coverless\n",
      "image steganography, ensuring the invertibility of the translation process is the more important goal. To achieve invertible image translation, we utilize DDIM Inversion based on deterministic DDIM. DDIM Inversion Makes an Invertible Image Translation. DDIM Inversion (shown in Fig. 2\n",
      "(b)), as the name implies, refers to the process of using DDIM to achieve the conversion from an\n",
      "image to a latent noise and back to the original image. The idea is based on the approximation of\n",
      "forward and backward differentials in solving ordinary differential equations [54, 29]. Intuitively, in\n",
      "the case of deterministic DDIM, it allows s and t in Eq. 2 to be any two steps (i.e., allowing s < t\n",
      "and s > t). When s < t, Eq. 2 performs the backward process, and when s > t, Eq. 2 performs the\n",
      "forward process. As the trajectories of forward and backward processes are similar, the input and\n",
      "output images are very close, and the intermediate noise xT can be considered as the latent variable\n",
      "of the inversion. In our work, we use the following formulas:\n",
      "xT = ODESolve(x0; ϵθ, c, 0, T),\n",
      "x′\n",
      "0 = ODESolve(xT ; ϵθ, c, T, 0),\n",
      "(5)\n",
      "to represent the DDIM Inversion process from the original image x0 to the latent code xT and from\n",
      "the latent code xT back to the original image x0 (the output image is denoted as x′\n",
      "0 and x′\n",
      "0 ≈x0). Based on DDIM Inversion, we have achieved the invertible relationship between images and latent\n",
      "noises. As long as we use deterministic DDIM to construct the image translation framework, the\n",
      "entire framework can achieve invertibility with two DDIM Inversion loops. It is the basis of our\n",
      "coverless image steganography framework, which will be described in detail in the next subsection. 3.3\n",
      "The Coverless Image Steganography Framework CRoSS\n",
      "Our basic framework CRoSS is based on a conditional diffusion model, whose noise estimator is\n",
      "represented by ϵθ, and two different conditions that serve as inputs to the diffusion model. In our\n",
      "work, these two conditions can serve as the private key and public key (denoted as kpri and kpub), as\n",
      "shown in Fig.3, with detailed workflow described in Algo.1 and Algo. 2. We will introduce the entire\n",
      "CRoSS framework in two parts: the hide process and the reveal process. The Process of Hide Stage. In the hide stage, we attempt to perform translation between the\n",
      "secret image xsec and the container image xcont using the forward and backward processes of\n",
      "deterministic DDIM. In order to make the images before and after the translation different, we use\n",
      "the pre-trained conditional diffusion model with different conditions in the forward and backward\n",
      "processes respectively. These two different conditions also serve as private and public keys in the\n",
      "CRoSS framework. Specifically, the private key kpri is used for the forward process, while the\n",
      "5\n",
      "\fAlgorithm 2 The Reveal Process of CRoSS. Input: The container image x′\n",
      "cont that has been transmitted over the Internet (may be degraded\n",
      "from xcont), the pre-trained conditional diffusion model with noise estimator ϵθ, the number T of\n",
      "time steps for sampling, the private key kpri and public key kpub. Output: The revealed image xrev. x′\n",
      "noise = ODESolve(x′\n",
      "cont; ϵθ, kpub, 0, T)\n",
      "xrev = ODESolve(x′\n",
      "noise; ϵθ, kpri, T, 0)\n",
      "return xrev\n",
      "Reveal Backward\n",
      "Reveal Forward\n",
      "Social Media\n",
      "Internet\n",
      "Transmission\n",
      "Candidate Revealed Images\n",
      "Scenario#2\n",
      "Guessed Public Key\n",
      "Scenario#1\n",
      "Possible Private Keys\n",
      "“What is the secret \n",
      "image behind the \n",
      "container image? I \n",
      "just see a tiger in the \n",
      "container image. Maybe we can take \n",
      "this word as public key \n",
      "to try.”\n",
      "Receiver\n",
      "Receiver\n",
      "“I guess the private key \n",
      "may be something related \n",
      "to animals, like a cat, lion, \n",
      "leopard, ... . I try to use \n",
      "these words as the private \n",
      "keys to generate some \n",
      "condidates, but I do not \n",
      "know which one is true:(”\n",
      "Receiver\n",
      "“I got this from social media and there \n",
      "are no evidences for me to judge if this \n",
      "image is a container. The visual quality of \n",
      "this image is high and I can not find any \n",
      "difference between this image and other \n",
      "natural images.”\n",
      "Scenario#3\n",
      "Security for Avoiding Detection\n",
      "Transmitted Container Image\n",
      "“cat”\n",
      "“lion”\n",
      "“leopard”\n",
      "Figure 4: Further explanation of the CRoSS framework. We simulated the possible problems that a\n",
      "receiver may encounter in three different scenarios during the reveal process. public key kpub is used for the backward process. After getting the container image xcont, it will be\n",
      "transmitted over the Internet and publicly accessible to all potential receivers. The Roles of the Private and Public Keys in Our CRoSS Framework. In CRoSS, we found\n",
      "that these given conditions can act as keys in practical use. The private key is used to describe\n",
      "the content in the secret image, while the public key is used to control the content in the container\n",
      "image. For the public key, it is associated with the content in the container image, so even if it is\n",
      "not manually transmitted over the network, the receiver can guess it based on the received container\n",
      "image (described in Scenario#2 of Fig. 4). For the private key, it determines whether the receiver can\n",
      "successfully reveal the original image, so it cannot be transmitted over public channels. The Process of Reveal Stage. In the reveal stage, assuming that the container image has been\n",
      "transmitted over the Internet and may have been damaged as x′\n",
      "cont, the receiver needs to reveal it\n",
      "back to the secret image through the same forward and backward process using the same conditional\n",
      "diffusion model with corresponding keys. Throughout the entire coverless image steganography\n",
      "process, we do not train or fine-tune the diffusion models specifically for image steganography tasks\n",
      "but rely on the inherent invertible image translation guaranteed by the DDIM Inversion. The Security Guaranteed by CRoSS. Some questions about security may be raised, such as:\n",
      "What if the private key is guessed by the receivers? Does the container image imply the possible\n",
      "hidden secret image? We clarify these questions from two aspects: (1) Since the revealed image is\n",
      "generated by the diffusion model, the visual quality of the revealed image is relatively high regardless\n",
      "of whether the input private key is correct or not. The receiver may guess the private key by exhaustive\n",
      "method, but it is impossible to judge which revealed image is the true secret image from a pile of\n",
      "candidate revealed images (described in Scenario#1 of Fig. 4). (2) Since the container image is\n",
      "also generated by the diffusion model, its visual quality is guaranteed by the generative prior of the\n",
      "diffusion model. Moreover, unlike cover-based methods that explicitly store clues in the container\n",
      "image, the container image in CRoSS does not contain any clues that can be detected or used to extract\n",
      "the secret image. Therefore, it is hard for the receiver to discover that the container image hides other\n",
      "images or to reveal the secret image using some detection method (described in Scenario#3 of Fig. 4). Various Variants for Public and Private Keys. Our proposed CRoSS relies on pre-trained con-\n",
      "ditional diffusion models with different conditions kpub, kpri and these conditions serve as keys in\n",
      "the CRoSS framework. In practical applications, we can distinguish different types of conditions of\n",
      "diffusion models in various ways. Here are some examples: (1) Prompts: using the same checkpoint\n",
      "6\n",
      "\fFigure 5: Deep steganalysis results by the latest SID [61]. As the number of leaked samples increases,\n",
      "methods whose detection accuracy curves grow more slowly and approach 50% exhibit higher\n",
      "security. The right is the recall curve of different methods under the StegExpose [7] detector. The\n",
      "closer the area enclosed by the curve and the coordinate axis is to 0.5, the closer the method is to the\n",
      "ideal evasion of the detector. of text-to-image diffusion models like Stable Diffusion [49] but different prompts as input condi-\n",
      "tions; (2) LoRAs [23]: using the same checkpoint initialization, but loading different LoRAs; (3)\n",
      "ControlNets [74]: loading the same checkpoint but using ControlNet with different conditions. 4\n",
      "Experiment\n",
      "4.1\n",
      "Implementation Details\n",
      "Experimental Settings. In our experiment, we chose Stable Diffusion [49] v1.5 as the conditional\n",
      "diffusion model, and we used the deterministic DDIM [54] sampling algorithm. Both the forward\n",
      "and backward processes consisted of 50 steps. To achieve invertible image translation, we set the\n",
      "guidance scale of Stable Diffusion to 1. For the given conditions, which serve as the private and\n",
      "public keys, we had three options: prompts, conditions for ControlNets [74] (depth maps, scribbles,\n",
      "segmentation maps), and LoRAs [23]. All experiments were conducted on a GeForce RTX 3090\n",
      "GPU card, and our method did not require any additional training or fine-tuning for the diffusion\n",
      "model. The methods we compared include RIIS [68], HiNet [26], Baluja [6], and ISN [35]. Data Preparation. To perform a quantitative and qualitative analysis of our method, we collect a\n",
      "benchmark with a total of 260 images and generate corresponding prompt keys specifically tailored\n",
      "for the coverless image steganography, dubbed Stego260. We categorize the dataset into three classes,\n",
      "namely humans, animals, and general objects (such as architecture, plants, food, furniture, etc.). The\n",
      "images in the dataset are sourced from publicly available datasets [1, 2] and Google search engines. For generating prompt keys, we utilize BLIP [32] to generate private keys and employ ChatGPT or\n",
      "artificial adjustment to perform semantic modifications and produce public keys in batches. More\n",
      "details about the dataset can be found in the supplementary material. 4.2\n",
      "Property Study#1: Security\n",
      "Methods\n",
      "NIQE ↓\n",
      "|Detection Accuracy - 50| ↓\n",
      "XuNet [67]\n",
      "YedroudjNet [70]\n",
      "KeNet [71]\n",
      "Baluja [6]\n",
      "3.43±0.08\n",
      "45.18±1.69\n",
      "43.12±2.18\n",
      "46.88±2.37\n",
      "ISN [35]\n",
      "2.87±0.02\n",
      "5.14±0.44\n",
      "3.01±0.29\n",
      "8.62±1.19\n",
      "HiNet [26]\n",
      "2.94±0.02\n",
      "5.29±0.44\n",
      "3.12±0.36\n",
      "8.33±1.22\n",
      "RIIS [68]\n",
      "3.13±0.05\n",
      "0.73±0.13\n",
      "0.24±0.08\n",
      "4.88±1.15\n",
      "CRoSS (ours)\n",
      "3.04\n",
      "1.32\n",
      "0.18\n",
      "2.11\n",
      "Table 1: Security analysis. NIQE indicates the vi-\n",
      "sual quality of container images, lower is better. The closer the detection rate of a method approxi-\n",
      "mates 50%, the more secure the method is consid-\n",
      "ered, as it suggests its output is indistinguishable\n",
      "from random chance. The best results are red and\n",
      "the second-best results are blue. In Fig. 5, the recent learning-based steganalysis\n",
      "method Size-Independent-Detector (SID) [61]\n",
      "is retrained with leaked samples from testing\n",
      "results of various methods on Stego260. The\n",
      "detection accuracy of CRoSS increases more\n",
      "gradually as the number of leaked samples rises,\n",
      "compared to other methods. The recall curves\n",
      "on the right also reveal the lower detection ac-\n",
      "curacy of our CRoSS, indicating superior anti-\n",
      "steganalysis performance. Our security encompasses two aspects: imper-\n",
      "ceptibility in visual quality against human suspi-\n",
      "cion and resilience against steganalysis attacks. NIQE is a no-reference image quality assessment (IQA) model to measure the naturalness and visual\n",
      "7\n",
      "\fPrompt1: a young cute chinese girl with long hair\n",
      "Prompt2: a baby with long hair\n",
      "Prompt1: a koala is sitting on a branch \n",
      "Prompt2: a baby raccoon cub is walking on a branch\n",
      "Prompt1: a grey chair in the living room \n",
      "Prompt2: a grey sofa in the living room\n",
      "Prompt1: Sydney Opera House \n",
      "Prompt2: a boat\n",
      "Secret\n",
      "Container\n",
      "Revealed\n",
      "Prompt1: leaning tower of pisa\n",
      "Prompt2: a lighthouse with a red flag\n",
      "Prompt1: a baby badger cub is walking through the grass\n",
      "Prompt2: a baby raccoon cub is walking through the grass\n",
      "Prompt1: a young girl with blond hair\n",
      "Prompt2: a wrinkled elderly woman with white hair\n",
      "Prompt1: a multi-layer hamburger\n",
      "Prompt2: a three-layer sandwich\n",
      "Secret\n",
      "Container\n",
      "Revealed\n",
      "Prompt1: a cute rabbit\n",
      "Prompt2: a cute cat\n",
      "Prompt1: Eiffel Tower under the blue sky\n",
      "Prompt2: a tree under the blue sky\n",
      "Prompt1: Obama is giving a speech\n",
      "Prompt2: Biden is giving a speech\n",
      "Prompt1: tomatoes hanging on tree\n",
      "Prompt2: green apples hanging on tree\n",
      "Secret\n",
      "Container\n",
      "Revealed\n",
      "Figure 6: Visual results of the proposed CRoSS controlled by different prompts. The container\n",
      "images are realistic and the revealed images have well semantic consistency with the secret images. Secret\n",
      "Container\n",
      "Revealed\n",
      "Secret\n",
      "Container\n",
      "Revealed\n",
      "Secret\n",
      "Container\n",
      "Revealed\n",
      "ControlNets\n",
      "LoRAs\n",
      "Figure 7: Visual results of our CRoSS controlled by different ControlNets and LoRAs. Depth maps,\n",
      "scribbles, and segmentation maps are presented in the lower right corner of the images. security without any reference image or human feedback. In Tab. 1, the lower the NIQE score, the\n",
      "less likely it is for the human eye to identify the image as a potentially generated container for hiding\n",
      "secret information. Our NIQE is close to those of other methods, as well as the original input image\n",
      "(2.85), making it difficult to discern with human suspicion. Anti-analysis security is evaluated by\n",
      "three steganalysis models XuNet[67], YedroudjNet[70], and KeNet[71], for which lower detection\n",
      "accuracy denotes higher security. Our CRoSS demonstrates the highest or near-highest resistance\n",
      "against various steganalysis methods. 4.3\n",
      "Property Study#2: Controllability\n",
      "To verify the controllability and flexibility of the proposed CRoSS, various types of private and public\n",
      "keys such as prompts, ControlNets, and LoRAs † are incorporated in our framework. As illustrated in\n",
      "Fig. 6, our framework is capable of effectively hiding the secret images in the container images based\n",
      "on the user-provided “Prompt2” without noticeable artifacts or unrealistic image details. The container\n",
      "image allows for the seamless modification of a person’s identity information, facial attributes, as\n",
      "†The last row of Fig. 7 are generated via LoRAs downloaded from https://civitai.com/. 8\n",
      "\fRevealed image\n",
      "Secret image\n",
      "Clean\n",
      "Shoot\n",
      "WeChat\n",
      "Clean\n",
      "Shoot\n",
      "WeChat\n",
      "Clean\n",
      "Shoot\n",
      "WeChat\n",
      "CRoSS\n",
      "HiNet\n",
      "RIIS\n",
      "Figure 8: Visual comparisons of our CRoSS and other methods [68, 26] under two real-world\n",
      "degradations, namely “WeChat” and “Shoot”. Obviously, our method can reconstruct the content of\n",
      "secret images, while other methods exhibit significant color distortion or have completely failed. Methods\n",
      "clean\n",
      "Gaussian noise\n",
      "Gaussian denoiser [62]\n",
      "JPEG compression\n",
      "JPEG enhancer [62]\n",
      "σ = 10\n",
      "σ = 20\n",
      "σ = 30\n",
      "σ = 10\n",
      "σ = 20\n",
      "σ = 30\n",
      "Q = 20\n",
      "Q = 40\n",
      "Q = 80\n",
      "Q = 20\n",
      "Q = 40\n",
      "Q = 80\n",
      "Baluja [6]\n",
      "34.24\n",
      "10.30\n",
      "7.54\n",
      "6.92\n",
      "7.97\n",
      "6.10\n",
      "5.49\n",
      "6.59\n",
      "8.33\n",
      "11.92\n",
      "5.21\n",
      "6.98\n",
      "9.88\n",
      "ISN [35]\n",
      "41.83\n",
      "12.75\n",
      "10.98\n",
      "9.93\n",
      "11.94\n",
      "9.44\n",
      "6.65\n",
      "7.15\n",
      "9.69\n",
      "13.44\n",
      "5.88\n",
      "8.08\n",
      "11.63\n",
      "HiNet [26]\n",
      "42.98\n",
      "12.91\n",
      "11.54\n",
      "10.23\n",
      "11.87\n",
      "9.32\n",
      "6.87\n",
      "7.03\n",
      "9.78\n",
      "13.23\n",
      "5.59\n",
      "8.21\n",
      "11.88\n",
      "RIIS [68]\n",
      "43.78\n",
      "26.03\n",
      "18.89\n",
      "15.85\n",
      "20.89\n",
      "15.97\n",
      "13.92\n",
      "22.03\n",
      "25.41\n",
      "27.02\n",
      "13.88\n",
      "16.74\n",
      "20.13\n",
      "CRoSS (ours)\n",
      "23.79\n",
      "21.89\n",
      "20.19\n",
      "18.77\n",
      "21.39\n",
      "21.24\n",
      "21.02\n",
      "21.74\n",
      "22.74\n",
      "23.51\n",
      "20.60\n",
      "21.22\n",
      "21.19\n",
      "Table 2: PSNR(dB) results of the proposed CRoSS and other methods under different levels of\n",
      "degradations. The proposed CRoSS can achieve superior data fidelity in most settings.\n",
      "\n",
      "###\n",
      "\n",
      "Hiding Images in Plain Sight:\n",
      "\n",
      "Deep Steganography\n",
      "\n",
      "Shumeet Baluja\n",
      "Google Research\n",
      "\n",
      "Google, Inc. shumeet@google.com\n",
      "\n",
      "Abstract\n",
      "\n",
      "Steganography is the practice of concealing a secret message within another,\n",
      "ordinary, message. Commonly, steganography is used to unobtrusively hide a small\n",
      "message within the noisy regions of a larger image. In this study, we attempt\n",
      "to place a full size color image within another image of the same size. Deep\n",
      "neural networks are simultaneously trained to create the hiding and revealing\n",
      "processes and are designed to speciﬁcally work as a pair. The system is trained on\n",
      "images drawn randomly from the ImageNet database, and works well on natural\n",
      "images from a wide variety of sources. Beyond demonstrating the successful\n",
      "application of deep learning to hiding images, we carefully examine how the result\n",
      "is achieved and explore extensions. Unlike many popular steganographic methods\n",
      "that encode the secret message within the least signiﬁcant bits of the carrier image,\n",
      "our approach compresses and distributes the secret image’s representation across\n",
      "all of the available bits. 1\n",
      "\n",
      "Introduction to Steganography\n",
      "\n",
      "Steganography is the art of covered or hidden writing; the term itself dates back to the 15th century,\n",
      "when messages were physically hidden. In modern steganography, the goal is to covertly communicate\n",
      "a digital message. The steganographic process places a hidden message in a transport medium, called\n",
      "the carrier. The carrier may be publicly visible. For added security, the hidden message can also be\n",
      "encrypted, thereby increasing the perceived randomness and decreasing the likelihood of content\n",
      "discovery even if the existence of the message detected. Good introductions to steganography and\n",
      "steganalysis (the process of discovering hidden messages) can be found in [1–5]. There are many well publicized nefarious applications of steganographic information hiding, such as\n",
      "planning and coordinating criminal activities through hidden messages in images posted on public\n",
      "sites – making the communication and the recipient difﬁcult to discover [6]. Beyond the multitude of\n",
      "misuses, however, a common use case for steganographic methods is to embed authorship information,\n",
      "through digital watermarks, without compromising the integrity of the content or image. The challenge of good steganography arises because embedding a message can alter the appearance\n",
      "and underlying statistics of the carrier. The amount of alteration depends on two factors: ﬁrst, the\n",
      "amount of information that is to be hidden. A common use has been to hide textual messages in\n",
      "images. The amount of information that is hidden is measured in bits-per-pixel (bpp). Often, the\n",
      "amount of information is set to 0.4bpp or lower. The longer the message, the larger the bpp, and\n",
      "therefore the more the carrier is altered [6, 7]. Second, the amount of alteration depends on the carrier\n",
      "image itself. Hiding information in the noisy, high-frequency ﬁlled, regions of an image yields less\n",
      "humanly detectable perturbations than hiding in the ﬂat regions. Work on estimating how much\n",
      "information a carrier image can hide can be found in [8]. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. Figure 1: The three components of the full system. Left: Secret-Image preparation. Center: Hiding\n",
      "the image in the cover image. Right: Uncovering the hidden image with the reveal network; this is\n",
      "trained simultaneously, but is used by the receiver. The most common steganography approaches manipulate the least signiﬁcant bits (LSB) of images to\n",
      "place the secret information - whether done uniformly or adaptively, through simple replacement or\n",
      "through more advanced schemes [9, 10]. Though often not visually observable, statistical analysis\n",
      "of image and audio ﬁles can reveal whether the resultant ﬁles deviate from those that are unaltered. Advanced methods attempt to preserve the image statistics, by creating and matching models of the\n",
      "ﬁrst and second order statistics of the set of possible cover images explicitly; one of the most popular\n",
      "is named HUGO [11]. HUGO is commonly employed with relatively small messages (< 0.5bpp). In contrast to the previous studies, we use a neural network to implicitly model the distribution of\n",
      "natural images as well as embed a much larger message, a full-size image, into a carrier image. Despite recent impressive results achieved by incorporating deep neural networks with steganaly-\n",
      "sis [12–14], there have been relatively few attempts to incorporate neural networks into the hiding\n",
      "process itself [15–19]. Some of these studies have used deep neural networks (DNNs) to select which\n",
      "LSBs to replace in an image with the binary representation of a text message. Others have used\n",
      "DNNs to determine which bits to extract from the container images. In contrast, in our work, the\n",
      "neural network determines where to place the secret information and how to encode it efﬁciently;\n",
      "the hidden message is dispersed throughout the bits in the image. A decoder network, that has been\n",
      "simultaneously trained with the encoder, is used to reveal the secret image. Note that the networks\n",
      "are trained only once and are independent of the cover and secret images. In this paper, the goal is to visually hide a full N × N × RGB pixel secret image in another\n",
      "N × N × RGB cover image, with minimal distortion to the cover image (each color channel is 8\n",
      "bits). However, unlike previous studies, in which a hidden text message must be sent with perfect\n",
      "reconstruction, we relax the requirement that the secret image is losslessly received. Instead, we\n",
      "are willing to ﬁnd acceptable trade-offs in the quality of the carrier and secret image (this will be\n",
      "described in the next section). We also provide brief discussions of the discoverability of the existence\n",
      "of the secret message. Previous studies have demonstrated that hidden message bit rates as low as\n",
      "0.1bpp can be discovered; our bit rates are 10× - 40× higher. Though visually hard to detect, given\n",
      "the large amount of hidden information, we do not expect the existence of a secret message to be\n",
      "hidden from statistical analysis. Nonetheless, we will show that commonly used methods do not ﬁnd\n",
      "it, and we give promising directions on how to trade-off the difﬁculty of existence-discovery with\n",
      "reconstruction quality, as required. 2 Architectures and Error Propagation\n",
      "\n",
      "Though steganography is often conﬂated with cryptography, in our approach, the closest analogue\n",
      "is image compression through auto-encoding networks. The trained system must learn to compress\n",
      "the information from the secret image into the least noticeable portions of the cover image. The\n",
      "architecture of the proposed system is shown in Figure 1. The three components shown in Figure 1 are trained as a single network; however, it is easiest to\n",
      "describe them individually. The leftmost, Prep-Network, prepares the secret image to be hidden. This\n",
      "component serves two purposes. First, in cases in which the secret-image (size M × M) is smaller\n",
      "than the cover image (N × N), the preparation network progressively increases the size of the secret\n",
      "image to the size of the cover, thereby distributing the secret image’s bits across the entire N × N\n",
      "\n",
      "2\n",
      "\n",
      "\fFigure 2: Transformations made by the preparation network (3 examples shown). Left: Original\n",
      "Color Images. Middle: the three channels of information extracted by the preparation network that\n",
      "are input into the middle network. Right: zoom of the edge-detectors. The three color channels are\n",
      "transformed by the preparation-network. In the most easily recognizable example, the 2nd channel\n",
      "activates for high frequency regions, e.g. textures and edges (shown enlarged (right)). pixels. (For space reasons, we do not provide details of experiments with smaller images, and instead\n",
      "concentrate on full size images). The more important purpose, relevant to all sizes of hidden images,\n",
      "is to transform the color-based pixels to more useful features for succinctly encoding the image –\n",
      "such as edges [20, 21], as shown in Figure 2. The second/main network, the Hiding Network, takes as input the output of the preparation-network\n",
      "and the cover image, and creates the Container image. The input to this network is a N × N pixel\n",
      "ﬁeld, with depth concatenated RGB channels of the cover image and the transformed channels of\n",
      "the secret image. Over 30 architectures for this network were attempted for our study with varying\n",
      "number of hidden layers and convolution sizes; the best consisted of 5 convolution layers that had 50\n",
      "ﬁlters each of {3 × 3, 4 × 4, 5 × 5} patches. Finally, the right-most network, the Reveal Network,\n",
      "is used by the receiver of the image; it is the decoder. It receives only the Container image (not the\n",
      "cover nor secret image). The decoder network removes the cover image to reveal the secret image. As mentioned earlier, our approach borrows heavily from auto-encoding networks [22]; however,\n",
      "instead of simply encoding a single image through a bottleneck, we encode two images such that the\n",
      "intermediate representation (the container image) appears as similar as possible to the cover image. The system is trained by reducing the error shown below (c and s are the cover and secret images\n",
      "respectively, and β is how to weigh their reconstruction errors):\n",
      "\n",
      "L(c, c(cid:48), s, s(cid:48)) = ||c − c(cid:48)|| + β||s − s(cid:48)||\n",
      "\n",
      "(1)\n",
      "\n",
      "It is important to note where the errors are computed and the weights that each error affects, see\n",
      "Figure 3. In particular, note that the error term ||c − c(cid:48)|| does not apply to the weights of the\n",
      "reveal-network that receives the container image and extracts the secret image. On the other hand,\n",
      "all of the networks receive the error signal β||s − s(cid:48)|| for reconstructing the hidden image. This\n",
      "ensures that the representations formed early in the preparation network as well as those used for\n",
      "reconstruction of the cover image also encode information about the secret image. Figure 3: The three networks are trained as a single, large, network. Error term 1 affects only the ﬁrst\n",
      "two networks. Error term 2 affects all 3. S is the secret image, C is the cover image. 3\n",
      "\n",
      "\fTo ensure that the networks do not simply encode the secret image in the LSBs, a small amount of\n",
      "noise is added to the output of the second network (e.g. into the generated container image) during\n",
      "training. The noise was designed such that the LSB was occasionally ﬂipped; this ensured that the\n",
      "LSB was not the sole container of the secret image’s reconstruction. Later, we will discuss where the\n",
      "secret image’s information is placed. Next, we examine how the network performs in practice. 3 Empirical Evaluation\n",
      "\n",
      "The three networks were trained as described above using Adam [23]. For simplicity, the reconstruc-\n",
      "tions minimized the sum of squares error of the pixel difference, although other image metrics could\n",
      "have easily been substituted [24, 25]. The networks were trained using randomly selected pairs of\n",
      "images from the ImageNet training set [26]. Quantitative results are shown in Figure 4, as measured by the SSE per pixel, per channel. The\n",
      "testing was conducted on 1,000 image pairs taken from ImageNet images (not used in training). For\n",
      "comparison, also shown is the result of using the same network for only encoding the cover image\n",
      "without the secret image (e.g. β = 0). This gives the best reconstruction error of the cover using\n",
      "this network (this is unattainable while also encoding the secret image). Also shown in Figure 4 are\n",
      "histograms of errors for the cover and reconstruction. As can be seen, there are few large pixel errors. Deep-Stego\n",
      "Deep-Stego\n",
      "Deep-Stego\n",
      "Cover Only\n",
      "\n",
      "β\n",
      "\n",
      "0.75\n",
      "1.00\n",
      "1.25\n",
      "0.00\n",
      "\n",
      "Cover\n",
      "\n",
      "Secret\n",
      "\n",
      "2.8\n",
      "3.0\n",
      "6.4\n",
      "0.1\n",
      "\n",
      "3.6\n",
      "3.2\n",
      "2.8\n",
      "(n/a)\n",
      "\n",
      "Figure 4: Left: Number of intensity values off (out of 256) for each pixel, per channel, on cover and\n",
      "secret image. Right: Distribution of pixel errors for cover and secret images, respectively. Figure 5 shows the results of hiding six images, chosen to show varying error rates. These images are\n",
      "not taken from ImageNet to demonstrate that the networks have not over-trained to characteristics of\n",
      "the ImageNet database, and work on a range of pictures taken with cell phone cameras and DSLRs. Note that most of the reconstructed cover images look almost identical to the original cover images,\n",
      "despite encoding all the information to reconstruct the secret image. The differences between the\n",
      "original and cover images are shown in the rightmost columns (magniﬁed 5× in intensity). Consider how these error rates compare to creating the container through simple LSB substitution:\n",
      "replacing the 4 least signiﬁcant bits (LSB) of the cover image with the 4 most-signiﬁcant 4-bits\n",
      "(MSB) of the secret image. In this procedure, to recreate the secret image, the MSBs are copied\n",
      "from the container image, and the remaining bits set to their average value across the training dataset. Doing this, the average pixel error per channel on the cover image’s reconstruction is 5.4 (in a range\n",
      "of 0-255). The average error on the reconstruction of the secret image (when using the average value\n",
      "for the missing LSB bits) is approximately 4.0.1 Why is the error for the cover image’s reconstruction\n",
      "larger than 4.0? The higher error for the cover image’s reconstruction reﬂects the fact that the\n",
      "distribution of bits in the natural images used are different for the MSBs and LSBs; therefore, even\n",
      "though the secret and cover image are drawn from the same distribution, when the MSB from the\n",
      "secret image are used in the place of the LSB, larger errors occur than simply using the average values\n",
      "of the LSBs. Most importantly, these error rates are signiﬁcantly higher than those achieved by our\n",
      "system (Figure 4). 1Note that an error of 4.0 is expected when the average value is used to ﬁll in the LSB: removing 4 bits\n",
      "from a pixel’s encoding yields 16x fewer intensities that can be represented. By selecting the average value to\n",
      "replace the missing bits, the maximum error can be 8, and the average error is 4, assuming uniformly distributed\n",
      "bits. To avoid any confusion, we point out that though it is tempting to consider using the average value for the\n",
      "cover image also, recall that the LSBs of the cover image are where the MSBs of the secret image are stored. Therefore, those bits must be used in this encoding scheme, and hence the larger error. 4\n",
      "\n",
      "\fOriginal\n",
      "cover\n",
      "\n",
      "secret\n",
      "\n",
      "Reconstructed\n",
      "cover\n",
      "\n",
      "secret\n",
      "\n",
      "Differences ×5\n",
      "cover\n",
      "\n",
      "secret\n",
      "\n",
      "Figure 5: 6 Hiding Results. Left pair of each set: original cover and secret image. Center pair: cover\n",
      "image embedded with the secret image, and the secret image after extraction from the container. Right pair: Residual errors for cover and hidden – enhanced 5×. The errors per pixel, per channel are\n",
      "the smallest in the top row: (3.1, 4.5) , and largest in the last (4.5, 7.9). We close this section with a demonstration of the limitation of our approach. Recall that the networks\n",
      "were trained on natural images found in the ImageNet challenge. Though this covers a very large\n",
      "range of images, it is illuminating to examine the effects when other types of images are used. Five\n",
      "such images are shown in Figure 6. In the ﬁrst row, a pure white image is used as the cover, to\n",
      "examine the visual effects of hiding a colorful secret image. This simple case was not encountered\n",
      "in training with ImageNet images. The second and third rows change the secret image to bright\n",
      "pink circles and uniform noise. As can be seen, even though the container image (4th column)\n",
      "contains only limited noise, the recovered secret image is extremely noisy. In the ﬁnal two rows, the\n",
      "cover image is changed to circles, and uniform noise, respectively. As expected, the errors for the\n",
      "reconstruction of the cover and secret are now large, though the secret image remains recognizable. 3.1 What if the original cover image became accessible?\n",
      "\n",
      "For many steganographic applications, it can safely be assumed that access to the original cover\n",
      "image (without the secret image embedded) is impossible for an attacker. However, what if the\n",
      "original cover image was discovered? What could then be ascertained about the secret image, even\n",
      "without access to the decoding network? In Figure 5, we showed the difference image between the\n",
      "original cover and the container with 5x enhancement – almost nothing was visible. We reexamine\n",
      "\n",
      "5\n",
      "\n",
      "\fFigure 6: Results with images outside the set of natural images. the residual image at 5x, 10x, and 20x enhancement (with clipping at 255 where appropriate), see\n",
      "Figure 7. In the ﬁrst row, note that the residual (at 20x) strongly resembles the cover image. In the\n",
      "second row, the residual is a combination of the cover and secret image, and in the third row, we see\n",
      "the most troubling result – features of the secret image are revealed. (Recall that this happens only\n",
      "when the original, unperturbed image, is available for comparison). There are many standard methods\n",
      "for obfuscation, such as adding cryptographic encodings of the secret image before embedding it\n",
      "into the cover image. We demonstrate another avenue that can be used in conjunction with any other\n",
      "approach: modifying the network’s error function. In addition to the two error terms described, we add an error term that minimizes the pixel-wise\n",
      "correlation between the residual of the cover image and the secret image corr(Rc, S) where Rc =\n",
      "||C − C(cid:48)|| and S is the secret image. Many weightings for this term were empirically tested. In the\n",
      "results shown in Figure 7(Bottom), it is scaled to approximately (0.15 * number of pixel * channels). Minimizing the residual’s correlation with the secret image removed many of the secret image’s\n",
      "features from the residuals – even when 20x magniﬁcation is used. Naturally, the robustness and\n",
      "resilience comes at a price; the quality of some of the reconstructions have decreased, as shown (see\n",
      "saturation of reds in ﬁrst image). 4 Where is the Secret Image Encoded?\n",
      "\n",
      "The primary focus of this paper is to concretely demonstrate that it is possible to encode a large\n",
      "amount of information in an image with limited visually noticeable artifacts. However, no explicit\n",
      "attempt has been made to actively hide the existence of that information from machine detection. Though we cannot expect to completely hide the fact that up to 1/2 of the information is part of a\n",
      "hidden message, measures can be taken to make it more difﬁcult to discover. First, however, we must\n",
      "determine where the information of the secret image resides. Is the network simply hiding the information about the secret image in the least signiﬁcant bits of the\n",
      "cover image? Tools exist to seek out hidden information in the LSBs. One such publicly available\n",
      "steganalysis toolkit, StegExpose, was used to test the detectability of our hidden images [27–29]. Per the description of the tool: “StegExpose rating algorithm is derived from an intelligent and\n",
      "thoroughly tested combination of pre-existing pixel based steganalysis methods including Sample\n",
      "Pairs by Dumitrescu (2003), RS Analysis by Fridrich (2001), Chi Square Attack by Westfeld (2000)\n",
      "and Primary Sets by Dumitrescu (2002)” [27]. In addition to the default settings (threshold = 0.2),\n",
      "the detection thresholds were varied throughout a large range. The ROC curve for StegExpose is\n",
      "shown in Figure 8. Note the little variation beyond random guessing (the green line). StegExpose should have been able to ﬁnd the information if it were simply placed in the LSB bits. We turn to a second method to ﬁnd where the information is stored. The images used in the study\n",
      "\n",
      "6\n",
      "\n",
      "\fFigure 7: Top 3 rows. If the original image is leaked and is subtracted from the container image, the\n",
      "residual can be computed. With enough enhancement (20x), some of the secret image is revealed. Bottom 3 rows: by explicitly creating an error term that minimized the correlation between the\n",
      "residual and the secret image, the residual reveals less about the secret image; however, the pixel\n",
      "errors for the container rise (note the less saturated colors in some of the red regions). Figure 8: ROC curves: True Positive Rate vs. False Positive\n",
      "Rate for StegExpose when trying to detect images embedded\n",
      "via the proposed method. are composed, at each pixel, of 24 bits (8 × (R, G, B)). If we ﬂip the ﬁrst bit of the R channel\n",
      "of all the pixels in the container image, we can measure its effects on the reconstructions on the\n",
      "container image itself and also, by propagating the modiﬁed image through reveal network, on the\n",
      "reconstruction of the secret image. The effects are striking, see Figure 9. In Figure 9, the left half of the ﬁgure should be considered the “control”. The upper left bar-chart,\n",
      "shows that the effects of changing a bit in the Red channel of the container only has an effect on the\n",
      "red-channel in the container, and that the magnitude is proportional to the signiﬁcance of the bit; this\n",
      "is exactly as expected and holds true for the Green and Blue channels as well (shown on the diagonal). Much more interesting is the right side of Figure 9. Here, we see that a bit ﬂip in any bit position\n",
      "in any color channel of the container image, has an effect across all color channels in the secret\n",
      "\n",
      "7\n",
      "\n",
      "\fFigure 9: Sensitivity to Bits in the Container Image. (Left:) Control case, the container image. Changing a bit in the container image obviously has no effect on other bits. (Right): However, that\n",
      "same bit ﬂip in the container image has effects across all color bands in the recovered secret image. For example, if we want to measure the effect of ﬂipping the 2nd bit in the Blue channel, we would\n",
      "look at the marked entries (marked with *). image’s reconstruction. Moreover, the error does not follow the standard signiﬁcance ordering of the\n",
      "bit positions. In other words, the information for the secret image is spread across the color channels;\n",
      "hence the reason it was not detected earlier. Despite StegExpose not ﬁnding the encodings, we suspected that this amount of information should\n",
      "be readily discoverable. It has recently been demonstrated in multiple studies that deep networks\n",
      "are competitive and often better than the more established methods of steganalysis that use hand\n",
      "selected image features [12–14]. Interestingly, many published steganalysis techniques are not blind:\n",
      "they assume that they are detecting hidden images only from known hiding methods and even that\n",
      "access to the statistics about the cover image distribution are available. This makes the steganalysis\n",
      "task easier while making the hiding task much more difﬁcult. We will not discuss the real-world\n",
      "limitations of that assumption here, but will follow that example. We trained the steganalysis networks as binary classiﬁers, using the unperturbed ImageNet images\n",
      "as negative samples, and our containers as positive examples. We discovered that the steganalysis\n",
      "networks performed well with 24 × (N × N ) binary input layers. Each of the 24 layers represented a\n",
      "bit in the pixel representation of the input image. The networks were trained in an analogous manner\n",
      "to others in this study; again a multitude of architectures and learning rates were attempted. The fully convolutional networks were able to obtain between 90-95% combined classiﬁcation rates. For comparison, the same networks were retrained to ﬁnd Least-Signiﬁcant-Bit substitution (where\n",
      "the L least signiﬁcant bits of each color channel in the cover image are replaced with the L most\n",
      "signiﬁcant bits from the secret image). The networks were trained and tested with L = 1, 2, 3, 4; all\n",
      "achieved over 99% combined rates. An additional set of tests revealed that even when a single bit\n",
      "was randomly placed in one of the 4 bit positions, the detection rate remained over 99%. Given these\n",
      "detection rates on even a single bit, the high detection rates with a full image are not surprising. Nonetheless, this leads to an immediate direction for future work. In this study, the steganalysis\n",
      "networks were trained and used after the complete encoding system was created. However, using\n",
      "pre-trained and/or simultaneously trained steganalysis networks in an adversarial learning framework,\n",
      "such as Generative-Adversarial-Networks (GAN) [30], during the training of steganography nets\n",
      "provides a method to incorporate an obfuscation based error metric. The adversary provides a\n",
      "supplemental error signal based upon the discoverability of the hidden message that is minimized in\n",
      "addition to the reconstruction errors. [31] has recently successfully explored a very similar procedure\n",
      "with small messages hidden in images. 5 Discussion & Future Work\n",
      "\n",
      "In this section, we brieﬂy discuss a few observations found in this study and present ideas for future\n",
      "work. First, lets consider the possibility of training a network to recover the hidden images after the\n",
      "system has been deployed and without access to the original network. One can imagine that if an\n",
      "\n",
      "8\n",
      "\n",
      "\fattacker was able to obtain numerous instances of container images that were created by the targeted\n",
      "system, and in each instance if at least one of the two component images (cover or secret image) was\n",
      "also given, a network could be trained to recover both constituent components. What can an attacker\n",
      "do without having access to this ground-truth “training” data? Using a smoothness constraint or other\n",
      "common heuristic from more classic image decomposition and blind source separation [32–34] may\n",
      "be a ﬁrst alternative. With many of these approaches, obtaining even a modest amount of training\n",
      "data would be useful in tuning and setting parameters and priors. If such an attack is expected, it is\n",
      "open to further research how much adapting the techniques described in Section 3.1 may mitigate the\n",
      "effectiveness of these attempts. As described in the previous section, in its current form, the correct detection of the existence (not\n",
      "necessarily the exact content) of a hidden image is indeed possible. The discovery rate is high because\n",
      "of the amount of information hidden compared to the cover image’s data (1:1 ratio). This is far\n",
      "more than state-of-the-art systems that transmit reliably undetected messages. We presented one of\n",
      "many methods to make it more difﬁcult to recover the contents of the hidden image by explicitly\n",
      "reducing the similarity of the cover image’s residual to the hidden image. Though beyond the scope\n",
      "of this paper, we can make the system substantially more resilient by supplementing the presented\n",
      "mechanisms as follows. Before hiding the secret image, the pixels are permuted (in-place) in one\n",
      "of M previously agreed upon ways. The permuted-secret-image is then hidden by the system, as is\n",
      "the key (an index into M). This makes recovery difﬁcult even by looking at the residuals (assuming\n",
      "access to the original image is available) since the residuals have no spatial structure. The use of\n",
      "this approach must be balanced with (1) the need to send a permutation key (though this can be sent\n",
      "reliably in only a few bytes), and (2) the fact that the permuted-secret-image is substantially more\n",
      "difﬁcult to encode; thereby potentially increasing the reconstruction-errors throughout the system. Finally, it should be noted that in order to employ this approach, the trained networks in this study\n",
      "cannot be used without retraining. The entire system must be retrained as the hiding networks can no\n",
      "longer exploit local structure in the secret image for encoding information. This study opens a new avenue for exploration with steganography and, more generally, in placing\n",
      "supplementary information in images. Several previous methods have attempted to use neural net-\n",
      "works to either augment or replace a small portion of an image-hiding system. We have demonstrated\n",
      "a method to create a fully trainable system that provides visually excellent results in unobtrusively\n",
      "placing a full-size, color image into another image. Although the system has been described in the\n",
      "context of images, the same system can be trained for embedding text, different-sized images, or\n",
      "audio. Additionally, by using spectrograms of audio-ﬁles as images, the techniques described here\n",
      "can readily be used on audio samples. There are many immediate and long-term avenues for expanding this work. Three of the most\n",
      "immediate are listed here. (1) To make a complete steganographic system, hiding the existence of the\n",
      "message from statistical analyzers should be addressed. This will likely necessitate a new objective in\n",
      "training (e.g. an adversary), as well as, perhaps, encoding smaller images within large cover images. (2) The proposed embeddings described in this paper are not intended for use with lossy image ﬁles. If lossy encodings, such as jpeg, are required, then working directly with the DCT coefﬁcients instead\n",
      "of the spatial domain is possible [35]. (3) For simplicity, we used a straightforward SSE error metric\n",
      "for training the networks; however, error metrics more closely associated with human vision, such as\n",
      "SSIM [24], can be easily substituted. References\n",
      "[1] Gary C Kessler and Chet Hosmer. An overview of steganography. Advances in Computers, 83(1):51–107,\n",
      "\n",
      "2011. [2] Gary C Kessler. An overview of steganography for the computer forensics examiner. Forensic Science\n",
      "\n",
      "Communications, 6(3), 2014. [3] Gary C Kessler. An overview of steganography for the computer forensics examiner (web), 2015. [4] Jussi\n",
      "\n",
      "Parikka. image. https://unthinking.photography/themes/fauxtography/hidden-in-plain-sight-the-steganographic-image,\n",
      "2017. stagnographic\n",
      "\n",
      "Hidden\n",
      "\n",
      "in\n",
      "\n",
      "plain\n",
      "\n",
      "sight:\n",
      "\n",
      "The\n",
      "\n",
      "[5] Jessica Fridrich, Jan Kodovsk`y, Vojtˇech Holub, and Miroslav Goljan. Breaking hugo–the process discovery. In International Workshop on Information Hiding, pages 85–101. Springer, 2011. 9\n",
      "\n",
      "\f[6] Jessica Fridrich and Miroslav Goljan. Practical steganalysis of digital images: State of the art. In Electronic\n",
      "\n",
      "Imaging 2002, pages 1–13. International Society for Optics and Photonics, 2002. [7] Hamza Ozer, Ismail Avcibas, Bulent Sankur, and Nasir D Memon. Steganalysis of audio based on audio\n",
      "quality metrics. In Electronic Imaging 2003, pages 55–66. International Society for Optics and Photonics,\n",
      "2003. [8] Farzin Yaghmaee and Mansour Jamzad. Estimating watermarking capacity in gray scale images based on\n",
      "\n",
      "image complexity. EURASIP Journal on Advances in Signal Processing, 2010(1):851920, 2010. [9] Jessica Fridrich, Miroslav Goljan, and Rui Du. Detecting lsb steganography in color, and gray-scale images. IEEE multimedia, 8(4):22–28, 2001. [10] Abdelfatah A Tamimi, Ayman M Abdalla, and Omaima Al-Allaf. Hiding an image inside another image\n",
      "using variable-rate steganography. International Journal of Advanced Computer Science and Applications\n",
      "(IJACSA), 4(10), 2013. [11] Tomáš Pevn`y, Tomáš Filler, and Patrick Bas. Using high-dimensional image models to perform highly\n",
      "undetectable steganography. In International Workshop on Information Hiding, pages 161–177. Springer,\n",
      "2010. [12] Yinlong Qian, Jing Dong, Wei Wang, and Tieniu Tan. Deep learning for steganalysis via convolutional\n",
      "neural networks. In SPIE/IS&T Electronic Imaging, pages 94090J–94090J. International Society for Optics\n",
      "and Photonics, 2015. [13] Lionel Pibre, Jérôme Pasquet, Dino Ienco, and Marc Chaumont. Deep learning is a good steganalysis tool\n",
      "when embedding key is reused for different images, even if there is a cover source mismatch. Electronic\n",
      "Imaging, 2016(8):1–11, 2016. [14] Lionel Pibre, Pasquet Jérôme, Dino Ienco, and Marc Chaumont. Deep learning for steganalysis is better\n",
      "than a rich model with an ensemble classiﬁer, and is natively robust to the cover source-mismatch. arXiv\n",
      "preprint arXiv:1511.04855, 2015. [15] Sabah Husien and Haitham Badi. Artiﬁcial neural network for steganography. Neural Computing and\n",
      "\n",
      "Applications, 26(1):111–116, 2015. [16] Imran Khan, Bhupendra Verma, Vijay K Chaudhari, and Ilyas Khan. Neural network based steganography\n",
      "algorithm for still images. In Emerging Trends in Robotics and Communication Technologies (INTERACT),\n",
      "2010 International Conference on, pages 46–51. IEEE, 2010. [17] V Kavitha and KS Easwarakumar. Neural based steganography. PRICAI 2004: Trends in Artiﬁcial\n",
      "\n",
      "Intelligence, pages 429–435, 2004. [18] Alexandre Santos Brandao and David Calhau Jorge. Artiﬁcial neural networks applied to image steganog-\n",
      "\n",
      "raphy. IEEE Latin America Transactions, 14(3):1361–1366, 2016. [19] Robert Jarušek, Eva Volna, and Martin Kotyrba. Neural network approach to image steganography\n",
      "\n",
      "techniques. In Mendel 2015, pages 317–327. Springer, 2015. [20] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked\n",
      "denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11(Dec):3371–3408, 2010. [21] Anthony J Bell and Terrence J Sejnowski. The “independent components” of natural scenes are edge ﬁlters. Vision research, 37(23):3327–3338, 1997. [22] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507, 2006. [23] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. [24] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error\n",
      "\n",
      "visibility to structural similarity. IEEE transactions on image processing, 13(4):600–612, 2004. [25] Andrew B Watson. Dct quantization matrices visually optimized for individual images. In proc. SPIE,\n",
      "\n",
      "1993. [26] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\n",
      "Andrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Fei-Fei Li. Imagenet large\n",
      "scale visual recognition challenge. CoRR, abs/1409.0575, 2014. 10\n",
      "\n",
      "\f[27] Benedikt Boehm. Stegexpose - A tool for detecting LSB steganography. CoRR, abs/1410.6656, 2014. [28] Stegexpose - github. https://github.com/b3dk7/StegExpose. [29] darknet.org.uk. tool\n",
      "\n",
      "for detecting steganography in images.\n",
      "\n",
      "###\n",
      "\n",
      "Generating steganographic images via adversarial\n",
      "\n",
      "training\n",
      "\n",
      "Jamie Hayes\n",
      "\n",
      "University College London\n",
      "j.hayes@cs.ucl.ac.uk\n",
      "\n",
      "George Danezis\n",
      "\n",
      "University College London\n",
      "The Alan Turing Institute\n",
      "g.danezis@ucl.ac.uk\n",
      "\n",
      "Abstract\n",
      "\n",
      "Adversarial training has proved to be competitive against supervised learning\n",
      "methods on computer vision tasks. However, studies have mainly been conﬁned\n",
      "to generative tasks such as image synthesis. In this paper, we apply adversarial\n",
      "training techniques to the discriminative task of learning a steganographic algo-\n",
      "rithm. Steganography is a collection of techniques for concealing the existence\n",
      "of information by embedding it within a non-secret medium, such as cover texts\n",
      "or images. We show that adversarial training can produce robust steganographic\n",
      "techniques: our unsupervised training scheme produces a steganographic algorithm\n",
      "that competes with state-of-the-art steganographic techniques. We also show that\n",
      "supervised training of our adversarial model produces a robust steganalyzer, which\n",
      "performs the discriminative task of deciding if an image contains secret information.\n",
      "We deﬁne a game between three parties, Alice, Bob and Eve, in order to simulta-\n",
      "neously train both a steganographic algorithm and a steganalyzer. Alice and Bob\n",
      "attempt to communicate a secret message contained within an image, while Eve\n",
      "eavesdrops on their conversation and attempts to determine if secret information is\n",
      "embedded within the image. We represent Alice, Bob and Eve by neural networks,\n",
      "and validate our scheme on two independent image datasets, showing our novel\n",
      "method of studying steganographic problems is surprisingly competitive against\n",
      "established steganographic techniques.\n",
      "\n",
      "1\n",
      "\n",
      "Introduction\n",
      "\n",
      "Steganography and cryptography both provide methods for secret communication. Authenticity\n",
      "and integrity of communications are central aims of modern cryptography. However, traditional\n",
      "cryptographic schemes do not aim to hide the presence of secret communications. Steganography\n",
      "conceals the presence of a message by embedding it within a communication the adversary does\n",
      "not deem suspicious. Recent details of mass surveillance programs have shown that meta-data of\n",
      "communications can lead to devastating privacy leakages1. NSA ofﬁcials have stated that they “kill\n",
      "people based on meta-data” [8]; the mere presence of a secret communication can have life or death\n",
      "consequences even if the content is not known. Concealing both the content as well as the presence\n",
      "of a message is necessary for privacy sensitive communication.\n",
      "Steganographic algorithms are designed to hide information within a cover message such that the\n",
      "cover message appears unaltered to an external adversary. A great deal of effort is afforded to\n",
      "designing steganographic algorithms that minimize the perturbations within a cover message when\n",
      "a secret message is embedded within, while allowing for recovery of the secret message. In this\n",
      "work we ask if a steganographic algorithm can be learned in an unsupervised manner, without\n",
      "\n",
      "1See EFF’s\n",
      "\n",
      "guide:\n",
      "\n",
      "disproportionate.pdf.\n",
      "\n",
      "https://www.eff.org/files/2014/05/29/unnecessary_and_\n",
      "\n",
      "\fhuman domain knowledge. Note that steganography only aims to hide the presence of a message.\n",
      "Thus, it is nearly always the case that the message is encrypted prior to embedding using a standard\n",
      "cryptographic scheme; the embedded message is therefore indistinguishable from a random string.\n",
      "The receiver of the steganographic image will then decode to reveal the ciphertext of the message and\n",
      "then decrypt using an established shared key.\n",
      "For the unsupervised design of steganographic techniques, we leverage ideas from the ﬁeld of\n",
      "adversarial training [7]. Typically, adversarial training is used to train generative models on tasks\n",
      "such as image generation and speech synthesis. We design a scheme that aims to embed a secret\n",
      "message within an image. Our task is discriminative, the embedding algorithm takes in a cover image\n",
      "and produces a steganographic image, while the adversary tries to learn weaknesses in the embedding\n",
      "algorithm, resulting in the ability to distinguish cover images from steganographic images.\n",
      "The success of a steganographic algorithm or a steganalysis technique over one another amounts\n",
      "to ability to model the cover distribution correctly [5]. So far, steganographic schemes have used\n",
      "human-based rules to ‘learn’ this distribution and perturb it in a way that disrupts it least. However,\n",
      "steganalysis techniques commonly use machine learning models to learn the differences in distribu-\n",
      "tions between the cover and steganographic images. Based on this insight we pursue the following\n",
      "hypothesis:\n",
      "Hypothesis: Machine learning is as capable as human-based rules for the task of modeling the cover\n",
      "distribution, and so naturally lends itself to the task of designing steganographic algorithms, as well\n",
      "as performing steganalysis.\n",
      "In this paper, we introduce the ﬁrst steganographic algorithm produced entirely in an unsupervised\n",
      "manner, through a novel adversarial training scheme. We show that our scheme can be successfully\n",
      "implemented in practice between two communicating parties, and additionally that with supervised\n",
      "training, the steganalyzer, Eve, can compete against state-of-the-art steganalysis methods. To the best\n",
      "of our knowledge, this is one of the ﬁrst real-world applications of adversarial training, aside from\n",
      "traditional adversarial learning applications such as image generation tasks.\n",
      "\n",
      "2 Related work\n",
      "\n",
      "2.1 Adversarial learning\n",
      "\n",
      "Two recent designs have applied adversarial training to cryptographic and steganographic problems.\n",
      "Abadi and Andersen [2] used adversarial training to teach two neural networks to encrypt a short\n",
      "message, that fools a discriminator. However, it is hard to offer an evaluation to show that the\n",
      "encryption scheme is computationally difﬁcult to break, nor is there evidence that this encryption\n",
      "scheme is competitive against readily available public key encryption schemes. Adversarial training\n",
      "has also been applied to steganography [4], but in a different way to our scheme. Whereas we seek to\n",
      "train a model that learns a steganographic technique by itself, Volkhonskiy et al’s. work augments the\n",
      "original GAN process to generate images which are more susceptible to established steganographic\n",
      "algorithms. In addition to the normal GAN discriminator, they introduce a steganalyzer that receives\n",
      "examples from the generator that may or may not contain secret messages. The generator learns to\n",
      "generate realistic images by fooling the discriminator of the GAN, and learns to be a secure container\n",
      "by fooling the steganalyzer. However, they do not measure performance against state-of-the-art\n",
      "steganographic techniques making it difﬁcult to estimate the robustness of their scheme.\n",
      "\n",
      "2.2 Steganography\n",
      "\n",
      "Steganography research can be split into two subﬁelds: the study of steganographic algorithms and\n",
      "the study of steganalyzers. Research into steganographic algorithms concentrates on ﬁnding methods\n",
      "to embed secret information within a medium while minimizing the perturbations within that medium.\n",
      "Steganalysis research seeks to discover methods to detect such perturbations. Steganalysis is a binary\n",
      "classiﬁcation task: discovering whether or not secret information is present with a message, and so\n",
      "machine learning classiﬁers are commonly used as steganalyzers.\n",
      "Least signiﬁcant bit (LSB) [16] is a simple steganographic algorithm used to embed a secret message\n",
      "within a cover image. Each pixel in an image is made up of three RGB color channels (or one for\n",
      "grayscale images), and each color channel is represented by a number of bits. For example, it is\n",
      "\n",
      "2\n",
      "\n",
      "\fC\n",
      "\n",
      "M\n",
      "\n",
      "Alice\n",
      "\n",
      "C(cid:48)\n",
      "\n",
      "Eve\n",
      "\n",
      "p\n",
      "\n",
      "Bob\n",
      "\n",
      "M(cid:48)\n",
      "\n",
      "(a)\n",
      "\n",
      "(1)\n",
      "\n",
      "(2)\n",
      "\n",
      "(3)\n",
      "\n",
      "Alice\n",
      "\n",
      "Bob\n",
      "\n",
      "(b)\n",
      "\n",
      "Figure 1: (a) Diagram of the training game. (b) How two parties, Carol and David, use the scheme in practice:\n",
      "(1) Two parties establish a shared key. (2) Carol trains the scheme on a set of images. Information about model\n",
      "weights, architecture and the set of images used for training is encrypted under the shared key and sent to David,\n",
      "who decrypts to create a local copy of the models. (3) Carol then uses the Alice model to embed a secret\n",
      "encrypted message, creating a steganographic image. This is sent to David, who uses the Bob model to decode\n",
      "the encrypted message and subsequently decrypt.\n",
      "\n",
      "common to represent a pixel in a grayscale image with an 8-bit binary sequence. The LSB technique\n",
      "then replaces the least signiﬁcant bits of the cover image by the bits of the secret message. By only\n",
      "manipulating the least signiﬁcant bits of the cover image, the variation in color of the original image\n",
      "is minimized. However, information from the original image is always lost when using the LSB\n",
      "technique, and is known to be vulnerable to steganalysis [6].\n",
      "Most steganographic schemes for images use a distortion function that forces the embedding process\n",
      "to be localized to parts of the image that are considered noisy or difﬁcult to model. Advanced\n",
      "steganographic algorithms attempt to minimize the distortion function between a cover image, C,\n",
      "and a steganographic image, C(cid:48),\n",
      "\n",
      "d(C, C(cid:48)) = f (C, C(cid:48)) · |C − C(cid:48)|\n",
      "\n",
      "It is the choice of the function f, the cost of distorting a pixel, which changes for different stegano-\n",
      "graphic algorithms.\n",
      "HUGO [18] is considered to be one of the most secure steganographic techniques. It deﬁnes a\n",
      "distortion function domain by assigning costs to pixels based on the effect of embedding some\n",
      "information within a pixel, the space of pixels is condensed into a feature space using a weighted\n",
      "norm function. WOW (Wavelet Obtained Weights) [9] is another advanced steganographic method\n",
      "that embeds information into a cover image according to regions of complexity. If a region of an\n",
      "image is more texturally complex than another, the more pixel values within that region will be\n",
      "modiﬁed. Finally, S-UNIWARD [10] proposes a universal distortion function that is agnostic to the\n",
      "embedding domain. However, the end goal is much the same: to minimize this distortion function,\n",
      "and embed information in noisy regions or complex textures, avoiding smooth regions of the cover\n",
      "images. In Section 4.2, we compare out results against a state-of-the-art steganalyzer, ATS [13]. ATS\n",
      "uses labeled data to build artiﬁcial training sets of cover and steganographic images, and is trained\n",
      "using an SVM with a Gaussian kernel. They show that this technique outperforms other popular\n",
      "steganalysis tools.\n",
      "\n",
      "3 Steganographic adversarial training\n",
      "\n",
      "This section discusses our steganographic scheme, the models we use and the information each\n",
      "party wishes to conceal or reveal. After laying this theoretical groundwork, we present experiments\n",
      "supporting our claims.\n",
      "\n",
      "3.1 Learning objectives\n",
      "\n",
      "Our training scheme involves three parties: Alice, Bob and Eve. Alice sends a message to Bob, Eve\n",
      "can eavesdrop on the link between Alice and Bob and would like to discover if there is a secret\n",
      "message embedded within their communication. In classical steganography, Eve (the Steganalyzer)\n",
      "is passed both unaltered images, called cover images, and images with secret messages embedded\n",
      "\n",
      "3\n",
      "\n",
      "AliceEveBobMCC0M0pAliceEveBobMCC0M0p\fwithin, called steganographic images. Given an image, Eve places a conﬁdence score of how likely\n",
      "this is a cover or steganographic image. Alice embeds a secret message within the cover image,\n",
      "producing a steganographic image, and passes this to Bob. Bob knows the embedding process and so\n",
      "can recover the message. In our scheme, Alice, Bob and Eve are neural networks. Alice is trained to\n",
      "learn to produce a steganographic image such that Bob can recover the secret message, and such that\n",
      "Eve can do no better than randomly guess if a sample is a cover or steganographic image.\n",
      "The full scheme is depicted in Figure 1a: Alice receives a cover image, C, and a secret encrypted\n",
      "message, M, as inputs. Alice outputs a steganographic image, C(cid:48), which is given to both Bob and\n",
      "Eve. Bob outputs M(cid:48), the secret message he attempts to recover from C(cid:48). We say Bob performs\n",
      "perfectly if M = M(cid:48). In addition to the steganographic images, Eve also receives the cover images.\n",
      "Given an input X, Eve outputs the probability, p, that X = C. Alice tries to learn an embedding\n",
      "scheme such that Eve always outputs p = 1\n",
      "2. We do not train Eve to maximize her prediction error,\n",
      "since she can then simply ﬂip her decision and perform with perfect classiﬁcation accuracy. Figure 1b\n",
      "shows how the scheme should be used in pratice if two people wish to communicate a steganographic\n",
      "message using our scheme. The cost of sending the encrypted model information from Carol to David\n",
      "is low, with an average of 70MB. Note that in Figure 1b, steps (1) and (2), the set-up of the shared key\n",
      "and sharing of model information, is perfomed ofﬂine. We assume, as is common in cryptographic\n",
      "research, that this initial set-up phase is not visible to an adversary.\n",
      "At the beginning of training, a human can easily separate cover images from steganographic images,\n",
      "as Alice has not learned yet how to embed the secret message such that there is no visible difference\n",
      "in the cover image. However, we train Eve much like a discriminator in a GAN, where we tie her\n",
      "predictive power to the embedding capacity of Alice. When Alice produces a steganographic image\n",
      "that does not resemble the cover image, Eve does not have the ability to perfectly separate cover from\n",
      "steganographic images. As training continues, Eve becomes better at her task, but then so does Alice\n",
      "as her weights are updated, in part, based on the loss of Eve.\n",
      "Similarly to Abadi and Andersen [2], we let θA, θB, θC denote the parameters of Alice, Bob and Eve,\n",
      "respectively. We write A(θA, C, M ) for Alice’s output on C and M, B(θb, C(cid:48)) for Bob’s output on\n",
      "C(cid:48), and E(θE, C, C(cid:48)) for Eve’s output on C and C(cid:48). Let LA, LB, LC denote the loss of Alice, Bob\n",
      "and Eve, respectively. Then, we have the following relations:\n",
      "\n",
      "B(θb, C(cid:48)) = B(θb, A(θA, C, M ))\n",
      "\n",
      "E(θE, C, C(cid:48)) = E(θE, C, A(θA, C, M ))\n",
      "\n",
      "We set Bob’s loss (the secret message reconstruction loss), to be the Euclidean distance between M\n",
      "and M(cid:48):\n",
      "\n",
      "LB(θA, θB, M, C) = d(M, B(θb, C(cid:48)))\n",
      "\n",
      "= d(M, B(θb, A(θA, C, M ))\n",
      "= d(M, M(cid:48))\n",
      "\n",
      "As is common with GAN discriminator implementations, we set the Eve’s loss to be sigmoid cross\n",
      "entropy loss:\n",
      "\n",
      "LE(θE, C, C(cid:48)) = −y · log(E(θE, x))\n",
      "\n",
      "− (1 − y) · log(1 − E(θE, x)),\n",
      "\n",
      "where y = 0 if x = C(cid:48) and y = 1 if x = C. Alice’s loss is given as a weighted sum of Bob’s loss,\n",
      "Eve’s loss on steganographic images, and an additional reconstructive loss term:\n",
      "\n",
      "LA(θA, C, M ) = λA · d(C, C(cid:48)) + λB · LB\n",
      "+ λE · LE(θE, C, C(cid:48)),\n",
      "\n",
      "where d(C, C(cid:48)) is the Euclidean distance between the cover image and the steganographic image,\n",
      "and λA, λB, λE ∈ R deﬁne the weight given to each respective loss term.\n",
      "Our goal is not only to explore whether neural networks can produce steganographic embedding\n",
      "algorithms in an unsupervised manner, but whether they are competitive against steganographic\n",
      "algorithms like HUGO, WOW and S-UNIWARD, that have been designed by steganography experts.\n",
      "We did not intend to encode a speciﬁc algorithm within the neural network, rather we would like to\n",
      "give the networks the opportunity to devise their own.\n",
      "\n",
      "4\n",
      "\n",
      "\f(a) Cover images.\n",
      "\n",
      "Steganographic\n",
      "\n",
      "(b)\n",
      "(0.1bpp).\n",
      "\n",
      "images\n",
      "\n",
      "Steganographic\n",
      "\n",
      "(c)\n",
      "(0.4bpp).\n",
      "\n",
      "images\n",
      "\n",
      "Figure 2: Cover and steganographic images from the celebA dataset, with embedding rates of 0.1bpp and\n",
      "0.4bpp.\n",
      "\n",
      "(a) Alice, Bob and Eve error for 0.1bpp.\n",
      "\n",
      "(b) Alice, Bob and Eve error for 0.4bpp.\n",
      "\n",
      "(c) Bob decoding success for 0.1bpp.\n",
      "\n",
      "(d) Bob decoding success for 0.4bpp.\n",
      "\n",
      "Figure 3: Results on celebA dataset at an embedding rate of 0.1bpp and 0.4bpp.\n",
      "\n",
      "3.2 Architecture\n",
      "\n",
      "Alice (Steganographic Encoder). Alice accepts as input a cover image and a random n-bit binary\n",
      "message. The cover image is ﬂattened and concatenated with the secret message, which is then fed\n",
      "into the neural network. We would like the locality of the embedding process to be learned instead of\n",
      "ﬁxed; the ﬁrst layer is fully connected to the next, allowing the secret message to be combined with\n",
      "any region of the cover image. We then reshape to 4 × 4 × I · 8, where I is width of the cover image.\n",
      "Each of the next four layers are a sequence of upsampling fractionally-strided convolutions, batch\n",
      "normalization and ReLU [17], except for the ﬁnal layer where tanh [12] is used as the activation\n",
      "function.\n",
      "Bob (Steganographic Decoder). Bob accepts as input the steganographic image output by Alice.\n",
      "Bob uses a deep convolutional architecture to extract the important bits from the output of Alice.\n",
      "It is made up of four fractionally-strided convolutions and one ﬁnal fully connected layer. Leaky\n",
      "ReLU [15] is used in all but the ﬁnal layer, where tanh is used. Batch normalization is applied at each\n",
      "layer.\n",
      "Eve (Steganalyzer). Eve accepts as input both cover images and steganographic images. Eve has a\n",
      "similar architecture to Bob but uses the sigmoid activation function instead of tanh at the ﬁnal layer\n",
      "to output probabilities from the logits.\n",
      "\n",
      "5\n",
      "\n",
      "020406080100120140Steps0.51.01.52.0LossAliceBobEve020406080100120140Steps0.20.40.60.81.01.21.4LossAliceBobEve020406080100120140Steps020406080100Number of bits correctly decoded (out of 100)020406080100120140Steps050100150200250300350400Number of bits correctly decoded (out of 400)\f4 Experiments\n",
      "\n",
      "As a proof of concept, we implemented our adversarial training scheme on two image datasets:\n",
      "celebrity faces in the wild (celebA) [14] and a standard steganography research dataset, BOSS2.\n",
      "We concatenate an n-bit random message, with each sample of each dataset. We vary the size\n",
      "of the message, n, to test the limits of the amount of information that can be effectively hidden\n",
      "inside a cover image. This corresponds to the number of bits per pixel (bpp) that can be hidden by\n",
      "a steganographic algorithm, a common measure of the success of a steganographic research. On\n",
      "average, steganographic algorithms can successfully hide approximately 0.4bpp. Our datasets are\n",
      "formed of 32 × 32 pixel images, and so we vary n from 100 to 400, this corresponds to hiding\n",
      "between approximately 0.1bpp to 0.4bpp. We intentionally randomize the secret message so that the\n",
      "network does not learn a speciﬁc function associated with a speciﬁc group of messages. We want to\n",
      "be sure that our model can generalize to any message that is less than n bits3. For each experiment,\n",
      "we performed grid search to ﬁnd the optimum loss weights, λA, λB, λE, for Alice. All experiments\n",
      "in this section were performed in TensorFlow [1, 3], on a workstation with a Tesla K40 GPU card.\n",
      "\n",
      "4.1 CelebA dataset\n",
      "\n",
      "We ﬁrst motivate our work by showing our model can produce steganographic images that are almost\n",
      "indistinguishable from the cover images. We vary the size of the embedded secret message, to explore\n",
      "how many bits may be stored in a cover image while remaining undetected by a steganalyzer. As\n",
      "expected, we ﬁnd that a larger perturbation in the cover image results in more communicated bits but\n",
      "carries a greater risk of detection.\n",
      "The celebA dataset consists of 202, 599 images of celebrity faces from 10, 177 unique identities. We\n",
      "train in batches of 32, and use the Adam optimizer [11] with a learning rate of 2 · 10−4. At each\n",
      "batch we alternate training either Alice and Bob, or Eve.\n",
      "Figure 2b and Figure 2c show the steganographic images output by Alice after 150 training steps, for\n",
      "embedding rates of 0.1bpp and 0.4bpp, respectively. Our scheme easily copes with an embedding\n",
      "rate of 0.1bpp for a 32 × 32 image, which is equivalent to communicating a short 12 character\n",
      "ASCII message. Figure 3a and Figure 3c shows the training loss of each model, and the success\n",
      "of Bob decoding the secret message over time, respectively. In the ﬁrst few rounds of training, the\n",
      "visual quality of Alice’s output is low and Bob is essentially randomly guessing the message. After\n",
      "approximately 30 steps, Alice learns to successfully embed information in a cover image such that\n",
      "Eve is fooled, and Bob learns to correctly decode the message. After 150 training steps, Bob decodes\n",
      "the message with an average success of 100%, while Eve’s loss remains the same. Figure 3b and\n",
      "Figure 3d show similar results, with both sample quality and success of decoding both high by the end\n",
      "of training. Concealing 0.4bpp corresponds to successfully hiding a message of 51 ASCII characters.\n",
      "Due to the larger amount of information communicated between Alice and Bob, it takes a longer\n",
      "period of time for Alice to discover an effective embedding scheme, and for Bob to correctly discover\n",
      "the secret message. After 150 training steps, Bob can successfully decode 100% of the message.\n",
      "There is no noticeable image quality decrease between 0.1bpp and 0.4bpp, however we found after\n",
      "0.4bpp, image quality suffered and it was no longer possible to balance realistic image creation\n",
      "capable of fooling Eve, and convergence of message decoding. For both embedding rates of 0.1bpp\n",
      "and 0.4bpp, Eve’s loss remained constant at approximately 1.4. Eve’s loss is the sum of losses on\n",
      "both steganographic images and cover images, which both remained steady at approximately 0.69.\n",
      "Due to the logarithmic term in the loss function, this is equivalent to randomly guessing if an image\n",
      "carries a secret message.\n",
      "\n",
      "4.2 BOSS dataset\n",
      "\n",
      "In addition to our experiments on the celebA dataset, we trained our steganographic scheme on the\n",
      "BOSS image dataset, which is commonly used as a benchmark in steganography research. BOSS\n",
      "is made up of 10, 000 grayscale images depicting a variety of scenes such as landscapes, buildings\n",
      "and animals. We expected our scheme to perform worse than on the celebA dataset, since the cover\n",
      "\n",
      "2http://agents.fel.cvut.cz/boss/index.php?mode=VIEW&tmpl=materials\n",
      "3This ensures our scheme can embed ciphertexts of messages, which appear as random strings.\n",
      "\n",
      "6\n",
      "\n",
      "\f(a) Cover images of buildings, birds, skies and the\n",
      "ocean.\n",
      "\n",
      "(b) Steganographic images (0.1bpp).\n",
      "\n",
      "(c) Alice, Bob and Eve error for 0.1bpp.\n",
      "\n",
      "(d) Bob decoding success for 0.1bpp.\n",
      "\n",
      "Figure 4: Results on BOSS dataset at an embedding rate of 0.1bpp.\n",
      "\n",
      "images do not come from a single distribution. However, we found our scheme is still capable of\n",
      "embedding secret information successfully.\n",
      "Figure 4b shows the sample quality of steganographic images with an embedding rate of 0.1bpp,\n",
      "while Figure 4c and Figure 4d show the error rates of Alice, Bob and Eve, and the success of Bob\n",
      "decoding the secret message, respectively. While image quality suffers slightly more than on the\n",
      "celebA dataset, our scheme is still able to learn a steganographic algorithm. Our scheme is output\n",
      "samples that are not dissimilar from the original dataset, while Bob is able to learn to successfully\n",
      "decode the message. Alice and Bob both learn their respective tasks in a relatively short period of\n",
      "time, after which there is not much improvement in terms of hiding or recovering the secret message.\n",
      "At the end of training, Bob is able to successfully decode the secret message with 99.8% accuracy.\n",
      "\n",
      "4.3 Comparison with related work\n",
      "\n",
      "Fooling a steganalyzer, Eve, is easy by design, since we train in such a way that Eve never has a\n",
      "signiﬁcant competitive advantage. Thus, we additionally show that the resultant trained stegano-\n",
      "graphic model, Alice, can fool an independent steganalyzer. We compare our scheme against\n",
      "both state-of-the-art steganographic algorithms and steganalysis tools and show that it performs\n",
      "competitively.\n",
      "For both BOSS and CelebA, we compare our scheme against steganographic algorithms HUGO,\n",
      "WOW and S-UNIWARD. Additionally, we implement the Eve model using supervised training and\n",
      "compare against the steganalyzer ATS in Table 1. By design, Eve only performs slightly better than\n",
      "random. One may wonder whether the choice of model for Eve is wise; why not use an established\n",
      "steganalyzer in place of the Eve model? By training Eve in a supervised fashion, we show that Eve\n",
      "has the capacity to become a strong steganalyzer, competing against established techniques like ATS,\n",
      "and so is a good choice for the steganalyzer. Furthermore, Eve does not require a feature extraction\n",
      "preprocessing step as with ATS, and, from our experiments, is an order of magnitude quicker to\n",
      "train. For both the BOSS and CelebA datasets, we use 10, 000 samples and split in half, creating\n",
      "a training set and a test set. Alice was then trained on the 5000 samples from the training set. We\n",
      "then created an additional 10, 000 steganographic images for each steganographic algorithm (Alice,\n",
      "HUGO, WOW and S-UNIWARD). Now each steganographic algorithm has an associated training\n",
      "\n",
      "7\n",
      "\n",
      "020406080100120140Steps0.20.40.60.81.01.21.4LossAliceBobEve020406080100120140Steps020406080100Number of bits correctly decoded (out of 100)\fTable 1: Accuracy of distinguishing between cover and steganographic images for the steganalyzers, Eve and\n",
      "ATS, on the BOSS and CelebA datasets at an embedding rate of 0.4bpp.\n",
      "\n",
      "STEGANOGRAPHIC ALGORITHM STEGANALYZER\n",
      "\n",
      "BOSS\n",
      "\n",
      "ALICE\n",
      "HUGO\n",
      "WOW\n",
      "S-UNIWARD\n",
      "\n",
      "ATS\n",
      "0.83\n",
      "0.66\n",
      "0.75\n",
      "0.77\n",
      "\n",
      "EVE\n",
      "0.79\n",
      "0.59\n",
      "0.74\n",
      "0.72\n",
      "\n",
      "CELEBA\n",
      "\n",
      "STEGANALYZER\n",
      "ATS\n",
      "0.95\n",
      "0.94\n",
      "0.89\n",
      "0.91\n",
      "\n",
      "EVE\n",
      "0.90\n",
      "0.89\n",
      "0.85\n",
      "0.84\n",
      "\n",
      "set and test set, each consisting of 5000 cover images and 5000 steganographic images. For each\n",
      "steganographic algorithm we train both ATS and Eve on the associated training set, and then report\n",
      "accuracy of the steganalyzer on the test set. From Table 1, Eve performs competitively against the\n",
      "steganalyzer, ATS, and Alice also performs well against other steganographic techniques. While\n",
      "our scheme does not substantially improve on current popular steganographic methods, it is clear\n",
      "that it does not perform signiﬁcantly worse, and that unsupervised training methods are capable of\n",
      "competing with expert domain knowledge.\n",
      "\n",
      "4.4 Evaluating robust decryption\n",
      "\n",
      "Due to the non-convexity of the models in the training scheme, we cannot guarantee that two separate\n",
      "parties training on the same images will converge to the same model weights, and so learn the same\n",
      "embedding and decoding algorithms. Thus, prior to steganographic communication, we require\n",
      "one of the communicating parties to train the scheme locally, encrypt model information and pass\n",
      "it to the other party along with information about the set of training images. This ensures both\n",
      "parties learn the same model weights. To validate the practicality of our idea, we trained the scheme\n",
      "locally (Machine A) and then sent model information to another workstation (Machine B) that\n",
      "reconstructed the learned models. We then passed steganographic images, embedded by the Alice\n",
      "model from Machine A, to Machine B, who used the Bob model to recover the secret messages.\n",
      "Using messages of length corresponding to hiding 0.1bpp, and randomly selecting 10% of the CelebA\n",
      "dataset, Machine B was able to recover 99.1% of messages sent by Machine A, over 100 trials; our\n",
      "scheme can successfully decode the secret encrypted message from the steganographic image. Note\n",
      "that our scheme does not require perfect decoding accuracy to subsequently decrypt the message.\n",
      "A receiver of a steganographic message can successfully decode and decrypt the secret message if\n",
      "the mode of encryption can tolerate errors. For example, using a stream cipher such as AES-CTR\n",
      "guarantees that incorrectly decoded bits will not affect the ability to decrypt the rest of the message.\n",
      "\n",
      "5 Discussion & conclusion\n",
      "\n",
      "We have offered substantial evidence that our hypothesis is correct and machine learning can be\n",
      "used effectively for both steganalysis and steganographic algorithm design.\n",
      "In particular, it is\n",
      "competitive against designs using human-based rules. By leveraging adversarial training games,\n",
      "we conﬁrm that neural networks are able to discover steganographic algorithms, and furthermore,\n",
      "these steganographic algorithms perform well against state-of-the-art techniques. Our scheme does\n",
      "not require domain knowledge for designing steganographic schemes. We model the attacker as\n",
      "another neural network and show that this attacker has enough expressivity to perform well against a\n",
      "state-of-the-art steganalyzer.\n",
      "We expect this work to lead to fruitful avenues of further research. Finding the balance between\n",
      "cover image reconstruction loss, Bob’s loss and Eve’s loss to discover an effective embedding scheme\n",
      "is currently done via grid search, which is a time consuming process. Discovering a more reﬁned\n",
      "method would greatly improve the efﬁciency of the training process. Indeed, discovering a method\n",
      "to quickly check whether the cover image has the capacity to accept a secret message would be a\n",
      "great improvement over the trial-and-error approach currently implemented. It also became clear\n",
      "that Alice and Bob learn their tasks after a relatively small number of training steps, further research\n",
      "is needed to explore if Alice and Bob fail to improve due to limitations in the model or because of\n",
      "shortcomings in the training scheme.\n",
      "\n",
      "8\n",
      "\n",
      "\f6 Acknowledgements\n",
      "\n",
      "The authors would like to acknowledge ﬁnancial support from the UK Government Communications\n",
      "Headquarters (GCHQ), as part of University College London’s status as a recognised Academic\n",
      "Centre of Excellence in Cyber Security Research. Jamie Hayes is supported by a Google PhD\n",
      "Fellowship in Machine Learning. We thank the anonymous reviewers for their comments.\n",
      "\n",
      "References\n",
      "[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro,\n",
      "Greg S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensorﬂow: Large-scale\n",
      "machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467,\n",
      "2016.\n",
      "\n",
      "[2] Martín Abadi and David G Andersen. Learning to protect communications with adversarial\n",
      "\n",
      "neural cryptography. arXiv preprint arXiv:1610.06918, 2016.\n",
      "\n",
      "[3] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu\n",
      "Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for\n",
      "large-scale machine learning. 2016.\n",
      "\n",
      "[4] Boris Borisenko Denis Volkhonskiy and Evgeny Burnaev. Generative adversarial networks for\n",
      "\n",
      "image steganography. ICLR 2016 Open Review, 2016.\n",
      "\n",
      "[5] Tomáš Filler, Andrew D Ker, and Jessica Fridrich. The square root law of steganographic capac-\n",
      "ity for markov covers. In IS&T/SPIE Electronic Imaging, pages 725408–725408. International\n",
      "Society for Optics and Photonics, 2009.\n",
      "\n",
      "[6] Jessica Fridrich, Miroslav Goljan, and Rui Du. Detecting lsb steganography in color, and\n",
      "\n",
      "gray-scale images. IEEE multimedia, 8(4):22–28, 2001.\n",
      "\n",
      "[7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\n",
      "Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani,\n",
      "M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors, Advances in Neural\n",
      "Information Processing Systems 27, pages 2672–2680. Curran Associates, Inc., 2014.\n",
      "\n",
      "[8] M. Hayden. The price of privacy: Re-evaluating the nsa, 2014.\n",
      "\n",
      "[9] Vojtech Holub and Jessica Fridrich. Designing steganographic distortion using directional ﬁlters.\n",
      "In Information Forensics and Security (WIFS), 2012 IEEE International Workshop on, pages\n",
      "234–239. IEEE, 2012.\n",
      "\n",
      "[10] Vojtˇech Holub, Jessica Fridrich, and Tomáš Denemark. Universal distortion function for\n",
      "steganography in an arbitrary domain. EURASIP Journal on Information Security, 2014(1):1,\n",
      "2014.\n",
      "\n",
      "[11] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n",
      "\n",
      "arXiv:1412.6980, 2014.\n",
      "\n",
      "[12] Yann A LeCun, Léon Bottou, Genevieve B Orr, and Klaus-Robert Müller. Efﬁcient backprop.\n",
      "\n",
      "In Neural networks: Tricks of the trade, pages 9–48. Springer, 2012.\n",
      "\n",
      "[13] Daniel Lerch-Hostalot and David Megías. Unsupervised steganalysis based on artiﬁcial training\n",
      "\n",
      "sets. Eng. Appl. Artif. Intell., 50(C):45–59, April 2016.\n",
      "\n",
      "[14] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in\n",
      "the wild. In Proceedings of the IEEE International Conference on Computer Vision, pages\n",
      "3730–3738, 2015.\n",
      "\n",
      "[15] Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. Rectiﬁer nonlinearities improve neural\n",
      "\n",
      "network acoustic models. In Proc. ICML, volume 30, 2013.\n",
      "\n",
      "[16] Jarno Mielikainen. Lsb matching revisited. IEEE signal processing letters, 13(5):285–287,\n",
      "\n",
      "2006.\n",
      "\n",
      "9\n",
      "\n",
      "\f[17] Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines.\n",
      "In Proceedings of the 27th international conference on machine learning (ICML-10), pages\n",
      "807–814, 2010.\n",
      "\n",
      "[18] Tomáš Pevn`y, Tomáš Filler, and Patrick Bas. Using high-dimensional image models to perform\n",
      "highly undetectable steganography. In International Workshop on Information Hiding, pages\n",
      "161–177. Springer, 2010.\n",
      "\n",
      "10\n",
      "\n",
      "\f\n",
      "\n",
      "###\n",
      "\n",
      "Hiding Images in Deep Probabilistic Models\n",
      "Haoyu Chen\n",
      "Department of Computer Science\n",
      "City University of Hong Kong\n",
      "haoychen3-c@my.cityu.edu.hk\n",
      "Linqi Song\n",
      "Department of Computer Science\n",
      "City University of Hong Kong\n",
      "linqi.song@cityu.edu.hk\n",
      "Zhenxing Qian\n",
      "School of Computer Science\n",
      "Fudan University\n",
      "zxqian@fudan.edu.cn\n",
      "Xinpeng Zhang\n",
      "School of Computer Science\n",
      "Fudan University\n",
      "zhangxinpeng@fudan.edu.cn\n",
      "Kede Ma∗\n",
      "Department of Computer Science\n",
      "City University of Hong Kong\n",
      "kede.ma@cityu.edu.hk\n",
      "Abstract\n",
      "Data hiding with deep neural networks (DNNs) has experienced impressive suc-\n",
      "cesses in recent years. A prevailing scheme is to train an autoencoder, consisting of\n",
      "an encoding network to embed (or transform) secret messages in (or into) a carrier,\n",
      "and a decoding network to extract the hidden messages. This scheme may suffer\n",
      "from several limitations regarding practicability, security, and embedding capacity. In this work, we describe a different computational framework to hide images in\n",
      "deep probabilistic models. Specifically, we use a DNN to model the probability\n",
      "density of cover images, and hide a secret image in one particular location of\n",
      "the learned distribution. As an instantiation, we adopt a SinGAN, a pyramid of\n",
      "generative adversarial networks (GANs), to learn the patch distribution of one cover\n",
      "image. We hide the secret image by fitting a deterministic mapping from a fixed set\n",
      "of noise maps (generated by an embedding key) to the secret image during patch\n",
      "distribution learning. The stego SinGAN, behaving as the original SinGAN, is\n",
      "publicly communicated; only the receiver with the embedding key is able to extract\n",
      "the secret image. We demonstrate the feasibility of our SinGAN approach in terms\n",
      "of extraction accuracy and model security. Moreover, we show the flexibility of\n",
      "the proposed method in terms of hiding multiple images for different receivers and\n",
      "obfuscating the secret image. 1\n",
      "Introduction\n",
      "Data hiding generally refers to the process of hiding a form of secret message in another form of\n",
      "cover media, while minimizing the introduced distortions to the cover media [14, 47]. For human\n",
      "eavesdroppers, the measured distortion should be consistent with human judgments, penalizing errors\n",
      "that are most perceptually or cognitively noticeable [55, 5]; for machine eavesdroppers, the distortion\n",
      "should be “invisible” in a way that bypasses digital steganalysis tools such as StegExpose [9] and\n",
      "more recent deep learning-based ones [10]. Only the informed receiver typically with a shared\n",
      "embedding key (through a secure subliminal channel [50]) is able to extract the secret message. The\n",
      "∗Corresponding author. 36th Conference on Neural Information Processing Systems (NeurIPS 2022). form of the secret message can be encrypted bit streams [15], texts [30], audio signals [37], images\n",
      "[6], and videos[52]. Similarly, the cover media can also be texts [8], audio signals [31], images [6],\n",
      "videos [52], neural networks [5, 56], and even human behaviours[66]. Like many problems in signal and image processing, data hiding has been revolutionized by the\n",
      "remarkable development of DNNs [6, 67, 28]. These methods typically follow an autoencoder\n",
      "approach with two key components: an encoding network and a decoding network. For secret-in-\n",
      "image hiding [6, 67], the encoding network takes the cover image and the secret message as inputs,\n",
      "and generates a stego image with the hidden message (see Fig. 1 (a)). For secret-in-network hiding\n",
      "[5, 56], the cover media becomes some pre-selected weight layers of the encoding network, and\n",
      "the secret message usually serves as a watermark (see Fig. 1 (b)). For constructive (or generative)\n",
      "image hiding [28], the encoding network directly maps the secret message to the stego image without\n",
      "reliance on any cover image (see Fig. 1 (c)). In all cases, the decoding network is responsible for\n",
      "extracting the secret message. Despite demonstrated success, the autoencoder scheme may suffer\n",
      "from three main drawbacks. First, the decoding network, whose size may be significantly larger than\n",
      "that of the secret message, must be sent to the receiver side via the subliminal channel2, making the\n",
      "paradigm less practical. Second, it is not hard to re-train existing DNN-based steganalysis methods\n",
      "[10, 57] to identify stego images (or stego weight matrices), making the paradigm less secure. Third,\n",
      "it is difficult to hide multiple images for different receivers via the same encoding and decoding\n",
      "networks, making the paradigm less flexible. Our Contributions. In this paper, we propose to hide images in deep probabilistic models, which is\n",
      "substantially different from the previous autoencoder scheme (see Fig. 1 (d)). The key idea is to use a\n",
      "DNN to model the high-dimensional probability density of training cover images, and hide the secret\n",
      "image in one particular location of the learned distribution. The stego DNN for density estimation is\n",
      "publicly communicated, from which we may draw samples that look like training cover images. Only\n",
      "guided sampling by the embedding key (shared between the sender and receiver) is able to reproduce\n",
      "the secret image. We construct a specific example under the proposed probabilistic image hiding framework. Specif-\n",
      "ically, we adopt a SinGAN [49], a pyramid of generative adversarial networks (GANs) [21], to\n",
      "implicitly learn the patch distribution of a single cover image. During distribution learning, we use the\n",
      "same SinGAN to fit a deterministic mapping from a fixed set of noise maps (generated by the shared\n",
      "embedding key) to the secret image, which completes the image hiding process. The stego SinGAN\n",
      "that behaves like the original one is publicly communicated. A single forward propagation suffices\n",
      "to extract the secret image by the receiver with the embedding key, and no decoding network is\n",
      "trained and transmitted. Experiments demonstrate that the proposed method is 1) feasible, extracting\n",
      "the secret image with improved accuracy (compared to the autoencoder-based methods), 2) secure,\n",
      "behaving normally as the original SinGAN in several aspects, and 3) flexible, hiding multiple images\n",
      "for different receivers and obfuscating the secret image with graceful performance degradation. 2\n",
      "Related Work\n",
      "Depending on the applications, data hiding can be broadly divided into two subtopics: watermarking\n",
      "and steganography. Watermarking [24] aims to embed a digital watermark in a multimedia file for\n",
      "copyright protection and content management on social networks. Thus, watermarking techniques\n",
      "focus primarily on the robustness and perceptibility aspects of the embedded watermarks. Steganogra-\n",
      "phy [33, 47] aims to conceal a secret message within a cover media mainly for covert communication. Thus, steganography pays more attention to the trade-off among embedding capacity, extraction\n",
      "accuracy, and model security. Here we provide a concise review of image hiding techniques, and\n",
      "refer the interested readers to [47, 14, 63] for more comprehensive surveys of the field. Secret-in-Image Hiding. The most common image steganography modifies the least significant\n",
      "bits (LSBs) of images, either uniformly [13, 42] or adaptively [46, 25], guided by the design of\n",
      "novel distortion functions. Other representative techniques include pixel value differencing [60],\n",
      "histogram shifting [45, 64], and recursive code construction [65]. Transform domain steganography\n",
      "[48, 27, 7]) have also been proposed with improved capacity and security. Often, these methods\n",
      "leave traces in the form of certain statistical irregularities, which can be easily revealed by simple\n",
      "2The decoding network, if shared via a public channel, will be suspectable, as it is only trained to extract the\n",
      "secret message rather than performing typical machine learning tasks. 2\n",
      "\fPreparation \n",
      "Network\n",
      "Encoding \n",
      "Network\n",
      "Decoding \n",
      "Network\n",
      "Stego Image\n",
      "Secret Message\n",
      "Secret Message\n",
      "Cover Image\n",
      "(a) Secret-in-image hiding. The preparation network is optional. Decoding\n",
      "Network\n",
      "Secret Message\n",
      "Secret Message\n",
      "Selected Weights\n",
      "Encoding \n",
      "Network\n",
      "(b) Secret-in-network hiding. The secret message is usually in the form of a watermark. Decoding\n",
      "Network\n",
      "Secret Message\n",
      "Secret Message\n",
      "Stego Image\n",
      "Encoding \n",
      "Network\n",
      "(c) Constructive (or generative) image hiding. Regular\n",
      "Sampling\n",
      "Density Estimation \n",
      "Network\n",
      "Secret Image\n",
      "Secret Image\n",
      "Training Cover Images\n",
      "Guided \n",
      "Sampling\n",
      "Generated Cover Images\n",
      "(d) Proposed framework for hiding images in deep probabilistic models. Figure 1: Paradigms for hiding data using DNNs. steganalysis algorithms as countermeasures [19, 41, 26]. Zhu et al. [67] proposed one of the first\n",
      "DNN-based autoencoder for unified watermarking and steganography with an optional noise layer. Baluja [6] extended it to “image-in-image” steganography with a preparation network to preprocess\n",
      "the secret image. Weng et al. [59] further extended it to “video-in-video” steganography via temporal\n",
      "residual modeling. Normalizing flow-based invertible architectures [18] have also been investigated\n",
      "for multiple image hiding [40, 32, 22]. Here, we describe a different framework to accomplish a\n",
      "similar but more challenging goal - multiple image hiding for different users - with several other\n",
      "advantages. Secret-in-Network Hiding. A prerequisite for secret-in-network hiding is that the hidden message\n",
      "should not affect the network performance on the given machine learning task. As a result, the\n",
      "message (mostly the watermark for intellectual property protection of the neural network) is com-\n",
      "monly embedded during network training. Typical strategies for this purpose include parameter\n",
      "regularization [53], backdooring [5], output watermarking [61], and weight selection [56]. The\n",
      "proposed framework can be seen as a form of secret-in-network hiding, but with different goals\n",
      "(steganography instead of watermarking). Constructive (Generative) Image Hiding. Traditional methods hide secret messages during the\n",
      "construction of some specific types of images, such as textures [62] and fingerprints [36]. Recent\n",
      "DNN-based constructive image hiding methods mainly aim to construct the mapping between secret\n",
      "messages and stego images of more unconstrained content types [38, 58]. The proposed framework\n",
      "can be seen as a form of constructive image hiding, where we hide a secret image during the\n",
      "“construction” of a probability density function, but with larger embedding capacity and improved\n",
      "model security. 3\n",
      "Hiding Images in Deep Probabilistic Models\n",
      "General Framework. Without loss of generality, we describe the general framework of hiding a\n",
      "single image in deep probabilistic models. The straightforward extension to multiple image hiding\n",
      "is described in Sec. 4.3. We assume a cover image dataset D = {x(1), x(2), . . . , x(M)}, where\n",
      "3\n",
      "\fSinGAN\n",
      "SinGAN\n",
      "Noise \n",
      "Generator\n",
      "Embedding Key\n",
      "Fixed Noise\n",
      "Secret Image\n",
      "Randomly \n",
      "Sampled Noise\n",
      "Generated \n",
      "Cover Image\n",
      "Figure 2: Hiding images in a SinGAN. each image is drawn independently from an underlying image distribution p(x). D may include a\n",
      "single image (where x(i) becomes the i-th image patch), images of a specific class (e.g., textures,\n",
      "fingerprints, and faces), and images on the natural image manifold. Also given is a secret image x(s)\n",
      "of arbitrary content and an embedding key k shared by the sender and the receiver, which is a typical\n",
      "setting in the prisoner’s problem formulated by Simmons [50]. The proposed probabilistic image hiding framework consists of two main steps. First, we learn a\n",
      "probability density function ps(x) over Ds = D S{x(s)}, i.e., the combination of the cover image\n",
      "dataset and the secret image either explicitly via (approximate) maximum likelihood [35, 18, 51, 34]\n",
      "or implicitly via likelihood-free inference (e.g., density estimation by comparison) [43]. The learned\n",
      "probability model ps(x) is publicly communicated in the proposed framework, which is usually in\n",
      "the form of a DNN (as the scoring function [20] or the sample generating function [43]). We may\n",
      "as well learn a reference probability density function pc(x) on the cover image dataset D solely. To guarantee the security of our framework, ps(x) should be as close to pc(x) as possible in some\n",
      "statistical distance (i.e., distribution-preserving [12]). Provided that the adopted probability density\n",
      "estimator is robust (to the outlier image x(s)), which assigns an infinitesimal probability mass to\n",
      "x(s), the proposed framework is perfectly secure in any statistical distance sense. Second, we design\n",
      "a guided sampling procedure (with the help of the embedding key k) to draw a sample image ˆx(s)\n",
      "from ps(x) that looks identical to the secret image x(s). A third party without the embedding key\n",
      "is only able to generate samples that resemble cover images. It is noteworthy that the proposed\n",
      "framework does not require the learned ps(x) (or equivalently pc(x)) to be sufficiently close to\n",
      "the underlying p(x), a daunting (if not impossible) task to complete as digital images reside in a\n",
      "very high-dimensional space. In other words, it is perfectly fine that the samples drawn from ps(x)\n",
      "(without guidance) contain visually noticeable distortions as long as such distortions are shared by\n",
      "the samples drawn from pc(x). Specific Example. Within the general framework of probabilistic image hiding, we provide a specific\n",
      "example that relies on GANs [21], a family of implicit probability models, which are represented\n",
      "by a stochastic procedure of data sampling. Although more natural to work with in our context\n",
      "compared to prescribed probabilistic models [16], unconditional GANs have the notorious reputation\n",
      "of being difficult to train, especially when modeling image sets with diverse content complexities and\n",
      "rich semantics. Thus, to make our work easily reproducible, we opt for a SinGAN [49] to learn the\n",
      "internal patch distribution of a single cover image x(c)\n",
      "0\n",
      "at various scales. A SinGAN consists of a pyramid of generators {G0, G1 . . . , GN}, where Gn takes the upsampled\n",
      "version of the generated image by Gn+1 as well as an additive white Gaussian noise map zn of the\n",
      "same size as inputs, and produces an image sample at the n-th scale:\n",
      "ˆx(c)\n",
      "n\n",
      "= Gn\n",
      "\u0010\n",
      "zn,\n",
      "\u0010\n",
      "ˆx(c)\n",
      "n+1\n",
      "\u0011\n",
      "↑r\u0011\n",
      ",\n",
      "n < N,\n",
      "(1)\n",
      "where r > 1 is the pre-defined upsampling ratio. The generation process starts at the coarsest scale\n",
      "(i.e., the N-th scale), where the input is purely noise:\n",
      "ˆx(c)\n",
      "N = GN (zN) ,\n",
      "(2)\n",
      "and progressively makes use of all generators to produce the finest scale image ˆx(c)\n",
      "0\n",
      "with possibly\n",
      "different size and aspect ratio of x(c)\n",
      "0\n",
      "[49]. Coupled with the generators is a pyramid of discriminators\n",
      "{D0, D1, . . . , DN}, and each Dn is trained to discriminate between patches extracted from ˆx(c)\n",
      "n (in\n",
      "Eq. (1)) and x(c)\n",
      "n , which is a downsampled version of x(c)\n",
      "0\n",
      "by a factor of rn (i.e., downsampled n\n",
      "times by a factor of r). 4\n",
      "\fTable 1: Extraction accuracy comparison when hiding\n",
      "one image. \"↑\": larger is better, and vice versa. Method\n",
      "#params\n",
      "PSNR↑\n",
      "SSIM↑\n",
      "DISTS↓\n",
      "LSB\n",
      "—\n",
      "23.06\n",
      "0.785\n",
      "0.095\n",
      "Baluja17\n",
      "0.48M\n",
      "25.91\n",
      "0.874\n",
      "0.102\n",
      "HiDDeN\n",
      "0.38M\n",
      "27.99\n",
      "0.897\n",
      "0.096\n",
      "Weng19\n",
      "42.6M\n",
      "35.64\n",
      "0.942\n",
      "0.055\n",
      "HiNet\n",
      "4.05M\n",
      "35.59\n",
      "0.952\n",
      "0.047\n",
      "Ours\n",
      "0.67M\n",
      "36.84\n",
      "0.958\n",
      "0.038\n",
      "Table 2: Cover image quality, diver-\n",
      "sity, and weight distribution similarity\n",
      "between the original and stego SinGANs\n",
      "in terms of SIFID, DS, and KLD. #images\n",
      "SIFID↓\n",
      "DS↑\n",
      "KLD↓\n",
      "0 (ref)\n",
      "0.041\n",
      "0.407\n",
      "0\n",
      "1\n",
      "0.046\n",
      "0.430\n",
      "0.001\n",
      "2\n",
      "0.045\n",
      "0.415\n",
      "0.004\n",
      "3\n",
      "0.047\n",
      "0.427\n",
      "0.006\n",
      "4\n",
      "0.051\n",
      "0.438\n",
      "0.008\n",
      "The training objective for each scale is a weighted combination of an adversarial term and a recon-\n",
      "struction term:\n",
      "min\n",
      "Gn max\n",
      "Dn ℓadv\n",
      "\u0010\n",
      "Gn, Dn; x(c)\u0011\n",
      "+ λℓrec\n",
      "\u0010\n",
      "Gn; x(c)\u0011\n",
      ",\n",
      "(3)\n",
      "where λ is the trade-off parameter. The adversarial loss ℓadv is for penalizing the statistical difference\n",
      "between the patch distribution of the generated ˆx(c)\n",
      "n\n",
      "and that of the (downsampled) cover image\n",
      "x(c)\n",
      "n . The reconstruction loss ℓrec is for stabilizing the training process by ensuring that x(c)\n",
      "n can be\n",
      "reconstructed from a specific set of input noise maps. Following the original SinGAN paper [49],\n",
      "we use the WGAN-GP loss [23] and the mean squared error (MSE) to implement ℓadv and ℓrec,\n",
      "respectively. After training, the SinGAN is capable of generating new image samples that preserve the patch\n",
      "distribution of x(c), with novel and plausible scene configurations and structures. Once the learning\n",
      "procedure of the SinGAN is clear, hiding the secret image x(s) during the patch distribution learning\n",
      "can be straightforwardly done by modifying the training objective from Eq. (3) to\n",
      "min\n",
      "Gn max\n",
      "Dn ℓadv\n",
      "\u0010\n",
      "Gn, Dn; x(c)\u0011\n",
      "+ λℓrec\n",
      "\u0010\n",
      "Gn; x(s)\u0011\n",
      ". (4)\n",
      "That is, a reconstruction loss is replaced to enforce that a specific set of input noise maps z(s) =\n",
      "{z(s)\n",
      "0 , z(s)\n",
      "1 , . . . , z(s)\n",
      "N } is mapped to the secret image x(s) instead of the cover image x(c). z(s) can\n",
      "be generated by a standard Gaussian pseudo-random number generator [11] using the embedding key\n",
      "k as the seed. We may as well put back the reconstruction term for x(c), but we find this makes little\n",
      "difference during training and testing. We conjecture that the reconstruction loss in Eq. (4) not only\n",
      "enables hiding of the secret image, but also plays a similar role in improving the training stability and\n",
      "convergence. For ease of description, we refer to models optimized for Eq. (3) and Eq. (4) as the\n",
      "original and stego SinGANs, respectively. To extract the secret image x(s), the receiver uses the shared embedding key k to re-generate the\n",
      "noise maps z(s), which is fed to the publicly transmitted stego SinGAN for secret image extraction\n",
      "via a single forward propagation (see Fig. 2). 4\n",
      "Experiments\n",
      "In this section, we perform a series of experiments to verify the promise of our SinGAN approach. First, we evaluate secret image extraction accuracy both quantitatively and qualitatively in comparison\n",
      "to image-in-image hiding methods based on autoencoders. Second, we probe the security of the\n",
      "stego SinGAN by comparing it to the original one in terms of 1) quality and diversity of generated\n",
      "cover images, 2) marginal distribution similarity of model parameters [56], and 3) possibility of\n",
      "secret image leakage. Third, we experiment with our method in two more challenging scenarios: 1)\n",
      "hiding multiple images within a SinGAN for different users and 2) hiding the content-obfuscated\n",
      "image. We implement the generators and discriminators of the SinGAN by medium-size DNNs,\n",
      "whose specifications and training details are given in the Appendix. Our quantitative experiments\n",
      "make use of 200 test image pairs, whose details are also given in the Appendix. 5\n",
      "\f0\n",
      "50\n",
      "100\n",
      "150\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "0\n",
      "50\n",
      "100\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "(a) Cover x(c)\n",
      "(b) Secret x(s)\n",
      "(c) Generated ˆx(c) (d) Extracted ˆx(s)\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "(e) 5 × |x(s) −ˆx(s)|\n",
      "Figure 3: Visual results of our SinGAN approach. The second image of the last row is a pixel-shuffled\n",
      "version of the second image in the first row. Zoom in for improved visibility. 4.1\n",
      "Extraction Accuracy\n",
      "In image data hiding, extraction accuracy means how the secret image can be faithfully reproduced at\n",
      "the receiver side. Here, we apply three objective image quality measures to quantitatively evaluate\n",
      "the signal fidelity, perceptual fidelity, and perceptual quality of the extracted secret image: the peak\n",
      "signal-to-noise ratio (PSNR), the structural similarity (SSIM) index [55], and the deep image structure\n",
      "and texture similarity (DISTS) measure [17]. Although our SinGAN approach is the first of its kind in\n",
      "the proposed probabilistic image hiding framework, we compare it with one naïve LSB replacement\n",
      "method, and four image-in-image steganography methods - Baluja17 [6], HiDDeN [67], Weng19\n",
      "[59] and HiNet[32]. The LSB replacement method simply replaces the four LSB planes of the cover\n",
      "image with the four most significant bit (MSB) planes of the secret image. As the robustness is\n",
      "not our focus, we re-train HiDDeN [67] for image hiding without the noise layer. Weng19 [59] is\n",
      "a DNN-based video steganography method, and only the image hiding branch is used for testing. HiNet [32] relies on a normalizing flow-based invertible DNN to implement the encoder, such that\n",
      "the decoder is simply its inverse. We use the publicly available implementations for Baluja17 [1],\n",
      "HiDDeN[2], Weng19[3] and HiNet[4] with default training and testing3 settings, and implement the\n",
      "LSB replacement by ourselves. The extraction accuracy results are shown in Table 1, where we find that the proposed SinGAN\n",
      "approach performs favorably against existing autoencoder-based image-in-image steganography\n",
      "methods. We consider the obtained improvements as significant because secret-in-network hiding is\n",
      "generally considered much more difficult than secret-in-image hiding, where the latter has significant\n",
      "spatial redundancy to be reduced. After all, the most recent secret-in-network hiding method [56] has\n",
      "a limited embedding capacity up to 6, 000 bits. We also show some representative visual results of our\n",
      "method in Fig. 3. It is clear that the stego SinGAN is able to capture the internal patch distribution of\n",
      "the cover image, generating image samples with different but reasonable structures and configurations\n",
      "3It is important to note that the results of autoencoder-based methods in our paper (especially in terms of\n",
      "extraction accuracy in Table 1) may be noticeably different from some of the previous publications. This is\n",
      "because we choose to quantize the stego image from the single-precision floating-point format of 32 × 3 to\n",
      "8 × 3 bits per pixel before transmitting it to the receiver side. If such quantization is not properly enforced,\n",
      "trivial hiding solutions may exist because there are just more space to accommodate the cover and secret images\n",
      "by a simple concatenation. 6\n",
      "\f(a) Cover Image\n",
      "(b) Original\n",
      "(c) Stego\n",
      "Figure 4: Visual comparison of the generated cover images “Fire balloons” by the original and stego\n",
      "SinGANs. In our case, the secret is the “Birds” image, as shown in the second row of Fig. 3. of the same natural scene. Moreover, the extracted secret image by guided sampling is visually close\n",
      "to the original, which is adequate for use in the majority of real-world steganography applications. 4.2\n",
      "Model Security\n",
      "Steganographic analysis (i.e., steganalysis) aims to identify whether there are hidden messages in\n",
      "suspected media, which constitutes an indispensable part of steganography evaluation. Traditional\n",
      "statistics-based and recent DNN-based steganalysis methods [9, 10] are mostly designed to discrimi-\n",
      "nate between cover and stego images, thus not applicable when the cover media is a DNN. To the\n",
      "best of our knowledge, we are not aware of a steganalysis algorithm that accepts a full DNN (with\n",
      "millions of parameters) as input. Instead, we propose to probe the security of our SinGAN approach\n",
      "from the following three different aspects. Quality and Diversity of Generated Cover Images. In the proposed framework, the stego SinGAN\n",
      "is publicly transmitted, and thus it must function as the original SinGAN [49]. As the primary goal of\n",
      "SinGANs is to model internal patch distributions, we examine and compare the quality and diversity\n",
      "of generated cover images by the original and stego SinGANs. To quantify quality, we adopt the\n",
      "single image Fréchet inception distance (SIFID) metric, as suggested in [49]. To quantify diversity,\n",
      "for each cover image, we compute the diversity score (DS) as the standard deviation (std) of each\n",
      "pixel values over 25 generated samples of the cover content, averaged over all pixels and normalized\n",
      "by the std of pixel intensities of the cover image [49]. The average quality and diversity results\n",
      "over 200 test image pairs are shown in Table 2, where we find that both SIFID and DS of the stego\n",
      "SinGAN are statistically indistinguishable from those of the original SinGAN based on a hypothesis\n",
      "testing using t-statistics [44]. Fig. 4 shows some randomly sampled examples from the original and\n",
      "stego SinGANs, which provides additional visual evidence that they learn very similar internal patch\n",
      "distributions of the cover image. Marginal Distribution Similarity of Model Weights. As the proposed method essentially hides\n",
      "the secret image in the learned weights of the stego SinGAN, it is natural to ask whether its weight\n",
      "distributions significantly deviate from those of the original SinGAN. Following [56], we compute\n",
      "the Kullback–Leibler divergence (KLD) between the marginal distributions of model parameters of\n",
      "the original and stego SinGANs, as shown in the last column of Table 2. We find that the marginal\n",
      "distributions of the two sets of model parameters are almost identical, as evidenced by a KLD close to\n",
      "zero. Similar results can be obtained if the marginal distributions are compared in a per-stage fashion\n",
      "(see more results in the Appendix with visual comparison of histograms). Possibility of Secret Image Leakage. One may also wonder the possibility of secret image leakage\n",
      "if an adversary constantly draws samples from the stego SinGAN. Such steganalysis arises naturally\n",
      "from the fact that there is no theoretical guarantee that the mapping between the secret noise (generated\n",
      "by the embedding key) and the secret image is bijective (i.e., one-to-one). In other words, there\n",
      "might be some other sets of noise that are also mapped to the secret image, or at least some of its\n",
      "semantically meaningful content. Since the SinGAN is trained to be a black-box sampler, it is difficult\n",
      "to provide a theoretical analysis of the possibility of secret image leakage. Nevertheless, we conduct\n",
      "an empirical study, in which we randomly draw 100, 000 samples from each of the 200 trained stego\n",
      "SinGANs. Visual inspection of the thumbnails of generated samples indicates that no secret image\n",
      "7\n",
      "\fTable 3: Extraction accuracy of our method when\n",
      "hiding multiple images in one SinGAN. #images\n",
      "PSNR↑\n",
      "SSIM↑\n",
      "DISTS↓\n",
      "One\n",
      "36.84\n",
      "0.958\n",
      "0.038\n",
      "Two\n",
      "35.91\n",
      "0.946\n",
      "0.043\n",
      "Three\n",
      "34.93\n",
      "0.935\n",
      "0.049\n",
      "Four\n",
      "34.03\n",
      "0.923\n",
      "0.055\n",
      "Table 4: Extraction accuracy of our method with\n",
      "image obfuscation. Obfuscation\n",
      "PSNR↑\n",
      "SSIM↑\n",
      "DISTS↓\n",
      "No\n",
      "36.84\n",
      "0.958\n",
      "0.038\n",
      "Yes\n",
      "20.53\n",
      "0.726\n",
      "0.172\n",
      "(a) 1st Original\n",
      "(b) Extracted\n",
      "0\n",
      "100\n",
      "200\n",
      "(c) Error (5×)\n",
      "(d) 2nd Original\n",
      "(e) Extracted\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "(f) Error (5×)\n",
      "(g) 3rd Original\n",
      "(h) Extracted\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "(i) Error (5×)\n",
      "(j) 4th Original\n",
      "(k) Extracted\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "(l) Error (5×)\n",
      "Figure 5: Visual comparison between the original and extracted secret images from a SinGAN with\n",
      "four hidden images. (or images with similar semantics) is revealed. Therefore, it is safe to empirically conclude that the\n",
      "possibility of secret image leakage is less than 0.001%. In summary, we have empirically proven that the proposed SinGAN approach is secure: hiding\n",
      "a full-size photographic image into a SinGAN does not compromise the quality and diversity of\n",
      "generated cover images, nor skew the weight distribution. It also survives constant sampling by an\n",
      "adversary. Such level of security verifies our claims that the secret image indeed occupies a tiny\n",
      "portion of probability mass of ps(x), and that ps(x) and pc(x) are statistically close. 4.3\n",
      "Further Extensions\n",
      "Hiding Multiple Images for Different Receivers. Hiding multiple images in a DNN is challenging;\n",
      "doing so for different receivers is even more challenging, and has not been accomplished before. Here\n",
      "the main difficulties lie not only in that more embedding capacity is required, but also in that each\n",
      "receiver must only extract her/his piece of message, and cannot extract (or even affirm the existence of)\n",
      "other messages [56]. The proposed probabilistic image hiding framework provides a straightforward\n",
      "and elegant extension to hide multiple images for different users, which follows a similar two-step\n",
      "approach. First, learn a probabilistic density function ps(x) over Ds = D S{x(s1), . . . , x(sT )},\n",
      "where T is the number of secret images and is considerably smaller than M, the number of cover\n",
      "images. Second, design T guided sampling procedures using T different embedding keys K =\n",
      "{k(1), . . . , k(T )}, shared to T different receivers. The learning goal remains the same: ps(x) should\n",
      "be close in some statistical distance to the reference distribution pc(x). The described procedure is\n",
      "more easily understood using the SinGAN instantiation, where we just modify the objective function\n",
      "in Eq. (4) to\n",
      "min\n",
      "Gn max\n",
      "Dn ℓadv\n",
      "\u0010\n",
      "Gn, Dn; x(c)\u0011\n",
      "+ λ 1\n",
      "T\n",
      "T\n",
      "X\n",
      "t=1\n",
      "ℓrec\n",
      "\u0010\n",
      "Gn; x(st)\u0011\n",
      ". (5)\n",
      "After training, the t-th receiver is able to re-generate the t-th specific set of noise maps z(st) using\n",
      "the shared embedding key k(t) for the t-th secret image extraction. S/he is, by design, ignorant of the\n",
      "presence (or absence) of other secret images. Even if the receiver is informed in some way that the\n",
      "current SinGAN contains multiple secret images, without extra embedding keys, s/he cannot extract\n",
      "images that are not intended to share with her/him. 8\n",
      "\fWe train SinGANs to hide up to four secret images, i.e., T ∈{2, 3, 4}. For each value of T, we train\n",
      "200 SinGANs to hide different combinations of cover and secret images. The quality and diversity\n",
      "of the cover image, and the weight distribution similarity between the original and stego SinGANs\n",
      "are shown in Table 2, where we find that hiding multiple images does not seem to compromise the\n",
      "security of the proposed method. The extraction accuracy results and visual examples are shown in\n",
      "Table 3 and Fig. 5, respectively. With the number of increasing secret images, the extraction accuracy\n",
      "in terms of the three objective metrics degrades gracefully. The visual appearances of the extracted\n",
      "secret images are similar to those of the original, showing great promise of our method in hiding\n",
      "multiple images for different receivers. Obfuscating the Secret Image. Inspired by [6], we consider obfuscating the secret image by\n",
      "shuffling its pixels (i.e., image scrambling [54]), as a way of strengthening security. In this case, The\n",
      "shuffling key, together with the embedding key, is shared to the receiver. Table 4 shows the extraction\n",
      "accuracy results, where we see that pixel shuffling significantly increases the difficulty of image\n",
      "hiding. Nevertheless, from the last row of Fig. 3, we observe that the main content is clearly visible,\n",
      "despite somewhat noisy appearance. 5\n",
      "Conclusion and Discussion\n",
      "We have described a new computational framework for hiding images in deep probabilistic models,\n",
      "which is in stark contrast to previous steganography schemes. We provided an instantiation, where we\n",
      "used the SinGAN to build the internal patch distribution of the cover image, and hid the secret image\n",
      "during patch distribution learning. We conducted a series of experiments to demonstrate the feasibility\n",
      "of the proposed SinGAN approach in terms of extraction accuracy and model security. Moreover, our\n",
      "method is readily extended to hide multiple images for different receivers, a challenging task that has\n",
      "not been accomplished before. In addition, it works nicely with pixel shuffling, which adds additional\n",
      "security. The current work opens the door to a new class of image hiding methods, with many interesting\n",
      "problems to be explored. First, the extraction accuracy of our SinGAN approach still has quite\n",
      "some room for improvement. For applications that require precise recovery of the secret image, it\n",
      "is worth exploring more efficient network structures, optimized for perceptual losses that exploit\n",
      "the physiological properties of the human visual system. Second, our method naïvely bypasses\n",
      "the steganalysis tools specifically designed for secret-in-image steganography. Currently, we have\n",
      "designed three different tests to probe the model security. As SinGANs [49] can be applied in a much\n",
      "wider range of image manipulation tasks such as super-resolution and paint-to-image translation, the\n",
      "stego SinGAN should be tested in those applications as well for model security. More importantly,\n",
      "we expect future effort to be dedicated to building steganalysis methods that accept a full DNN as\n",
      "input and assess whether it contains secret messages. Third, so far, we have just given the theoretical\n",
      "intuition of the probabilistic image hiding framework, supplied with empirical evidence. It is of\n",
      "mathematical interest to rigorously measure the statistical distance between pc(x) and ps(x), in an\n",
      "attempt to answer important questions like 1) where the secret image is hidden in the network (or\n",
      "equivalently the learned distribution) and 2) what the maximum number of secret images is allowed for\n",
      "a given distance constraint (e.g., KLD(pc(x) ∥ps(x)) ≤ϵ). Fourth, many other generative modeling\n",
      "methods [51] are worthy of deep investigation within the proposed framework to circumvent some\n",
      "of the limitations of the SinGAN approach. For example, we may model the (Stein) score function\n",
      "[39, 29] of ps(x) (i.e., the gradient of log ps(x)), and develop guided Langevin-type sampling (by\n",
      "the embedding key) to extract the secret image. Acknowledgments and Disclosure of Funding\n",
      "The authors would like to thank Song Yang for inspiring discussion. This work was supported\n",
      "in part by the Hong Kong RGC ECS Grants 21213821 (to KDM) and 21212419 (to LQS), the\n",
      "National Natural Science Foundation of China under Grants 62071407, 61936214, U20B2051 and\n",
      "U1936214, the InnoHK initiative, the Government of the HKSAR, Laboratory for AI-Powered\n",
      "Financial Technologies, and the Tencent AI Lab Rhino-Bird Gift Fund. 9\n",
      "\fReferences\n",
      "[1] Deep steganography, 2017. [Source code]. Available: https://github.com/alexandremuzio/deep-steg. [2] HiDDeN: Hiding data with deep networks, 2018. [Source code]. Available: https://github.com/jirenz/\n",
      "hidden. [3] Pytorch-Deep-Steganography, 2019. [Source code].\n",
      "\n",
      "###\n",
      "\n",
      "Collaborative Score Distillation\n",
      "for Consistent Visual Editing\n",
      "Subin Kim∗,1\n",
      "Kyungmin Lee∗,1\n",
      "June Suk Choi1\n",
      "Jongheon Jeong1\n",
      "Kihyuk Sohn2\n",
      "Jinwoo Shin1\n",
      "1KAIST\n",
      "2Google Research\n",
      "∗{subin-kim, kyungmnlee}@kaist.ac.kr\n",
      "Abstract\n",
      "Generative priors of large-scale text-to-image diffusion models enable a wide range\n",
      "of new generation and editing applications on diverse visual modalities. However,\n",
      "when adapting these priors to complex visual modalities, often represented as\n",
      "multiple images (e.g., video or 3D scene), achieving consistency across a set of\n",
      "images is challenging. In this paper, we address this challenge with a novel method,\n",
      "Collaborative Score Distillation (CSD). CSD is based on the Stein Variational\n",
      "Gradient Descent (SVGD). Specifically, we propose to consider multiple samples\n",
      "as “particles” in the SVGD update and combine their score functions to distill\n",
      "generative priors over a set of images synchronously. Thus, CSD facilitates the\n",
      "seamless integration of information across 2D images, leading to a consistent visual\n",
      "synthesis across multiple samples. We show the effectiveness of CSD in a variety\n",
      "of editing tasks, encompassing the visual editing of panorama images, videos, and\n",
      "3D scenes. Our results underline the competency of CSD as a versatile method\n",
      "for enhancing inter-sample consistency, thereby broadening the applicability of\n",
      "text-to-image diffusion models.1\n",
      "1\n",
      "Introduction\n",
      "Text-to-image diffusion models [1, 2, 3, 4] have been scaled up by using billions of image-text\n",
      "pairs [5, 6] and efficient architectures [7, 8, 9, 4], showing impressive capability in synthesizing\n",
      "high-quality, realistic, and diverse images with the text given as an input. Furthermore, they have\n",
      "branched into various applications, such as image-to-image translation [10, 11, 12, 13, 14, 15, 16],\n",
      "controllable generation [17], or personalization [18, 19]. One of the latest applications in this regard\n",
      "is to translate the capability into other complex modalities, viz., beyond 2D images [20, 21] without\n",
      "modifying diffusion models using modality-specific training data. This paper focuses on the problem\n",
      "of adapting the knowledge of pre-trained text-to-image diffusion models to more complex high-\n",
      "dimensional visual manipulation tasks beyond 2D images without modifying diffusion models using\n",
      "modality-specific training data.\n",
      "We start from an intuition that many complex visual data, e.g., videos and 3D scenes, are represented\n",
      "as a set of images constrained by modality-specific consistency. For example, a video is a set of frames\n",
      "requiring temporal consistency, and a 3D scene is a set of multi-view frames with view consistency.\n",
      "Unfortunately, image diffusion models do not have a built-in capability to ensure consistency between\n",
      "a set of images for synthesis or editing because their generative sampling process does not take\n",
      "into account the consistency when using the image diffusion model as is. As such, when applying\n",
      "image diffusion models to manipulate these complex data without consistency in consideration, it\n",
      "*Equal contribution.\n",
      "1Visualizations are available at the website https://subin-kim-cv.github.io/CSD.\n",
      "37th Conference on Neural Information Processing Systems (NeurIPS 2023).\n",
      "\fFigure 1: Method overview. CSD-Edit enables various visual-to-visual translations with two novel\n",
      "components. First, a new score distillation scheme using Stein variational gradient descent, which\n",
      "considers inter-sample relationships (Section 3.1) to synthesize a set of images while preserving\n",
      "modality-specific consistency constraints. Second, our method edits images with minimal information\n",
      "given from text instruction by subtracting image-conditional noise estimate instead of random noise\n",
      "during score distillation (Section 3.2). By doing so, CSD-Edit is used for text-guided manipulation of\n",
      "various visual domains, e.g., panorama images, videos, and 3D scenes (Section 3.3).\n",
      "results in a highly incoherent output, as in Figure 2 (Patch-wise Crop), where one can easily identify\n",
      "where images are being stitched. Such behaviors are also reported in video editing, thus, recent\n",
      "works [22, 23, 24, 25] propose to handle video-specific temporal consistency when using the image\n",
      "diffusion model.\n",
      "Here, we take attention to an alternative approach, Score Distillation Sampling (SDS) [26], which\n",
      "enables the optimization of arbitrary differentiable operators by leveraging the rich generative prior\n",
      "of text-to-image diffusion models. SDS poses generative sampling as an optimization problem by\n",
      "distilling the learned diffusion density scores. While Poole et al. [26] has shown the effectiveness\n",
      "of SDS in generating 3D objects from the text by resorting on Neural Radience Fields [27] priors\n",
      "which inherently suppose coherent geometry in 3D space through density modeling, it has not been\n",
      "studied for consistent visual manipulation of other modalities, where modality-specific consistency\n",
      "constraints should be considered when manipulating.\n",
      "In this paper, we propose Collaborative Score Distillation (CSD), a simple yet effective method that\n",
      "extends the singular of the text-to-image diffusion model for consistent visual manipulation. The\n",
      "crux of our method is two-fold: first, we establish a generalization of SDS by using Stein variational\n",
      "gradient descent (SVGD), where multiple samples share their knowledge distilled from diffusion\n",
      "models to accomplish inter-sample consistency. Second, we present CSD-Edit, an effective method\n",
      "for consistent visual editing by leveraging CSD with Instruct-Pix2Pix [14], a recently proposed\n",
      "instruction-guided image diffusion model (See Figure 1).\n",
      "We demonstrate the versatility of our method in various editing applications such as panorama image\n",
      "editing, video editing, and reconstructed 3D scene editing. In editing a panorama image, we show\n",
      "that CSD-Edit obtains spatially consistent image editing by optimizing multiple patches of an image.\n",
      "Also, compared to other methods, our approach achieves a better trade-off between source-target\n",
      "image consistency and instruction fidelity. In video editing experiments, CSD-Edit obtains temporal\n",
      "consistency by taking multiple frames into optimization, resulting in temporal frame-consistent\n",
      "video editing. Furthermore, we apply CSD-Edit to 3D scene editing and generation, by encouraging\n",
      "consistent manipulation and synthesis among multiple views.\n",
      "2\n",
      "Preliminaries\n",
      "2.1\n",
      "Diffusion models\n",
      "Generative modeling with diffusion models consists of a forward process q that gradually adds\n",
      "Gaussian noise to the input x0 ∼pdata(x), and a reverse process p which gradually denoises from\n",
      "2\n",
      "\f“Turn it into a Van-Gogh style painting”\n",
      "Source\n",
      "Instruct-Pix2Pix, Patch-wise Crop\n",
      "Instruct-Pix2Pix + Overlapping Patches,     =7.5\n",
      "CSD-Edit (Ours), \n",
      "=15\n",
      "CSD-Edit (Ours), \n",
      "=7.5\n",
      "ωy\n",
      "ωy\n",
      "ωy\n",
      "Instruct-Pix2Pix + Overlapping Patches,     =15\n",
      "ωy\n",
      "Spatial \n",
      "Consistency\n",
      "Instruction\n",
      "Fidelity\n",
      "Instruct-Pix2Pix\n",
      "Patch-wise \n",
      "Crop\n",
      "❌\n",
      "✅\n",
      "Instruct-Pix2Pix \n",
      "Overlapping \n",
      "Patches\n",
      "✅\n",
      "❌\n",
      "CSD-Edit\n",
      "(Ours)\n",
      "✅\n",
      "✅\n",
      "Figure 2: Panorama image editing. (Top right) Instruct-Pix2Pix [14] on cropped patches results in\n",
      "inconsistent image edits. (Second row) Instruct-Pix2Pix on overlapping patches edits to a consistent\n",
      "image, but less fidelity to the instruction, even with high guidance scale ωy. (Third row) CSD-Edit\n",
      "provides consistent image editing with better instruction-fidelity by setting a proper guidance scale.\n",
      "the Gaussian noise xT ∼N(0, I). Formally, the forward process q(xt|x0) at timestep t is given by\n",
      "q(xt|x0) = N(xt; αtx0, σ2\n",
      "t I), where σt and α2\n",
      "t = 1 −σ2\n",
      "t are pre-defined constants designed for\n",
      "effective modeling [8, 28, 29]. Given enough timesteps, reverse process p also becomes a Gaussian\n",
      "and the transitions are given by posterior q with optimal MSE denoiser [30], i.e., pϕ(xt−1|xt) =\n",
      "N(xt−1; xt −ˆxϕ(xt; t), σ2\n",
      "t I), where ˆxϕ(xt; t) is a learned optimal MSE denoiser. Ho et al. [7]\n",
      "proposed to train an U-Net [31] autoencoder ϵϕ(xt; t) by minimizing following objective:\n",
      "LDiff(ϕ; x) = Et∼U(0,1),ϵ∼N (0,I)\n",
      "\u0002\n",
      "w(t)∥ϵϕ(xt; t) −ϵ∥2\n",
      "2\n",
      "\u0003\n",
      ",\n",
      "xt = αtx0 + σtϵ\n",
      "(1)\n",
      "where w(t) is a weighting function for each timestep t. Text-to-image diffusion models [1, 2, 4, 3]\n",
      "are trained by Eq. (1) with ϵϕ(xt; y, t) that estimates the noise conditioned on the text prompt\n",
      "y. To effectively guide the text-conditional generation, Ho et al. [32] proposed classifier-free\n",
      "guidance (CFG), where they jointly train the unconditional and conditional model and interpolate the\n",
      "unconditional and conditional model during the inference, i.e., the noise estimate is given by\n",
      "ϵω\n",
      "ϕ(xt; y, t) = ϵϕ(xt; t) + ωy\n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "The CLIP Model is Secretly an Image-to-Prompt\n",
      "Converter\n",
      "Yuxuan Ding∗\n",
      "School of Electronic Engineering\n",
      "Xidian University\n",
      "Xi’an 710071, China\n",
      "yxding@stu.xidian.edu.cn\n",
      "Chunna Tian †\n",
      "School of Electronic Engineering\n",
      "Xidian University\n",
      "Xi’an 710071, China\n",
      "chnatian@xidian.edu.cn\n",
      "Haoxuan Ding\n",
      "Unmanned System Research Institute\n",
      "Northwestern Polytechnical University\n",
      "Xi’an 710072, China\n",
      "haoxuan.ding@mail.nwpu.edu.cn\n",
      "Lingqiao Liu †\n",
      "Australian Institute for Machine Learning\n",
      "The University of Adelaide\n",
      "Adelaide 5005, Australia\n",
      "lingqiao.liu@adelaide.edu.au\n",
      "Abstract\n",
      "The Stable Diffusion model is a prominent text-to-image generation model that re-\n",
      "lies on a text prompt as its input, which is encoded using the Contrastive Language-\n",
      "Image Pre-Training (CLIP). However, text prompts have limitations when it comes\n",
      "to incorporating implicit information from reference images. Existing methods\n",
      "have attempted to address this limitation by employing expensive training pro-\n",
      "cedures involving millions of training samples for image-to-image generation.\n",
      "In contrast, this paper demonstrates that the CLIP model, as utilized in Stable\n",
      "Diffusion, inherently possesses the ability to instantaneously convert images into\n",
      "text prompts. Such an image-to-prompt conversion can be achieved by utilizing a\n",
      "linear projection matrix that is calculated in a closed form. Moreover, the paper\n",
      "showcases that this capability can be further enhanced by either utilizing a small\n",
      "amount of similar-domain training data (approximately 100 images) or incorpo-\n",
      "rating several online training steps (around 30 iterations) on the reference images.\n",
      "By leveraging these approaches, the proposed method offers a simple and flexible\n",
      "solution to bridge the gap between images and text prompts. This methodology can\n",
      "be applied to various tasks such as image variation and image editing, facilitating\n",
      "more effective and seamless interaction between images and textual prompts.\n",
      "1\n",
      "Introduction\n",
      "In recent years, there has been a surge of interest in vision-and-language research, particularly in the\n",
      "field of text-to-image generation. Prominent models in this domain include autoregression models\n",
      "like DALL-E [1] and Make-A-Scene [2], as well as diffusion models like DALL-E 2 [3] and Stable\n",
      "Diffusion [4]. These models have revolutionized the quality of generated images. They leverage\n",
      "text prompts to synthesize images depicting various objects and scenes that align with the given\n",
      "text. Among these models, Stable Diffusion [4] stands out as a significant open-source model. It\n",
      "serves as a foundation for many recent works, including image generation [5, 6, 7, 8], image editing\n",
      "[9, 10, 11, 12, 13, 14], and more.\n",
      "∗This work was done while Yuxuan Ding was visiting The University of Adelaide as a visiting researcher.\n",
      "†Corresponding author.\n",
      "37th Conference on Neural Information Processing Systems (NeurIPS 2023).\n",
      "\fFigure 1: Demonstration of image vari-\n",
      "ation. The image on the left is a real\n",
      "reference image, while the four on the\n",
      "right are generated from our method.\n",
      "Figure 2: Attention map of Stable Diffusion [4]. The\n",
      "bottom row sets attention weights of caption words to\n",
      "zero, only keeping the start-/end-token, so the caption\n",
      "maps of the bottom are black. Also notice the start-\n",
      "token has strong weights so the map is all white.\n",
      "However, text prompts have limitations when it comes to incorporating unspeakable information\n",
      "from reference images. It becomes challenging to generate a perfect and detailed prompt when users\n",
      "want to synthesize images related to a picture they have seen. Image variation techniques aim to\n",
      "address this limitation by enabling users to generate multiple variations of an input image, without\n",
      "relying on complex prompts. As illustrated in Fig. 1, the generated variations closely resemble the\n",
      "reference image, often sharing the same scene or objects but with distinct details.\n",
      "Stable Diffusion Reimagine (SD-R) [4]3 is a recently proposed image variation algorithm. It achieves\n",
      "this goal by retraining Stable Diffusion [4], where the text encoder is replaced with an image encoder\n",
      "to adapt the model for image input. The model is trained using millions of images and over 200,000\n",
      "GPU-hours, enabling it to effectively generate image variations based on reference images.\n",
      "In this paper, we make a significant discovery that allows a more cost-effective image-to-prompt\n",
      "conversion approach. We find the CLIP model [15], as utilized in Stable Diffusion, can be repurposed\n",
      "as an effective image-to-prompt converter. This converter can be directly employed or served as a\n",
      "valuable initialization for a data-efficient fine-tuning process. As a result, the expenses associated\n",
      "with constructing or customizing an image-to-prompt converter can be substantially reduced.\n",
      "More specifically, our method is built upon a surprising discovery: the control of image generation\n",
      "through text is primarily influenced by the embedding of the end-of-sentence (EOS) token. We\n",
      "found that masking all word tokens, except for the start and end tokens, does not adversely affect\n",
      "the quality of image generation, as illustrated in Figure 2. Simultaneously, during CLIP training, the\n",
      "projection of the end-token embedding is trained to align with the visual embedding. This inherent\n",
      "relationship enables us to derive a closed-form projection matrix that converts visual embedding\n",
      "into an embedding that is capable of controlling the generation of Stable Diffusion [4]. We call this\n",
      "method Stable Diffusion Image-to-Prompt Conversion (SD-IPC).\n",
      "In addition, we introduce two methods to enhance the quality and flexibility of image-to-prompt\n",
      "conversion. The first approach involves parameter-efficient tuning using a small amount of data,\n",
      "consisting of only 100 images and requiring just 1 GPU-hour. This method encourages the model\n",
      "to better preserve image information and enables practitioners to control the specific content they\n",
      "want to retain when generating new images. The second approach involves customizing the model\n",
      "on reference images using a few iterations, ensuring that the generated images are closer to specific\n",
      "concepts. While this approach has been explored in previous research, we demonstrate that with the\n",
      "advantageous initialization provided by SD-IPC, the online fine-tuning requires significantly fewer\n",
      "iterations to achieve desirable results.\n",
      "2\n",
      "Background and Related Works\n",
      "2.1\n",
      "Diffusion Model\n",
      "Firstly, we present a brief overview of the Stable Diffusion [4], which serves as our underlying model.\n",
      "Diffusion models (DMs) [16, 17, 18, 19] belong to a class of latent variable models. In DMs, there\n",
      "exist two Markov chains known as the diffusion process and the reverse process, both having a fixed\n",
      "3https://stability.ai/blog/stable-diffusion-reimagine\n",
      "2\n",
      "\flength T. The diffusion process progressively introduces Gaussian noise to the original data (x0)\n",
      "until the signal becomes corrupted (xT ). During DMs training, the reverse process is learned, which\n",
      "operates in the opposite direction of the diffusion process. The reverse process can be viewed as a\n",
      "denoising procedure, moving from xt to xt−1 at each step. After multiple denoising steps, the model\n",
      "obtains instances that closely resemble the real data.\n",
      "Stable Diffusion [4] is built on the Latent Diffusion Model (LDM) [4]. LDM [4] proposed to do\n",
      "diffusion process in a latent space rather than the usual pixel space, significantly reducing the training\n",
      "and inference cost of the diffusion model. The authors proposed to utilize a VAE compression to\n",
      "get the latent code z0, which is x0 above. Diffusion process will build on the latents. A U-Net\n",
      "architecture [17] with timestep and text conditions would do the reverse. The text prompt is injected\n",
      "into the model with cross-attention layers. We denote ϵθ (zt, ctxt(ptxt), t) as the output of the U-Net,\n",
      "which is the predicted denoising result. ptxt is the textual prompt and ctxt(ptxt) is the prompt\n",
      "embedding from the text encoder. t is the timestep. The training objective of DMs is as followed:\n",
      "Eϵ,z,ptxt,t\n",
      "h\n",
      "∥ϵ −ϵθ (zt, ctxt(ptxt), t)∥2\n",
      "2\n",
      "i\n",
      ",\n",
      "(1)\n",
      "where ϵ ∼N (0, I) is the noise used to corrupt clean latent variables. During the generation, the latent\n",
      "zt, which starts at a random Gaussian noise zT , will recursively go through a denoising operation\n",
      "until z0 is sampled. Finally, z0 is reconstructed to an image by the VAE.\n",
      "2.2\n",
      "CLIP Model\n",
      "The CLIP model [15] has garnered significant acclaim as a groundbreaking zero-shot model in recent\n",
      "years. Its training process demands optimizing a contrastive loss function using extensive 400-million\n",
      "pairs of images and corresponding text descriptions. Through the meticulous training, the model has\n",
      "been able to achieve unparalleled capabilities in zero-shot classification and image-text retrieval.\n",
      "The model comprises an image encoder CLIPi (·), a text encoder CLIPt (·), a visual projection layer\n",
      "Wi, and a textual projection layer Wt. The image encoder encodes an input image x into a visual\n",
      "embedding fimg derived from a special class-token. By applying the visual projection layer, the\n",
      "embedding is projected into the CLIP visual embedding f c\n",
      "img. Similarly, the text encoder processes\n",
      "the input text, yielding a sequence of output embeddings ftxt for each text token and a start token and\n",
      "end-of-sentence (EOS) )token. The embedding of the EOS token f t,⟨eos⟩\n",
      "txt\n",
      ", where t denotes the length\n",
      "of the sentence, is projected into the CLIP textual embedding f c\n",
      "txt through Wt. Formally,\n",
      "fimg = CLIPi (x) ,\n",
      "f c\n",
      "img = Wi · fimg,\n",
      "(2)\n",
      "ftxt = CLIPt (s) ,\n",
      "f c\n",
      "txt = Wt · f t,⟨eos⟩\n",
      "txt\n",
      ".\n",
      "(3)\n",
      "The training objective of CLIP is to maximize the cosine similarity between f c\n",
      "txt and f c\n",
      "img for matched\n",
      "sentence-image pair while minimizing this similarity for unmatched pairs. For the simplicity of\n",
      "discussion, we denote the space spanned by ftxt as T -space and the space spanned by f c\n",
      "∗as C-space.\n",
      "The CLIP text encoder [15] is directly used in Stable Diffusion to encode text prompts. It encodes a\n",
      "text prompt as a sequence of embeddings:\n",
      "ftxt :=\n",
      "h\n",
      "f 0,⟨sos⟩\n",
      "txt\n",
      ", f 1,w0\n",
      "txt , ..., f t,⟨eos⟩\n",
      "txt\n",
      ", ..., f 76,⟨eos⟩\n",
      "txt\n",
      "i\n",
      "(4)\n",
      "where f 0,⟨sos⟩\n",
      "txt\n",
      ", f i,w\n",
      "txt and f t,⟨eos⟩\n",
      "txt\n",
      "denote the embeddings corresponding to the start-token, the i-th\n",
      "word token and end-token, respectively. From f t+1,⟨eos⟩\n",
      "txt\n",
      "to f 76,⟨eos⟩\n",
      "txt\n",
      "are padded tokens.\n",
      "2.3\n",
      "Image Variation & Customized Generation\n",
      "Image Variation. Image variation aims to generate images similar to the reference image but not\n",
      "identical. SD-R [4] is proposed to address this problem, which builds upon the Stable-unCLIP model4.\n",
      "The authors fine-tuned the Stable Diffusion model [4] to align with the CLIP visual embedding. In\n",
      "SD-R [4], images can be directly input into the diffusion model through CLIP image encoder. Since\n",
      "the original Stable Diffusion is conditioned on text only, an expensive fine-tuning is required to\n",
      "4https://huggingface.co/stabilityai/stable-diffusion-2-1-unclip\n",
      "3\n",
      "\faccommodate this new input. The process took 200,000 GPU-hours on NVIDIA A100-40GB GPU\n",
      "while our approach only requires 1 GPU-hour on NVIDIA A5000-24GB GPU5.\n",
      "Customized Generation. Recent works such as DreamBooth [11], Textual Inversion [14], and\n",
      "Custom Diffusion [13] focus on learning a special text prompt to feature specific objects or persons\n",
      "from the reference images. For instance, given several photos of a particular cat, these methods use a\n",
      "special-token \"⟨s⟩cat\" to represent the concept and incorporate it with the text prompt. DreamBooth\n",
      "[11] and Custom Diffusion [13] also perform simultaneous fine-tuning of diffusion model parameters.\n",
      "However, the fine-tuning process is still somewhat time-consuming, with Custom Diffusion [13]\n",
      "requiring nearly 6 minutes on 2 NVIDIA A100 GPUs. In contrast, our fast update SD-IPC only needs\n",
      "1 minute on 2 A5000 GPUs.\n",
      "Image Editing. Stable Diffusion [4] is commonly used for image editing tasks. Prompt-to-Prompt\n",
      "[9] and Plug-and-Play [10] utilize attention map as a bridge to enable concept and style manipulation.\n",
      "Null-Text Inversion [20] and Pix2Pix-Zero [21] relies on inversion-based methods. InstructPix2Pix\n",
      "[22] creates a dataset of paired edited images and fine-tunes Stable Diffusion [4] as an editing model.\n",
      "It’s important to highlight that while our primary focus in developing this method was to enhance\n",
      "image variation, it can also be employed to generate images based on prompts that combine both\n",
      "textual instructions and accompanying images. Notably, unlike existing approaches that frequently\n",
      "reproduce the layout of the original image in the generated output, our method operates without being\n",
      "confined to replicating the exact layout of the source image.\n",
      "3\n",
      "Methodology\n",
      "3.1\n",
      "Image-to-Prompt Conversion via Projecting CLIP embedding\n",
      "By design, the image generation process in the stable diffusion model should be influenced by\n",
      "embeddings of all tokens in a prompt, like Eq. (4). Interestingly, we have discovered that masking\n",
      "word tokens, by setting their attention weights to 0 except for the start-/end-token, does not have a\n",
      "negative impact on the quality of generated images. This finding is visually illustrated in Figure 2.\n",
      "On another note, the training objective of CLIP [15] is to match the embeddings f c\n",
      "img and f c\n",
      "txt,\n",
      "with f c\n",
      "txt being essentially a projection of f t,⟨eos⟩\n",
      "txt\n",
      ". This inherent relationship, coupled with the\n",
      "aforementioned observation, leads us to establish a connection between f c\n",
      "img and f t,⟨eos⟩\n",
      "txt\n",
      ", effectively\n",
      "converting the visual embedding to a prompt embedding.\n",
      "Formally, we assume that after training, CLIP model can induce high cosine similarity between the\n",
      "f c\n",
      "img and ftxt and we can further make the following approximation:\n",
      "f c\n",
      "img\n",
      "∥f c\n",
      "img∥≈\n",
      "f c\n",
      "txt\n",
      "∥f c\n",
      "txt∥, with f c\n",
      "txt = Wtf t,⟨eos⟩\n",
      "txt\n",
      ".\n",
      "(5)\n",
      "By using Moore-Penrose pseudo-inverse [23] on Wt6, we obtain an estimate of f t,⟨eos⟩\n",
      "txt\n",
      "from f c\n",
      "img:\n",
      "f t,⟨eos⟩\n",
      "txt\n",
      "≈∥f c\n",
      "txt∥\n",
      "∥f c\n",
      "img∥W +\n",
      "t f c\n",
      "img := f cnvrt\n",
      "txt\n",
      ", where, W +\n",
      "t =\n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "UDH: Universal Deep Hiding for Steganography,\n",
      "Watermarking, and Light Field Messaging\n",
      "Chaoning Zhang∗\n",
      "KAIST\n",
      "chaoningzhang1990@gmail.com\n",
      "Philipp Benz∗\n",
      "KAIST\n",
      "pbenz@kaist.ac.kr\n",
      "Adil Karjauv∗\n",
      "KAIST\n",
      "mikolez@gmail.com\n",
      "Geng Sun\n",
      "KAIST\n",
      "tosungeng@gmail.com\n",
      "In So Kweon\n",
      "KAIST\n",
      "iskweon77@kaist.ac.kr\n",
      "Abstract\n",
      "Neural networks have been shown effective in deep steganography for hiding\n",
      "a full image in another. However, the reason for its success remains not fully\n",
      "clear. Under the existing cover (C) dependent deep hiding (DDH) pipeline, it is\n",
      "challenging to analyze how the secret (S) image is encoded since the encoded\n",
      "message cannot be analyzed independently. We propose a novel universal deep\n",
      "hiding (UDH) meta-architecture to disentangle the encoding of S from C. We\n",
      "perform extensive analysis and demonstrate that the success of deep steganography\n",
      "can be attributed to a frequency discrepancy between C and the encoded secret\n",
      "image. Despite S being hidden in a cover-agnostic manner, strikingly, UDH\n",
      "achieves a performance comparable to the existing DDH. Beyond hiding one image,\n",
      "we push the limits of deep steganography. Exploiting its property of being universal,\n",
      "we propose universal watermarking as a timely solution to address the concern of\n",
      "the exponentially increasing number of images and videos. UDH is robust to a\n",
      "pixel intensity shift on the container image, which makes it suitable for challenging\n",
      "application of light ﬁeld messaging (LFM). Our work is the ﬁrst to demonstrate\n",
      "the success of (DNN-based) hiding a full image for watermarking and LFM. Code:\n",
      "https://github.com/ChaoningZhang/Universal-Deep-Hiding\n",
      "1\n",
      "Introduction\n",
      "The craft of steganography describes the secret communication without revealing the transported\n",
      "information to a third-party [25, 27, 14, 28]. The challenge for image steganography is to hide\n",
      "more information while keeping the container image look natural [17, 10, 9]. Recently, deep neural\n",
      "networks [32] have been shown to successfully hide a full image in another one [2] with a message ca-\n",
      "pacity of 24 bits per pixel (bpp) signiﬁcantly exceeding that of traditional techniques, e.g. HUGO [39]\n",
      "hides < 0.5 bpp. The task of (image) “steganography\" with traditional techniques often requires\n",
      "perfectly decoding the secret message while remaining undetected by steganalysis [40]. In contrast,\n",
      "deep steganography in [2] has introduced a conceptually similar but technically different task of\n",
      "hiding a full image. Speciﬁcally, it relaxed the constraint of perfect decoding while focused on a high\n",
      "hiding capacity with a visual quality trade-off between container image and decoded secret image [2]. Due to the large hiding capacity, it is unlikely that the hidden image can remain undetected [3]. This\n",
      "new task has also been explored in a wide range of works [45, 47]. Acknowledging the difference\n",
      "between traditional steganography and deep steganography, in this work we adopt the term “deep\n",
      "∗Equal contribution\n",
      "34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada. steganography” to be consistent with [2, 45, 47, 46]. The success of deep steganography also inspired\n",
      "the exploration of hiding binary information in deep watermarking [55] and deep photographic\n",
      "steganography, also termed light ﬁled messaging (LFM) [46]. Despite large information capacity,\n",
      "deep steganography has a high visual quality, the reason of which remains yet unexplored. With the\n",
      "focus of hiding a secret image, our work is the ﬁrst one towards explaining how deep steganography\n",
      "works as well as investigating it for applications in watermarking and LFM. Figure 1: Existing DDH meta-architecture with (left) [2] or without(right) [45] P network. In this work, the general practice of hiding one image in another one is termed deep hiding which\n",
      "serves as a hypernym or umbrella term including deep steganography, watermarking and LFM. The\n",
      "existing deep hiding pipelines fall into one meta-architecture category termed cover-dependent deep\n",
      "hiding (DDH). As shown in Figure 1, the cover image (C) and (processed) secret image (S) are\n",
      "concatenated as the input of a hiding (H) network to generate a container image (C′). Another\n",
      "reveal (R) network is used to recover the secret image (S′). The objective is to minimize ||C′ −C||\n",
      "and ||S′ −S|| simultaneously. Given C′ remains natural-looking, i.e. ||C′ −C|| is so small that it\n",
      "is human imperceptible, it is striking that the reveal (R) network can decode S′ almost perfectly\n",
      "from C′ [2]. The phenomenon of imperceptible hidden information triggering the R network\n",
      "echos with a parallel research line of adversarial attack [42, 18, 48, 21, 5, 8, 7, 1, 15], where a\n",
      "small imperceptible perturbation fools a target network. More intriguingly, a single image-agnostic\n",
      "perturbation is found to exist for attacking most images and often called universal adversarial\n",
      "perturbations [35, 36, 23, 49, 50, 6]. Inspired by this, we explore the possibility to hide an image in\n",
      "a cover-agnostic manner, i.e. universal deep hiding (UDH). The primary motivation of UDH is to facilitate explaining the success of deep steganography [2]. One\n",
      "natural guess is that messages are hidden in the least signiﬁcant bits (LSB) [10], however, preliminary\n",
      "analysis in [2] rules out this possibility. Intuitively, Se = C′ −C represents how S is encoded in C′,\n",
      "however, it is not meaningful to analyze Se independent of C in the existing DDH because Se, (being\n",
      "equal to H(C, S) −C), is dependent on C. Since S is encoded in C′, one alternative is to analyze\n",
      "C′ as a whole but the magnitude dominance of C over Se makes it impractical. The above reasons\n",
      "complicate the exploration of how S is encoded under the existing DDH. In the proposed UDH (See\n",
      "Figure 2), Se (being equal to H(S)) is independent of C. Thus, Se can be analyzed directly, which\n",
      "is a noticeable merit of UDH for understanding where and/or how the S is encoded. We ﬁnd that\n",
      "the success of UDH can be directly attributed to a frequency discrepancy between Se and C. With a\n",
      "cross-test of H and R from DDH and UDH, we also successfully demonstrate how DDH works. Overall, compared with DDH, UDH is a more challenging task because the algorithm of UDH can not\n",
      "adaptively encode Se based on C. Empirically, however, we ﬁnd that UDH results in a more smooth\n",
      "training and achieves comparable performance for deep steganography. Beyond hiding one image,\n",
      "we further push the limits of deep steganography with higher hiding capacity. Exploiting its property\n",
      "of being universal for high efﬁciency, we are the ﬁrst to investigate and demonstrate the possibility\n",
      "of (DNN-based) universal watermarking. This can be a timely solution for efﬁcient watermarking\n",
      "tackling the exponentially increasing number of images or videos. In contrast to HiDDeN [55] which\n",
      "watermarks by hiding binary information, we are the ﬁrst to demonstrate (DNN-based) watermarking\n",
      "by hiding images. The UDH for hiding images without retraining can be readily extended to hide\n",
      "simple binary information, achieving superior performance than [55]. UDH is robust to pixel intensity\n",
      "shift on C′, which makes it more suitable for the task of LFM. In contrast to [46] that only hides\n",
      "binary information, UDH is the ﬁrst to successfully hide and transmit an image robust to light\n",
      "effect, increasing its real-world applicability. It is also worth mentioning that UDH does not require\n",
      "collecting a large screen-pair dataset (1.9TB) as in [46]. For transmitting simple binary information,\n",
      "UDH achieves signiﬁcantly better performance than [46]. 2\n",
      "\f2\n",
      "Related work\n",
      "Traditional steganography and watermarking have been extensively studied in [44, 34, 12, 16, 41, 33,\n",
      "20], and we refer the readers to [4, 11] for an overall review. Our work focuses on understanding and\n",
      "harnessing deep learning for hiding messages in images and we summarize its recent advancement. Hiding a binary message in an image. With their great success in a wide range of applications [53,\n",
      "29, 30, 38, 52], DNNs also found adoption in steganography and watermarking [22]. In early\n",
      "explorations, DNNs have been adopted to mainly substitute a single stage of a larger pipeline [24, 26,\n",
      "37]. Recently, the trend is to train networks end-to-end for the whole working pipeline. Hayes et al. ﬁrst trained DNNs with adversarial training to hide binary messages in an end-to-end manner [19]. Taking robustness into account, HiDDeN [55] explored hiding binary messages for watermarking. Adversarial training was adopted in HiDDeN to minimize the artiﬁcial effect on C′. By encoding\n",
      "hyperlinks into binary bits, a concurrent work [43] also shows that DNNs can be trained to perform a\n",
      "robust encoding and decoding for physical photographs. The performance of these approaches can be\n",
      "evaluated by various metrics, such as capacity, secrecy, and robustness. There is often an inherent\n",
      "conﬂict between these metrics [19, 55]. For example, models with high capacity have low secrecy\n",
      "since hiding more information results in larger distortions on images. The models that are robust to\n",
      "distortions tend to sacriﬁce both secrecy and capacity. To increase robustness for watermarking, the\n",
      "hiding capacity in HiDDeN was less than 0.002 bpp [55]. Hiding an image message in an image. Hiding binary messages with DNNs has a low information\n",
      "capacity (typically lower than 0.5 bpp), which does not fully exploit the potential of deep hiding. In a seminal work [2], deep steganography has been shown to hide a full image with a very high\n",
      "capacity of 24 bpp. It adopted an additional preparation (P) network to process the image into a\n",
      "new form before concatenating it with the cover image, see Figure 1 (left). The technique of hiding\n",
      "an image in another can be easily extended to hide videos in videos, by sequentially hiding each\n",
      "frame of one video in the frame of another video. This approach has been explored in [45] where\n",
      "temporal redundancy has been exploited to hide the residual secret frame instead of the original\n",
      "image frame. Hiding 8 frames in 8 frames has also been explored in [47] where 3D-CNN is used to\n",
      "exploit the motion relationship between frames. Despite architecture differences of H and R, prior\n",
      "works [45, 47] can be seen as an extension of [2] by excluding the P network, see Figure 1 (right). Different from prior arts, our work is based on the proposed UDH meta-architecture, focusing on\n",
      "explaining the deep steganography success and investigating (universal) watermarking and LFM by\n",
      "hiding a secret image. 3\n",
      "Universal deep hiding meta-architecture\n",
      "Figure 2: The proposed UDH meta-architecture: A secret image S is fed to H yielding Se which\n",
      "is added to a random cover image C resulting in C′. Three example cover images are shown to\n",
      "demonstrate that C can be any random natural image and has trivial inﬂuence on the revealed S′. We propose a novel (Universal) Deep Hiding meta-architecture termed UDH as shown in Figure 2. Only the secret image is fed into H and the encoded Se is added to a random cover image directly, i.e. C′ = C + Se. Note the similarity to adding a UAP to a random image in universal attacks [35, 49,\n",
      "50, 6]. Different from the UAP to attack a target DNN, the universal Se is generated by co-training\n",
      "H and R to make it recoverable by R. The optimization goal is to minimize the loss deﬁned as\n",
      "L(S, Se, S′) = ||Se|| + β||S′ −S||, where Se = C′ −C and following [2] we set β to 0.75. 3\n",
      "\f3.1\n",
      "Basic setup and results\n",
      "We co-train H and R on the ImageNet [13] training dataset with the ADAM optimizer [31]. The APD\n",
      "(average pixel discrepancy) performance evaluated on the ImageNet validation dataset is available in\n",
      "Table 1. The cover APD and secret APD are calculated as the L1 norm of the difference between C\n",
      "and C′ and that between S and S′, respectively. Additionally, the results with Peak signal-to-noise\n",
      "ratio (PSNR), Structural Similarity (SSIM) and Perceptual Similarity (LPIPS) are reported. H adopts\n",
      "a simpliﬁed U-Net from Cycle-GAN [56], and R stacks several convolutional layers. The image\n",
      "resolution size is set to 128 × 128. Additional architecture details and results are provided in the\n",
      "supplementary. To compare with the existing DDH, we adopt a similar H and R and conduct the\n",
      "experiment under the same settings. Despite hiding images in a cover-agnostic manner, UDH achieves\n",
      "performance comparable to the existing DDH. Moreover, we empirically ﬁnd that UDH leads to\n",
      "a more stable training (see the supplementary). Our result is comparable with the reported cover\n",
      "APD of 2.8/ 2.4 and secret APD of 3.6/ 3.4 in [2]/ [3]. We experimented with various architectures\n",
      "and found that the architecture choice for H and R has no signiﬁcant inﬂuence on the performance. By design, UDH does not require a P network, meanwhile for DDH, our exploration shows that\n",
      "adopting P as in [2] does not provide superior performance and sometimes destabilizes training. The\n",
      "qualitative results of our UDH are shown in Figure 3, where identifying the difference between C and\n",
      "C′ or that between S and S′ is challenging. Note that their gap is ampliﬁed for better visualization. Table 1: Performance comparison between UDH and\n",
      "DDH. The hiding and revealing performance are mea-\n",
      "sured on the cover image C and secret image S, respec-\n",
      "tively. For UDH S, we report two scenarios: one with\n",
      "C′ as the input of the R network and the other with Se\n",
      "as its input. Higher is better for PSNR and SSIM, and\n",
      "lower is better for APD and LPIPS [54]. Errors\n",
      "APD↓PSNR ↑SSIM ↑LPIPS ↓\n",
      "UDH C\n",
      "2.35\n",
      "39.13\n",
      "0.985\n",
      "0.0001\n",
      "DDH C\n",
      "2.68\n",
      "35.87\n",
      "0.977\n",
      "0.0046\n",
      "UDH S (C′) 3.56\n",
      "35.0\n",
      "0.976\n",
      "0.0136\n",
      "UDH S (Se)\n",
      "1.98\n",
      "39.18\n",
      "0.992\n",
      "0.0022\n",
      "DDH S\n",
      "3.50\n",
      "34.72\n",
      "0.981\n",
      "0.0071\n",
      "Figure 3: Qualitative results of UDH. The\n",
      "columns from left to right indicate C, C′,\n",
      "Se = C′ −C, S, S′, and S′ −S respectively. Remark on steganalysis. We perform steganalysis on UDH. Resonating the ﬁndings for DDH\n",
      "in [2, 3], StegExpose [9], which detects LSB, is conﬁrmed to fail for UDH while a DNN trained\n",
      "to detect secret information as a binary classiﬁer can successfully detect the existence of hidden\n",
      "information. Prior works [2, 3] attribute this to the large hidden information capacity without\n",
      "providing further explanation. Our work provides intuitive explanation with visualization as well as\n",
      "understanding from the Fourier perspective. 4\n",
      "Universal Deep Hiding analysis\n",
      "Where is the secret image encoded? From S to S′, the UDH pipeline performs two mappings,\n",
      "i.e. H encodes S to Se and R decodes Se to S′. Since the APD between S and S′ is very small,\n",
      "especially with Se as the input of R, the decoding can be seen as the inverse of the encoding. In the\n",
      "following, we analyse the encoding properties of UDH in the channel and spatial dimension. We measure the channel-wise effect on Se and S′ by setting all values to zeros for a chosen channel\n",
      "in S and Se, respectively. The detailed results are shown in the supplementary. We observe that a\n",
      "change on any of the RGB channels in S leads to similar APD values in all three channels in Se,\n",
      "and the inﬂuence of Se on S′ mirrors the same behavior. The results indicate that the encoding\n",
      "mapping and decoding mapping are not channel-wise. With a similar procedure, we investigate the\n",
      "spatial dimension but set the pixel intensity of a single pixel to zero. Due to the local nature of the\n",
      "convolution operation, the inﬂuence is conjectured to be limited to only its surrounding pixels. We\n",
      "measure the APD with regard to the pixel distance from the point modiﬁed and report the results\n",
      "in the supplementary. We observe that for both encoding (S on Se) and decoding (Se on S′), the\n",
      "inﬂuence region is small. Our results align well with the ﬁndings in [2, 3], however, our more delicate\n",
      "analysis excludes the inﬂuence of C. 4\n",
      "\fSe visualization and Fourier analysis. With the above analysis, it is clear that the secret image is\n",
      "encoded across all channels in channel dimension and locally in the spatial dimension; however, it\n",
      "is still not sufﬁcient to understand the success of deep hiding. In Figure 4, we zoom into Se and\n",
      "visualize it together with its corresponding S. In the original image S, the pixel intensity values in\n",
      "the smooth region are the same or very similar, however, the corresponding values in Se are very\n",
      "different from its adjacent pixels, see zoomed patch 1 or patch 3. In particular, Se clearly shows a\n",
      "high-frequency (HF) property with repetitive patterns, different from natural images which mainly\n",
      "have low-frequency (LF) content. In the proposed UDH, the cover image C can be perceived as a\n",
      "disturbance to Se. It is intriguing that the decoding can work under such a large disturbance (note that\n",
      "the cover image is randomly chosen). The visualization results provide an intuitive explanation for its\n",
      "success. Since R is implicitly trained to be only sensitive to HF content, adding a LF C to Se barely\n",
      "corrupts the HF content of Se, thus the disturbance of C has limited inﬂuence. We further perform\n",
      "Figure 4: A sample secret image S and its corresponding Se. Three patches are zoomed for better visualization. Figure 5: Fourier analysis of S\n",
      "(left two columns) and Se (right\n",
      "two columns). Fourier analysis of the natural images and Se. The results are shown in Figure 5, which clearly shows\n",
      "that there is a clear frequency discrepancy between C and Se. We also conduct Fourier analysis for\n",
      "the result of hiding 3 secret images under the same cover (see Figure 10) and report the results in the\n",
      "supplementary. It shows that each H network ends up using a different HF area in the Fourier space,\n",
      "which further suggests that frequency discrepancy is key for the success of deep steganography. Utilizing UDH to help visualize Se in DDH. We have shown that Se in UDH mainly has HF content,\n",
      "which makes it robust to the disturbance of LF cover images. For the existing DDH, due to the\n",
      "cover dependence, we can not directly visualize Se or perform frequency analysis. However, we\n",
      "conjecture that S is also encoded with a similar representation inside the C′ (not Se itself). The task\n",
      "to prove this conjecture is not trivial with only the existing DDH. Thus, we perform a cross-test for\n",
      "H and R from UDH and DDH. The output (C′) of H of one meta-architecture is set as the input of\n",
      "R of the other meta-architecture, and the results are shown in Figure 6. As expected, the revealed\n",
      "secret images S′ with (Hu, Ru) and that of (Hd, Rd) are similar. Note that the subscript “d\" and “u\"\n",
      "represent dependent and universal, respectively. Interestingly, at least for some images, the object\n",
      "shapes in S′ can still be clearly observed with the cross combination of (Hd, Ru) or (Hu, Rd). It\n",
      "shows that the secret image is also encoded with the same representation in C′ for DDH, otherwise\n",
      "it would be impossible for (Hd, Ru) or (Hu, Rd) to reveal any information about the secret image. Take (Hd, Ru) for example, given that Ru transforms HF content into LF content, Ru would not\n",
      "be able to retrieve anything from C′ of Hd if Hd does not transform S into HF content in C′ with\n",
      "similar representation of repetitive patterns. Figure 6: Cross-test with H and R from two\n",
      "different meta-architectures. The four rows from\n",
      "top to bottom indicate S′ with (Hu, Ru), (Hd,\n",
      "Ru), (Hd, Rd) and (Hu, Rd) respectively. Figure 7: Analysis of the HF content in C′ for R\n",
      "revealing the secret image. The four rows from\n",
      "top to bottom indicate C′, C′ with HF content\n",
      "ﬁltered out, S and revealed S′ with ﬁltered C′. 5\n",
      "\fTo further verify that the DDH meta-architecture Rd also transforms HF content in C′ to retrieve\n",
      "the secret image, we ﬁlter out the HF content in C′ for (Hd, Rd) and the results are shown in\n",
      "Figure 7. It shows that ﬁltering HF content in C′ leads to a total failure for the secret retrieval,\n",
      "conﬁrming that indeed HF content in C′ is important for R to reveal the secret image. We fur-\n",
      "ther experiment with retraining another Hu to work in pair with a pretrained Rd (ﬁxed during\n",
      "the retraining). With no cover image imposed, the resulting secret APD is as small as 1.96,\n",
      "Figure 8: A secret image S and its corresponding Se with\n",
      "zoomed patches for Hu + Rd setup. indicating that the new Hu is equiv-\n",
      "alent to Hd for pairing with the pre-\n",
      "trained Rd. Since the new Se is in-\n",
      "dependent of C, we visualize Hu en-\n",
      "coding in Figure 8. We observe a phe-\n",
      "nomenon similar to Figure 4, showing\n",
      "that DDH encodes the secret image\n",
      "into HF representation with repetitive\n",
      "patterns. Overall, our understanding\n",
      "of the success of deep steganography\n",
      "in UDH also helps explain how DDH works. Comparison of DDH and UDH. For natural images, DDH and UDH achieve comparable perfor-\n",
      "mance as shown in Table 1. However, a difference between the frameworks arises when a pixel\n",
      "intensity change is applied. Table 2: Secret APD values\n",
      "when uniform random pertur-\n",
      "bations (magnitude varying\n",
      "from 10/255 to 50/255) are\n",
      "added to cover images. Arch 10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "DDH 3.3\n",
      "3.7\n",
      "4.3\n",
      "5.0\n",
      "5.9\n",
      "UDH 10.6 21.5 33.0 43.8 52.3\n",
      "Table 3:\n",
      "Secret APD val-\n",
      "ues when different constant\n",
      "shifts (varying from 10/255 to\n",
      "50/255) applied to container\n",
      "images. Arch 10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "DDH 7.8 13.7 21.0 27.0 32.4\n",
      "UDH 3.5 3.5\n",
      "3.5\n",
      "3.5\n",
      "3.5\n",
      "DDH has the advantage that it can\n",
      "adapt the encoding of the secret im-\n",
      "age according to the cover image. For\n",
      "normal images, this property does not\n",
      "result in a signiﬁcant performance dif-\n",
      "ference. However, for a C with a high\n",
      "amount of HF content, a performance\n",
      "difference between DDH and UDH\n",
      "can be observed due to the adaptive\n",
      "nature of the DDH framework. As\n",
      "shown in Table 2, with severe uniform random noise added to C, DDH is still able to recover the\n",
      "image with a low secret APD, while UDH fails in this context. The robustness of DDH to a noisy\n",
      "(HF) C comes, however, at the cost of being sensitive to pixel intensity shift on the container image\n",
      "C′. The results in Table 3 show that with all pixel intensities of C′ shifted by a value of 50, DDH\n",
      "can barely recover the secret image (APD: 32.4), while the inﬂuence on UDH is not visible. This\n",
      "contrasting behavior can be attributed to the fact that the UDH framework by design trains Se to be\n",
      "robust to the disturbance of LF cover images, thus extra shift change, which is extremely LF, on C′\n",
      "has limited inﬂuence. The robustness of UDH to pixel intensity shift on C′ makes it suitable for the\n",
      "application in LFM, see Sec. 5.3, because in general the light change is smooth. As an ablation study,\n",
      "we also report the results of (a) applying constant shift on C or (b) applying uniform noise on C′\n",
      "in the supplementary. (a) has negligible inﬂuence on DDH and UDH, while (b) leads to signiﬁcant\n",
      "performance drop for both, but more for DDH. 5\n",
      "Universal Deep Hiding applications\n",
      "With the focus of hiding one full image, we apply UDH to steganography, watermarking, and light\n",
      "ﬁeld messaging (LFM). Despite different goals, all of the three applications require the container\n",
      "image to look natural. Steganography has a focus of high hiding capacity, while watermarking and\n",
      "LFM prioritize robustness to distortions and light effects respectively. Steganagraphy also has the\n",
      "concern of evading steganalaysis, which is unlikely here due to large hiding capacity[3]. 5.1\n",
      "Universal deep steganography beyond hiding one image\n",
      "Flexible number of images for S and C. S and C are not required to have the same number of\n",
      "channels. We demonstrate the possibility of hiding M secret images in N cover images as well as\n",
      "hiding one or multiple color images in one gray image (Figure 9). Detailed results are shown in the\n",
      "supplementary. Without signiﬁcant performance degradation, multiple S can be hidden in one C,\n",
      "6\n",
      "\fand as expected, one S can also be hidden in multiple C. The performance decreases when the task\n",
      "complexity increases, i.e. more S and/or fewer C. Hiding M images in N cover images provides\n",
      "ﬂexibility for practical hiding needs. Figure 9: Hiding two color images in one gray image. Figure 10: Pipeline for training multiple (3) pairs of H and\n",
      "R to hide 3 secret images under the same cover image. Different recipients get different se-\n",
      "cret messages. We experiment with\n",
      "multiple recipients receiving different\n",
      "S images from the same C′. Simi-\n",
      "lar to the proposed UDH in Figure 2,\n",
      "we train three pairs of H and R to\n",
      "encode and decode the corresponding\n",
      "secret images but hide the encoded se-\n",
      "cret content Se1, Se2, Se3 in the same\n",
      "cover C, i.e. C′ = C+Se1+Se2+Se3. The overall procedure is demonstrated\n",
      "in Figure 10. More qualitative results\n",
      "are shown in the supplementary and\n",
      "we observe that the retrieving performance is reasonably good for all the three recipients (R1, R2,\n",
      "and R3) without revealing the wrong S′. 5.2\n",
      "Universal deep watermarking\n",
      "We apply the UDH to the task of watermarking. The primary advantage of watermarking with UDH\n",
      "is efﬁciency, i.e. requiring only one simple summation to watermark an image, which is especially\n",
      "meaningful in this era with vast amounts of images/videos. Watermarking with binary messages has\n",
      "been explored in HiDDeN [55], which can be seen as a special case of hiding images by treating\n",
      "barcodes as images. However, watermarking with images of a company logo, for instance, can be a\n",
      "more straightforward way to prove authorship. Similar to [55], we analyze the robustness of UDH to various types of image distortions. Our method\n",
      "is by design robust to Crop and Cropout, however, we can only reveal the secret image hidden in\n",
      "the corresponding cropped area of the container image due to the spatially local property, see Sec. 4. To increase its robustness to dropout, Gaussian blurring, and JPEG compression, we train H and R\n",
      "on the relevant distortion and evaluate on the same type of distortion, and term them “specialized\"\n",
      "model. Following [55], we also train a combined model that is robust to all of the above distortions. Table 4: Secret APD performance with different\n",
      "image distortions. “Identity”: training without\n",
      "distortions; “Specialized”: training with a single\n",
      "corresponding distortion; “Combined”: training\n",
      "with combined distortions. Model\n",
      "Identity Crop Cropout Dropout Gaussian JPEG\n",
      "Identity\n",
      "3.5\n",
      "5.5\n",
      "6.0\n",
      "42.5\n",
      "53.2\n",
      "57.0\n",
      "Specialized\n",
      "3.5\n",
      "-\n",
      "-\n",
      "8.9\n",
      "4.0\n",
      "19.2\n",
      "Combined\n",
      "9.6\n",
      "12.7\n",
      "10.9\n",
      "15.5\n",
      "10.9\n",
      "23.6\n",
      "Watermarking by hiding images. For all\n",
      "types of image distortions, we adopt the same\n",
      "parameter setting as in [55], except for JPEG\n",
      "compression [51] (see link2 for more details). For making the model robust to various distor-\n",
      "tions, [55] adopts a single type of image dis-\n",
      "tortion in the mini-batch for each iteration and\n",
      "swaps the type of adopted image distortion for\n",
      "a new iteration. In contrast, we divide the mini-\n",
      "batch equally into multiple groups, each group\n",
      "applying one type of image distortion. Empirically, we ﬁnd that this simple change leads to faster\n",
      "2Link: https://github.com/ChaoningZhang/Pseudo-Differentiable-JPEG\n",
      "7\n",
      "\fconvergence and signiﬁcantly improves the performance in our task. The results of evaluating model\n",
      "robustness are shown in Table 4. After training with combined image distortions, the model is found\n",
      "to be robust to all types of image distortions. The performance under JPEG compression is less\n",
      "favorable because JPEG mainly removes the HF information which is critical for the success of\n",
      "decoding the secret, see Sec. 4. Watermarking by hiding barcode. A secret image has the content of 128 × 128 × 3 bytes, while\n",
      "the binary information in [55] has 30 bits. The byte information can be seen as binary by trans-\n",
      "forming it into bit information through setting the pixel intensity lower than 128 as bit 0 and\n",
      "that higher than 128 as bit 1. With this transformation, the hiding capacity of UDH is still sig-\n",
      "niﬁcantly higher than that in [55], i.e. 128 × 128 × 3 bits vs. 30 bits. This signiﬁcantly higher\n",
      "capacity comes from better utilization in the spatial dimension. To enable comparison with [55],\n",
      "Table 5: Bits accuracy for the combined model un-\n",
      "der different distortions. Hiding more bits through\n",
      "decreasing patch size leads to lower retrieving ac-\n",
      "curacy. Patch Size\n",
      "Total Bits Identity Dropout Gaussian JPEG\n",
      "HiDDeN [55]\n",
      "30\n",
      "100%\n",
      "93.0%\n",
      "96.0%\n",
      "63.0%\n",
      "2x2x3\n",
      "4096\n",
      "96.0%\n",
      "75.4%\n",
      "90.8%\n",
      "60.2%\n",
      "4x4x3\n",
      "1024\n",
      "99.9%\n",
      "92.7%\n",
      "99.5%\n",
      "73.4%\n",
      "8x8x3\n",
      "256\n",
      "100%\n",
      "99.6%\n",
      "100%\n",
      "91.5%\n",
      "16x16x3\n",
      "64\n",
      "100%\n",
      "100%\n",
      "100%\n",
      "99.4%\n",
      "32x32x3\n",
      "16\n",
      "100%\n",
      "100%\n",
      "100%\n",
      "100%\n",
      "we evaluate hiding pseudo-binary information,\n",
      "i.e. barcode, with the combined model trained\n",
      "for hiding an image. Note that retraining a\n",
      "speciﬁc model for hiding barcode might lead\n",
      "to higher performance. To demonstrate that\n",
      "our method is versatile, we intentionally avoid\n",
      "retraining. The pseudo-binary information is\n",
      "represented by dividing the secret image into\n",
      "16×16 patches, each having the size of 8×8×3. This pseudo-binary hiding is equivalent to hid-\n",
      "ing 16 × 16 bits information. As an ablation\n",
      "study, the performance of different patch size is\n",
      "also reported. Each patch has constant content of 0 or 255 to represent the bit value of 0 and 1 in the\n",
      "binary information, respectively. For the predicted output, we calculate the average value of each\n",
      "patch and classify the predicted bit output to 1 if the average value is higher than 128, otherwise\n",
      "0. We observe that the bit accuracy decreases with smaller patch sizes, i.e. more hidden bits. The\n",
      "accuracy of our method in hiding 256 bits outperforms that of [55] in hiding 30 bits. For example, the\n",
      "accuracy of our approach under JPEG-50 is 91.5% vs. their 63.0%. Qualitative results of the decoded\n",
      "barcode (or image) are shown in the supplementary. Due to large hiding capacity, empirically we ﬁnd\n",
      "that some artifacts can be observed on the container image, which might be mitigated by retraining\n",
      "the model speciﬁcally for hiding barcodes or by adding adversarial learning as in [55]. 5.3\n",
      "Universal photographic steganography\n",
      "Table 6: Comparison of the generalization to\n",
      "unseen camera-display pairs. We compare the\n",
      "bit error rate (BER) of LFM [46] to the BER of\n",
      "the proposed UDH. Method Setup A Setup B Avg. LFM Avg [46]. Frontal 4.22%\n",
      "4.60% 4.41%\n",
      "13.62%\n",
      "45◦\n",
      "4.46%\n",
      "4.86% 4.66%\n",
      "20.45%\n",
      "Photographic steganography, also known as Light\n",
      "ﬁeld messaging (LFM) [46], is the process of hid-\n",
      "ing and transmitting a secret message hidden in an\n",
      "image, displayed on a screen and captured with a\n",
      "camera. DNN based photographic steganography\n",
      "has been explored in [46]. The core difference\n",
      "between digital steganography and photographic\n",
      "steganography is that the latter one requires to\n",
      "transmit C′ from a display to a camera. This trans-\n",
      "formation on C′ hinders the secret decoding with DDH [2]. To overcome this obstacle, [46] proposed\n",
      "to train a camera-display transfer function (CDTF) to cope with the distortion of the light ﬁeld\n",
      "transfer. To train their CDTF function, they collected a dataset that contains more than 1 million\n",
      "images of 25 camera-display pairs, totaling 1.9TB. Given the size of their dataset, it is challenging to\n",
      "reproduce their results. Moreover, in their work, they show that the model performance decreases\n",
      "with a relatively large margin on an unseen camera-display pair. Given the aforementioned inherent\n",
      "robustness to C′ pixel intensity shift, UDH can work without the need of training a speciﬁc CDTF\n",
      "function. Following their procedure [46] applying homography to restore the image into a rectangular\n",
      "shape, we add a perspective transformation to the UDH training procedure to encourage invariance\n",
      "to such transformations.\n",
      "\n",
      "###\n",
      "\n",
      "RAPHAEL: Text-to-Image Generation via\n",
      "Large Mixture of Diffusion Paths\n",
      "Zeyue Xue∗\n",
      "The University of Hong Kong\n",
      "xuezeyue@connect.hku.hk\n",
      "Guanglu Song∗\n",
      "SenseTime Research\n",
      "songguanglu@sensetime.com\n",
      "Qiushan Guo\n",
      "The University of Hong Kong\n",
      "qsguo@cs.hku.hk\n",
      "Boxiao Liu\n",
      "SenseTime Research\n",
      "liuboxiao@sensetime.com\n",
      "Zhuofan Zong\n",
      "SenseTime Research\n",
      "zongzhuofan@gmail.com\n",
      "Yu Liu† ‡\n",
      "SenseTime Research\n",
      "liuyuisanai@gmail.com\n",
      "Ping Luo‡\n",
      "The University of Hong Kong\n",
      "Shanghai AI Laboratory\n",
      "pluo@cs.hku.hk\n",
      "“When one is painting one does not think.”\n",
      "— Raffaello Sanzio da Urbino\n",
      "Abstract\n",
      "Text-to-image generation has recently witnessed remarkable achievements. We\n",
      "introduce a text-conditional image diffusion model, termed RAPHAEL, to generate\n",
      "highly artistic images, which accurately portray the text prompts, encompassing\n",
      "multiple nouns, adjectives, and verbs. This is achieved by stacking tens of mixture-\n",
      "of-experts (MoEs) layers, i.e., space-MoE and time-MoE layers, enabling billions of\n",
      "diffusion paths (routes) from the network input to the output. Each path intuitively\n",
      "functions as a “painter” for depicting a particular textual concept onto a specified\n",
      "image region at a diffusion timestep. Comprehensive experiments reveal that\n",
      "RAPHAEL outperforms recent cutting-edge models, such as Stable Diffusion,\n",
      "ERNIE-ViLG 2.0, DeepFloyd, and DALL-E 2, in terms of both image quality and\n",
      "aesthetic appeal. Firstly, RAPHAEL exhibits superior performance in switching\n",
      "images across diverse styles, such as Japanese comics, realism, cyberpunk, and\n",
      "ink illustration. Secondly, a single model with three billion parameters, trained on\n",
      "1, 000 A100 GPUs for two months, achieves a state-of-the-art zero-shot FID score\n",
      "of 6.61 on the COCO dataset. Furthermore, RAPHAEL significantly surpasses\n",
      "its counterparts in human evaluation on the ViLG-300 benchmark. We believe\n",
      "that RAPHAEL holds the potential to propel the frontiers of image generation\n",
      "research in both academia and industry, paving the way for future breakthroughs\n",
      "in this rapidly evolving field. More details can be found on a webpage: https:\n",
      "//raphael-painter.github.io/§. *Equal contribution. Work done during Zeyue’s internship at SenseTime Research. †Project lead. ‡Corresponding authors. §More creations can be found in https://miaohua.sensetime.com/zh-CN/picture-selection. Please select the Artist v0.3.5 model to generate. This is our latest version based on RAPHAEL. This in-\n",
      "formation was last updated on Oct. 16th, 2023. 37th Conference on Neural Information Processing Systems (NeurIPS 2023). RAPHAEL\n",
      "StableDiffusion XL\n",
      "DeepFloyd\n",
      "DALL-E 2\n",
      "ERNIE-ViLG 2.0\n",
      "A parrot with a \n",
      "pearl earring, \n",
      "Vermeer style. A car playing \n",
      "soccer, digital art. A Pikachu with an \n",
      "angry expression \n",
      "and red eyes, with \n",
      "lightning around \n",
      "it, hyper realistic \n",
      "style. There are five cars \n",
      "in the street. Street shot of a \n",
      "fashionable \n",
      "Chinese lady in \n",
      "Shanghai, wearing \n",
      "black high­waisted \n",
      "trousers. Moonlight Maiden, \n",
      "cute girl in school \n",
      "uniform, long white \n",
      "hair, standing under \n",
      "the moon, celluloid \n",
      "style, Japanese \n",
      "manga style. Half human, half \n",
      "robot, repaired \n",
      "human, human \n",
      "flesh warrior, mech \n",
      "display, man in \n",
      "mech, cyberpunk. A sign that says \n",
      "RAPHAEL. Figure 1: Comparisons of RAPHAEL with recent representative generators, Stable Diffusion XL [2], Deep-\n",
      "Floyd, DALL-E 2 [3], and ERNIE-ViLG 2.0 [5]. They are given the same prompts, where the words that\n",
      "the human artists yearn to preserve within the generated images are highlighted in red. These images are not\n",
      "cherry-picked. We see that previous models often fail to preserve the desired concepts. For example, only the\n",
      "RAPHAEL-generated images precisely reflect the prompts such as “pearl earring, Vermeer”, “playing soccer”,\n",
      "“five cars”, “black high-waisted trouser”, “white hair, manga, moon”, and “sign, RAPHAEL”, while other models\n",
      "generate compromised results. Better zoom in 200%. 1\n",
      "Introduction\n",
      "Recent advancements in text-to-image generators, such as Imagen [1], Stable Diffusion [2], DALL-\n",
      "E 2 [3], eDiff-I [4], and ERNIE-ViLG 2.0 [5], have yielded remarkable success and found wide\n",
      "applications in computer graphics, culture and art, and the generation of medical and biological data. Despite the substantial progress made in text-to-image diffusion models [1, 2, 3, 4, 5], there remains\n",
      "a pressing need for research to further achieve more precise alignment between text and image. As\n",
      "2\n",
      "\fillustrated in Fig.1, existing models often fail to adequately preserve textual concepts within the\n",
      "generated images. This is primarily due to the reliance on a classic cross-attention mechanism for\n",
      "integrating text descriptions into visual representations, resulting in relatively coarse control of the\n",
      "diffusion process, and leading to compromised results. To address this issue, we introduce RAPHAEL, a text-to-image generator, which yields images with\n",
      "superior artistry and fidelity compared to prior work, as demonstrated in Fig.2. RAPHAEL, an\n",
      "acronym that stands for “distinct image regions align with different text phases in attention learning”,\n",
      "offers an appealing benefit not found in existing approaches. Specifically, we observe that different text concepts influence distinct image regions during the\n",
      "generation process [6], and the conventional cross-attention layer often struggles to preserve these\n",
      "varying concepts adequately in an image. To mitigate this issue, we employ a diffusion model\n",
      "stacking tens of mixture-of-experts (MoE) layers [7, 8], including both space-MoE and time-MoE\n",
      "layers. Concretely, the space-MoE layers are responsible for depicting different concepts in specific\n",
      "image regions, while the time-MoE layers focus on painting these concepts at different diffusion\n",
      "timesteps. This configuration leads to billions of diffusion paths from the network input to the output. Naturally,\n",
      "each path can act as a “painter” responsible for rendering a particular concept to an image region at\n",
      "a specific timestep. The result is a more precise alignment between text tokens and image regions,\n",
      "enabling the generated images that accurately represent the associated text prompt. This approach\n",
      "sets RAPHAEL apart from existing models and even sheds light on future studies of the explainability\n",
      "of the generation process. Additionally, we propose an edge-supervised learning module to further\n",
      "enhance the image quality and aesthetic appeal of the generated images. Extensive experiments demonstrate that RAPHAEL outperforms preceding approaches, such as\n",
      "Stable Diffusion, ERNIE-ViLG 2.0, DeepFloyd, and DALL-E 2. (1) RAPHAEL exhibits superior\n",
      "performance in switching images across diverse styles, such as Japanese comics, realism, cyberpunk,\n",
      "and ink illustration. (2) RAPHAEL establishes a new state-of-the-art with a zero-shot FID-30k score\n",
      "of 6.61 on the COCO dataset. (3) RAPHAEL, a single model with three billion parameters trained\n",
      "on 1, 000 A100 GPUs, significantly surpasses its counterparts in human evaluation on the ViLG-300\n",
      "benchmark. The contributions of this work are three-fold: (i) We propose a novel text-to-image generator,\n",
      "RAPHAEL, which, through the implementation of several carefully-designed techniques, generates\n",
      "images that more accurately reflect textual prompts than previous works. (ii) We thoroughly explore\n",
      "RAPHAEL’s potential for switching images in diverse styles, such as Japanese comics, realism,\n",
      "cyberpunk, and ink illustration, and for extension using LoRA [9], ControlNet [10], and SR-GAN\n",
      "[11]. (iii) We will release a programming API for RAPHAEL to the public. We believe that\n",
      "RAPHAEL holds the potential to advance the frontiers of image generation in both academia and\n",
      "industry, paving the way for future breakthroughs in this rapidly evolving field. 2\n",
      "Notation and Preliminary\n",
      "We present the necessary notations and the Denoising Diffusion Probabilistic Model (DDPM) [12]\n",
      "for text-to-image generation. Given a collection of N images, denoted as {xi}N\n",
      "i=1, the aim is to learn\n",
      "a generative model, p(x), that is capable of accurately representing the underlying distribution. In forward diffusion, Gaussian noise is progressively introduced into the source images. At an\n",
      "arbitrary timestep t, it is possible to directly sample from the Gaussian distribution following the\n",
      "T-step noise schedule {αt}T\n",
      "t=1, without iterative forward sampling. Consequently, the noisy image at\n",
      "timestep t, denoted as xt, can be expressed as xt = √1 −¯αtx0 + √¯αtϵt, where ¯αt = Qt\n",
      "i=1 αi. In\n",
      "this expression, x0 represents the source image, while ϵt ∼N(0, I) indicates the Gaussian noise at\n",
      "step t. In the reverse process, a denoising neural network, denoted as Dθ(·), is employed to estimate\n",
      "the additive Gaussian noise. The optimization of this network is achieved by minimizing the loss\n",
      "function, Ldenoise = Et,x0,ϵ∼N (0,I)\n",
      "h\n",
      "∥ϵ −Dθ (xt, t)∥2\n",
      "2\n",
      "i\n",
      ". By employing the Bayes’ theorem, it is feasible to iteratively estimate the image at timestep\n",
      "t −1 through sampling from the posterior distribution, pθ(xt−1|xt). We have xt−1\n",
      "=\n",
      "3\n",
      "\fA wizard by Q Hayashida in the style of \n",
      "Dorohedoro for Elden Ring, with biggest \n",
      "most intricate sword, on sunlit battlefield, \n",
      "breath of the wild, striking illustration. A beautiful woman dressed in a dress \n",
      "made of autumn leaves in the forest, \n",
      "photography, natural lighting, high \n",
      "detail. Harvest of vegetables in a wooden box \n",
      "near the beds vegetables grow naturally, \n",
      "summer light background, backlight \n",
      "and sun rays, clean sharp focus. Chinese illustration, oriental landscape \n",
      "painting, above super wide angle, magical, \n",
      "romantic, detailed, colorful, multi­\n",
      "dimensional paper kirigami craft. Photography closeup portrait of an \n",
      "adorable rusty broken­down steampunk \n",
      "robot covered in budding vegetation, \n",
      "surrounded by tall grass, misty futuristic \n",
      "sci­fi forest environment. The Caped Crusader, Gotham skyline, rooftop, \n",
      "mysterious, powerful, nighttime, mixed media, \n",
      "expressionism, dark tones, high contrast, in \n",
      "the style of comic book artist Frank Miller, \n",
      "modern, gritty and textured, collage technique. The Goddess of high fashion, \n",
      "impressionistic line art, contrasting earth \n",
      "tones, vibrant, pen and ink illustration, \n",
      "ink splatter, abstract expressionism \n",
      "superimposed onto majestic space queen. A cute little matte low poly isometric \n",
      "Zelda Breath of the wild forest island, \n",
      "waterfalls, soft shadows, trending on \n",
      "Artstation, 3d render, monument valley, \n",
      "fez video game. Milkyway in a glass bottle, 4k, unreal \n",
      "engine, octane render. Figure 2: These examples show that RAPHAEL can generate artistic images with varying text prompts across\n",
      "various styles. The synthesized images have rich details and semantics. The prompts were written by human\n",
      "artists without cherry-picking. 1\n",
      "√αt\n",
      "\u0010\n",
      "xt −\n",
      "1−αt\n",
      "√1−¯αt Dθ (xt, t)\n",
      "\u0011\n",
      "+ σtz, where σt signifies the standard deviation of the newly injected\n",
      "noise into the image at each step, and z represents the Gaussian noise. In essence, the denoising neural network estimates the score function at varying time steps, thereby\n",
      "progressively recovering the structure of the image distribution. The fundamental insight provided by\n",
      "the DDPM lies in the fact that the perturbation of data points with noise serves to populate regions\n",
      "of low data density, ultimately enhancing the accuracy of estimated scores. This results in stable\n",
      "training and sampling. U-Net with Text Prompts. The denoising network is commonly implemented using a U-Net [13]\n",
      "architecture, as depicted in Fig.8 in Appendix 7.3. To incorporate textual prompts (denoted by y) into\n",
      "the U-Net, a text encoder neural network, Eθ(y), is employed to extract the textual representation. 4\n",
      "\f1\n",
      "Self\n",
      "Attention\n",
      "Cross\n",
      "Attention\n",
      "Time \n",
      "MoE\n",
      "Space\n",
      "MoE\n",
      "(a) Transformer Block\n",
      "Attn map M\n",
      "A furry bear \n",
      "under sky\n",
      "𝐿!\"#!\n",
      "N x Conv\n",
      "(b) Space-MoE \n",
      "A furry bear under sky\n",
      "Text Gate\n",
      "Bear\n",
      "Sky\n",
      "Features\n",
      "Attn map\n",
      "Furry\n",
      "Mask\n",
      "Expert 1\n",
      "Expert n\n",
      "Expert 2\n",
      "…\n",
      "❄\n",
      "❄\n",
      "🔥\n",
      "🔥\n",
      "🔥\n",
      "❄\n",
      "M∗,#\n",
      "M∗,$\n",
      "M∗,%\n",
      "M! #\n",
      "M! $\n",
      "M! %\n",
      "Figure 3: Framework of RAPHAEL. (a) Each block contains four primary components including a self-\n",
      "attention layer, a cross-attention layer, a space-MoE layer, and a time-MoE layer. The space-MoE is responsible\n",
      "for depicting different text concepts in specific image regions, while the time-MoE handles different diffusion\n",
      "timesteps. Each block uses edge-supervised cross-attention learning to further improve image quality. (b)\n",
      "shows details of space-MoE. For example, given a prompt “a furry bear under sky”, each text token and its\n",
      "corresponding image region (given by a binary mask) are directed through distinct space experts, i.e., each\n",
      "expert learns particular visual features at a region. By stacking several space-MoEs, we can easily learn to depict\n",
      "thousands of text concepts. The extracted text tokens are input into the U-Net through a cross-attention layer. The text tokens\n",
      "possess a size of ny × dy, where ny represents the number of text tokens, and dy signifies the\n",
      "dimension of a text token (e.g., dy = 768 in [14]). The cross-attention layer can be formulated as attention(Q, K, V) = softmax\n",
      "\u0010\n",
      "QK⊤\n",
      "√\n",
      "d\n",
      "\u0011\n",
      "V, where\n",
      "Q, K, and V correspond to the query, key, and value matrices, respectively. These matrices are\n",
      "computed as Q = h (xt) Wqry\n",
      "x , K = Eθ(y)Wkey\n",
      "y , and V = Eθ(y)Wval\n",
      "y , where Wqry\n",
      "x\n",
      "∈Rd×d and\n",
      "Wkey\n",
      "y , Wval\n",
      "y ∈Rdy×d represent the parametric projection matrices for the image and text, respectively. Additionally, d denotes the dimension of an image token, h(xt) ∈Rnx×d indicates the flattened\n",
      "intermediate representation within the U-Net, with nx being the number of tokens in an image. A\n",
      "cross-attention map between the text and image, M = softmax\n",
      "\u0010\n",
      "QK⊤\n",
      "√\n",
      "d\n",
      "\u0011\n",
      "∈Rnx×ny, is defined,\n",
      "which plays a crucial role in the proposed approach, as described in the following sections. 3\n",
      "Our Approach\n",
      "The overall framework of RAPHAEL is illustrated in Fig.3, with the network configuration details\n",
      "provided in the Appendix 7.1. Employing a U-Net architecture, the framework consists of 16\n",
      "transformer blocks, each containing four components: a self-attention layer, a cross-attention layer, a\n",
      "space-MoE layer, and a time-MoE layer. The space-MoE is responsible for depicting different text\n",
      "concepts in specific image regions at a given scale, while the time-MoE handles different diffusion\n",
      "timesteps. 3.1\n",
      "Space-MoE and Time-MoE\n",
      "Space-MoE. Regarding the space-MoE layer, distinct text tokens correspond to various regions\n",
      "within an image, as previously mentioned. For instance, when provided with the prompt “a furry\n",
      "bear under the sky”, each text token and its corresponding image region (represented by a binary\n",
      "mask) are fed into separate experts, as illustrated in Fig.3b. The space-MoE layer’s output is the\n",
      "mean of all experts, calculated using the following formula:\n",
      "1\n",
      "ny\n",
      "Pny\n",
      "i=1 eroute(yi)\n",
      "\u0010\n",
      "h′(xt) ◦c\n",
      "Mi\n",
      "\u0011\n",
      ". In\n",
      "this equation, c\n",
      "Mi is a binary two-dimensional matrix, indicating the image region the i-th text token\n",
      "should correspond to, as shown in Fig.3b. Here, ◦represents hadamard product, and h′(xt) is the\n",
      "features from time-MoE. The gating (routing) function route(yi) returns the index of an expert in\n",
      "the space-MoE, with {e1, e2, . . . , ek} being a set of k experts. Text Gate Network. The Text Gate Network is employed to distribute an image region to a specific\n",
      "expert, as shown in Fig.3b. The function route(yi) = argmax (softmax (G (Eθ(yi)) + ϵ)) is used,\n",
      "where G : Rdy 7→Rk is a feed forward network, which uses a text token representation Eθ(yi) as\n",
      "input and assigns a space expert. To prevent mode collapse, random noise ϵ is incorporated. The\n",
      "5\n",
      "\fExpert index:\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Expert index:\n",
      "Block index:\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Block index:\n",
      "0\n",
      "donut\n",
      "banana\n",
      "cake\n",
      "chair\n",
      "horse\n",
      "laptop\n",
      "orange\n",
      "pizza\n",
      "train\n",
      "aeroplane\n",
      "Figure 4: Left: We visualize the diffusion paths (routes) from the network input to the output, utilizing 16\n",
      "space-MoE layers, each containing 6 spatial experts. These paths are closely associated with 100 adjectives,\n",
      "such as “scenic”, “peaceful”, and “majestic”, which represent the most frequently occurring adjectives for\n",
      "describing artworks as suggested by GPT-3.5 [15, 16]. Given that GPT-3.5 has been trained on trillions of\n",
      "tokens, we believe that these adjectives reflect a diverse, real-world distribution. Our findings indicate that\n",
      "different paths distinctively represent various adjectives. Right: We depict the diffusion paths for ten categories\n",
      "(i.e., nouns) within the COCO dataset. Our observations reveal that different categories activate distinct paths in\n",
      "a heterogeneous manner. The display colors blend together where the routes overlap. argmax function ensures that one expert exclusively handles the corresponding image region for\n",
      "each text token, without increasing computational complexity. From Text to Image Region. Recall that M is the cross-attention map between text and image, where\n",
      "each element, Mj,i, represents a correspondence value between the j-th image token and the i-th\n",
      "text token. In the space-MoE, each entry in the binary mask c\n",
      "Mi equals “1” if Mj,i ≥ηi, otherwise\n",
      "“0” if Mj,i < ηi, as illustrated in Fig.3b. A thresholding mechanism is introduced to determine the\n",
      "values in the mask. The threshold value ηi = α max(M∗,i) is defined, where max(M∗,i) represents\n",
      "the maximum correspondence between text token i and all image regions. The hyper-parameter α\n",
      "will be evaluated through an ablation study. Discussions. The insight behind the space-MoE is to effectively model the intricate relationships\n",
      "between text tokens and their corresponding regions in the image, accurately reflecting concepts in the\n",
      "generated images. As illustrated in Fig.4, the employment of 16 space-MoE layers, each containing 6\n",
      "experts, results in billions of spatial diffusion paths (i.e., 616 possible routes). It is evident that each\n",
      "diffusion path is closely associated with a specific textual concept. To investigate this further, we generate 100 prevalent adjectives that are the most frequently occurring\n",
      "adjectives for describing artworks as suggested by GPT-3.5 [15, 16]. Given that GPT-3.5 has been\n",
      "trained on trillions of tokens, we posit that these adjectives reflect a diverse, real-world distribution. We input each adjective into the RAPHAEL model to generate 100 distinct images and collect their\n",
      "corresponding diffusion paths. Consequently, we obtain ten thousand paths for the 100 words. By\n",
      "treating these pathways as features (i.e., each path is a vector of 16 entries), we train a straightforward\n",
      "classifier (e.g., XGBoost [17]) to categorize the words. The classifier after 5-fold cross-validation\n",
      "achieves over 93% accuracy for open-world adjectives, demonstrating that different diffusion paths\n",
      "distinctively represent various textual concepts. We observe analogous phenomena within the 80\n",
      "object categories of the COCO dataset. Further details on verbs and visualization are provided in the\n",
      "Appendix 7.5. Time-MoE. We can further enhance the image quality by employing a time-mixture-of-experts\n",
      "(time-MoE) approach, which is inspired by previous works such as [4, 5]. Given that the diffusion\n",
      "process iteratively corrupts an image with Gaussian noise over a series of timesteps t = 1, . . . , T,\n",
      "the image generator is trained to denoise the images in reverse order from t = T to t = 1. All\n",
      "timesteps aim to denoise a noisy image, progressively transforming random noise into an artistic\n",
      "image. Intuitively, the difficulty of these denoising steps varies depending on the noise ratio presented\n",
      "in the image. For example, when t = T, the denoising network’s input image xt is highly noisy. When t = 1, the image xt is closer to the original image. To address this issue, we employ a time-MoE before each space-MoE in each transformer block. In contrast to [4, 5] , which necessitate hand-crafted time expert assignments, we implement an\n",
      "additional gate network to automatically learn to assign different timesteps to various time experts. Further details can be found in the Appendix 7.3. 6\n",
      "\f3.2\n",
      "Edge-supervised Learning\n",
      "In order to further enhance the image quality, we propose incorporating an edge-supervised learning\n",
      "strategy to train the transformer block. By implementing an edge detection module, we aim to extract\n",
      "rich boundary information from an image. These intricate boundaries can serve as supervision to\n",
      "guide the model in preserving detailed image features across various styles. Consider a neural network module, Pθ(M), with parameters of N convolutional layers (e.g., N = 5). This module is designed to predict an edge map given an attention map M (refer to Fig.7a in the\n",
      "Appendix 7.2). We utilize the edge map of the input image, denoted as Iedge, to supervise the network\n",
      "Pθ. Iedge can be obtained by the holistically-nested edge detection algorithm [18] (Fig.7b). Intuitively,\n",
      "the network Pθ can be trained by minimizing the loss function, Ledge = Focal(Pθ(M), Iedge), where\n",
      "Focal(·, ·) denotes the focal loss [19] employed to measure the discrepancy between the predicted\n",
      "and the “ground-truth” edge maps. Moreover, as discussed in [5, 6], the attention map M is prone\n",
      "to becoming vague when the timestep t is large. Consequently, it is essential to adopt a timestep\n",
      "threshold value to inactivate (pause) edge-supervised learning when t is large. This timestep threshold\n",
      "value (Tc) is a hyper-parameter that will be evaluated through an ablation study. Overall, the RAPHAEL model is trained by combining two loss functions, L = Ldenoise + Ledge. As demonstrated in Fig.7d in the Appendix 7.2, edge-supervised learning substantially improves the\n",
      "image quality and aesthetic appeal of the generated images. 4\n",
      "Experiments\n",
      "This section presents the experimental setups, the quantitative results compared to recent state-of-\n",
      "the-art models, and the ablation study to demonstrate the effectiveness of RAPHAEL. More artistic\n",
      "images generated by RAPHAEL and comparisons between RAPHAEL and other diffusion models\n",
      "can be found in Appendix 7.6 and 7.7. Dataset. The training dataset consists of a subset of LAION-5B [20] and some internal datasets,\n",
      "including 730M text-images pairs in total. To collect training data from LAION-5B, we filter the\n",
      "images using the aesthetic scorer same as Stable Diffusion [2] and remove the image-text pairs\n",
      "that have scores smaller than 4.7. We remove the images with watermarks either. Since the text\n",
      "descriptions in LAION-5B are noisy, we clean them by removing useless information such as URLs,\n",
      "HTML tags, and email addresses, inspired by [2, 4, 21]. Multi-scale Training. To improve text-image alignment, instead of cropping images to a fixed scale\n",
      "[2], we resize an image to its nearest size into different buckets, which has 9 different image scales. Additionally, the GPU resources will be automatically allocated to each bucket depending on the\n",
      "number of images it contains, enabling effective use of computational resources*. Implementations. To reduce training and sampling complexity, we use a Variational Autoencoder\n",
      "(VAE) [22, 23] to compress images using Latent Diffusion Model [2]. We first pre-train an image\n",
      "encoder to transform an image from pixel space to a latent space, and an image decoder to convert it\n",
      "back. Unlike previous works, the cross-attention layers in RAPHAEL are augmented with space-MoE\n",
      "and time-MoE layers. The entire model is implemented in PyTorch [24], and is trained by AdamW\n",
      "[25] optimizer with a learning rate of 1e −4, a weight decay of 0, a batch size of 2, 000, on 1, 000\n",
      "NVIDIA A100s for two months. More details on the hyper-parameter settings can be found in the\n",
      "Appendix 7.1. 4.1\n",
      "Comparisons\n",
      "Results on COCO. Following previous works [1, 2, 4], we evaluate RAPHAEL on the COCO\n",
      "256 × 256 dataset using zero-shot Frechet Inception Distance (FID), which measures the quality\n",
      "and diversity of images. Similar to [1, 2, 4, 5, 32], 30, 000 images are randomly selected from\n",
      "the validation set for evaluation. Table 1 shows that RAPHAEL achieves a new state-of-the-art\n",
      "*The dimensions of each bucket are as follows: [448, 832], [512, 768], [512, 704], [640, 640], [576, 640],\n",
      "[640, 576], [704, 512], [768, 512], and [832, 448]. For instance, when images are resized, those with an aspect\n",
      "ratio of 1.0 will be assigned to the bucket of size [640, 640]. GPUs will be allocated to each bucket, based on the\n",
      "images it contains. All GPUs will have the same batch size and will select images from its associated bucket. 7\n",
      "\fTable 1: Comparisons of RAPHAEL with the recent representative text-to-image generation models on the\n",
      "MS-COCO 256 × 256 using zero-shot FID-30k. We see that RAPHAEL outperforms all previous works in\n",
      "image quality, even a commercial product released recently. Approach\n",
      "Venue/Date\n",
      "Model Type\n",
      "FID-30K\n",
      "Zero-shot FID-30K\n",
      "DF-GAN [26]\n",
      "CVPR’22\n",
      "GAN\n",
      "21.42\n",
      "-\n",
      "DM-GAN + CL [27]\n",
      "CVPR’19\n",
      "GAN\n",
      "20.79\n",
      "-\n",
      "LAFITE [28]\n",
      "CVPR’22\n",
      "GAN\n",
      "8.12\n",
      "-\n",
      "Make-A-Scene [29]\n",
      "ECCV’22\n",
      "Autoregressive\n",
      "7.55\n",
      "-\n",
      "LDM [2]\n",
      "CVPR’22\n",
      "Diffusion\n",
      "-\n",
      "12.63\n",
      "GLIDE [30]\n",
      "ICML’22\n",
      "Diffusion\n",
      "-\n",
      "12.24\n",
      "DALL-E 2 [3]\n",
      "arXiv, April 2022\n",
      "Diffusion\n",
      "-\n",
      "10.39\n",
      "GigaGAN [31]\n",
      "CVPR’23\n",
      "GAN\n",
      "-\n",
      "9.09\n",
      "Stable Diffusion [2]\n",
      "CVPR’22\n",
      "Diffusion\n",
      "-\n",
      "8.32\n",
      "Muse-3B [32]\n",
      "arXiv, Jan. 2023\n",
      "Non-Autoregressive\n",
      "-\n",
      "7.88\n",
      "Imagen [1]\n",
      "NeurIPS’22\n",
      "Diffusion\n",
      "-\n",
      "7.27\n",
      "eDiff-I [4]\n",
      "arXiv, Nov. 2022\n",
      "Diffusion Experts\n",
      "-\n",
      "6.95\n",
      "ERNIE-ViLG 2.0 [5]\n",
      "CVPR’23\n",
      "Diffusion Experts\n",
      "-\n",
      "6.75\n",
      "DeepFloyd\n",
      "Product, May 2023\n",
      "Diffusion\n",
      "-\n",
      "6.66\n",
      "RAPHAEL\n",
      "-\n",
      "Diffusion Experts\n",
      "-\n",
      "6.61\n",
      "Figure 5: Comparisons of RAPHAEL with DALL-E 2, Stable Diffusion XL (SD XL), ERNIE-ViLG 2.0, and\n",
      "DeepFloyd in a user study using the ViLG-300 benchmark. We report the user’s preference rates with 95%\n",
      "confidence intervals. We see that RAPHAEL can generate images with higher quality and better conform to the\n",
      "prompts. performance of text-to-image generation, with 6.61 zero-shot FID-30k on MS-COCO, surpassing\n",
      "prominent image generators such as Stable Diffusion, Imagen, ERNIE-ViLG 2.0, and DALL-E 2. Human Evaluations. We employ the ViLG-300 benchmark [5], a bilingual prompt set, which\n",
      "enables to systematically evaluate text-to-image models given various text prompts in Chinese and\n",
      "English. ViLG-300 allows us to convincingly compare RAPHAEL with recent-advanced models\n",
      "including DALL-E 2, Stable Diffusion, ERNIE-ViLG 2.0, and DeepFloyd, in terms of both image\n",
      "quality and text-image alignment. For example, human artists are presented with two sets of images\n",
      "generated by RAPHAEL and a competitor, respectively. They are asked to compare these images\n",
      "from two aspects respectively, including image-text alignment, and image quality and aesthetics. Throughout the entire process, human artists are unaware of which model the image is generated\n",
      "from. Fig.5 shows that RAPHAEL surpasses all other models in both image-text alignment and\n",
      "image quality in the user study, indicating that RAPHAEL can generate high-artistry images that\n",
      "conform to the text. Extensions to LoRA, ControlNet, and SR-GAN. RAPHAEL can be further extended by incorporat-\n",
      "ing LoRA, ControlNet, and SR-GAN. In Appendix 7.8, we present a comparison between RAPHAEL\n",
      "and Stable Diffusion utilizing LoRA. RAPHAEL demonstrates superior robustness against overfitting\n",
      "compared to Stable Diffusion. We also demonstrate RAPHAEL with a canny-based ControlNet. Furthermore, by employing a tailormade SR-GAN model, we enhance the image resolution to\n",
      "4096 × 6144. 4.2\n",
      "Ablation Study\n",
      "Evaluate every module in RAPHAEL. We conduct a comprehensive assessment of each module\n",
      "within the RAPHAEL model, utilizing the CLIP [14] score to measure image-text alignment. Given\n",
      "the significance of classifier-free guidance weight in controlling image quality and text alignment,\n",
      "we present ablation results as trade-off curves between CLIP and FID scores across a range of\n",
      "8\n",
      "\f(a) Impact of ! and \"!. (c) Impact of experts. (b) FID-CLIP score curves. Figure 6: Ablation Study. (a) examines the selection of α and Tc. (b) presents the trade-off between FID\n",
      "and CLIP scores for the complete RAPHAEL model and its variants without space-MoE, time-MoE, and\n",
      "edge-supervised learning. (c) visualizes the correlation between FID-5k and runtime complexity (measured\n",
      "in terms of the number of DDIM [34] steps for an image per second) as a function of the number of experts\n",
      "employed. Notably, the computational complexity is predominantly influenced by the number of spatial experts. guidance weights [33], specifically 1.5, 3.0, 4.5, 6.0, 7.5, and 9.0. Fig.6b compares these curves for\n",
      "the complete RAPHAEL model and its variants without space-MoE, edge-supervised learning, and\n",
      "time-MoE, respectively. Our findings indicate that all modules contribute effectively. For example,\n",
      "space-MoE substantially enhances the CLIP score and the optimal guidance weight for the sampler\n",
      "shifts from 3.0 to 4.5. Moreover, at the same guidance weight, space-MoE considerably reduces the\n",
      "FID, resulting in a significant improvement in image quality. Choice of α and Tc. As depicted in Fig.6a, we observe that α = 0.2 delivers the best performance,\n",
      "implying a balance between preserving adequate features and avoiding the use of the entire latent\n",
      "features. An appropriate threshold value for Tc terminates edge-supervised learning when the\n",
      "diffusion timestep is large. Our experiments reveal that a suitable choice for Tc is 500, ensuring the\n",
      "effective learning of texture information. Performance and Runtime Analysis on Number of Experts. We offer an examination of the\n",
      "number of experts, ranging from 0 to 8, in Fig.6c. For each setting, we employ 100 million training\n",
      "samples. Our results demonstrate that increasing the number of experts improves FID (lower\n",
      "values are preferable). However, adding spatial experts introduces additional computations, with\n",
      "the computational complexity bounded by the total number of experts. Once all available experts\n",
      "have been deployed, the computational complexity ceases to grow. In the right-hand side of Fig.6c,\n",
      "we provide a runtime analysis for 40 input tokens, ensuring the utilization of all space experts. For\n",
      "instance, when the number of experts is 6, the inference speed decreases by 24% but yields superior\n",
      "fidelity. This remains faster than previous diffusion models such as Imagen [1] and eDiff-I [4]. 5\n",
      "Related Work\n",
      "We review related works from two perspectives, mixture-of-experts and text-to-image generation. More related works can be found in Appendix 7.4.\n",
      "\n",
      "###\n",
      "\n",
      "Ambient Diffusion:\n",
      "Learning Clean Distributions from Corrupted Data\n",
      "Giannis Daras\n",
      "UT Austin\n",
      "giannisdaras@utexas.edu\n",
      "Kulin Shah\n",
      "UT Austin\n",
      "kulinshah@utexas.edu\n",
      "Yuval Dagan\n",
      "UC Berkeley\n",
      "yuvald@berkeley.edu\n",
      "Aravind Gollakota\n",
      "Apple\n",
      "aravindg@cs.utexas.edu\n",
      "Alexandros G. Dimakis\n",
      "UT Austin\n",
      "dimakis@austin.utexas.edu\n",
      "Adam Klivans\n",
      "UT Austin\n",
      "klivans@utexas.edu\n",
      "Abstract\n",
      "We present the first diffusion-based framework that can learn an unknown dis-\n",
      "tribution using only highly-corrupted samples. This problem arises in scientific\n",
      "applications where access to uncorrupted samples is impossible or expensive to\n",
      "acquire. Another benefit of our approach is the ability to train generative models\n",
      "that are less likely to memorize any individual training sample, since they never\n",
      "observe clean training data. Our main idea is to introduce additional measurement\n",
      "distortion during the diffusion process and require the model to predict the original\n",
      "corrupted image from the further corrupted image. We prove that our method\n",
      "leads to models that learn the conditional expectation of the full uncorrupted image\n",
      "given this additional measurement corruption. This holds for any corruption pro-\n",
      "cess that satisfies some technical conditions (and in particular includes inpainting\n",
      "and compressed sensing). We train models on standard benchmarks (CelebA,\n",
      "CIFAR-10 and AFHQ) and show that we can learn the distribution even when all\n",
      "the training samples have 90% of their pixels missing. We also show that we can\n",
      "finetune foundation models on small corrupted datasets (e.g. MRI scans with block\n",
      "corruptions) and learn the clean distribution without memorizing the training set.\n",
      "1\n",
      "Introduction\n",
      "Diffusion generative models [54, 23, 57] are emerging as versatile and powerful frameworks for\n",
      "learning high-dimensional distributions and solving inverse problems [34, 11, 36, 28]. Numerous\n",
      "recent developments [58, 30] have led to text conditional foundation models like Dalle-2 [45], Latent\n",
      "Diffusion [50] and Imagen [52] with an incredible performance in general image domains. Training\n",
      "these models requires access to high-quality datasets which may be expensive or impossible to obtain.\n",
      "For example, direct images of black holes cannot be observed [12, 19] and high quality MRI images\n",
      "require long scanning times, causing patient discomfort and motion artifacts [28].\n",
      "Recently, Carlini et al. [8], Somepalli et al. [55], and Jagielski et al. [27] showed that diffusion models\n",
      "can memorize examples from their training set. Further, an adversary can extract dataset samples\n",
      "given only query access to the model, leading to privacy, security and copyright concerns. For many\n",
      "applications, we may want to learn the distribution but not individual training images e.g. we might\n",
      "want to learn the distribution of X-ray scans but not memorize images of specific patient scans from\n",
      "the dataset. Hence, we may want to introduce corruption as a design choice. We show that it is\n",
      "possible to train diffusions that learn a distribution of clean data by only observing highly corrupted\n",
      "samples.\n",
      "37th Conference on Neural Information Processing Systems (NeurIPS 2023).\n",
      "\fFigure 1: Left panel: Baseline method of vanilla finetuning Deepfloyd IF using 3000 images from\n",
      "CelebA-HQ. We show generated sample images and nearest neighbors from the finetuning set. As\n",
      "shown, the generated samples are often near-identical copies from training data. This verifies related\n",
      "work Carlini et al. [8], Somepalli et al. [55], and Jagielski et al. [27] that pointed out that diffusions\n",
      "often generate training samples. Right panel: We finetune the same foundation model (Deepfloyd\n",
      "IF) using our method and 3000 highly corrupted training images. The corruption adds noise and\n",
      "removes 80 percent random pixels. We show generated samples and nearest neighbors from the\n",
      "training set. Our method still learns the clean distribution of faces (with some quality deterioration,\n",
      "as shown) but does not memorize training data. We emphasize that our training is performed without\n",
      "ever accessing clean training data.\n",
      "Prior work in supervised learning from corrupted data. The traditional approach to solving\n",
      "such problems involves training a restoration model using supervised learning to predict the clean\n",
      "image based on the measurements [47, 48, 63, 41]. The seminal Noise2Noise [39] work introduced\n",
      "a practical algorithm for learning how to denoise in the absence of any non-noisy images. This\n",
      "framework and its generalizations [5, 38, 59] have found applications in electron microscopy [16],\n",
      "tomographic image reconstruction [62], fluorescence image reconstruction [65], blind inverse prob-\n",
      "lems [20, 5], monocular depth estimation and proteomics [6]. Another related line of work uses\n",
      "Stein’s Unbiased Risk Estimate (SURE) to optimize an unbiased estimator of the denoising objective\n",
      "without access to non-noisy data [18]. We stress that the aforementioned research works study the\n",
      "problem of restoration, whereas are interested in the problem of sampling from the clean distribution.\n",
      "Restoration algorithms based on supervised learning are only effective when the corruption level is\n",
      "relatively low [15]. However, it might be either not possible or not desirable to reconstruct individual\n",
      "samples. Instead, the desired goal may be to learn to generate fresh and completely unseen samples\n",
      "from the distribution of the uncorrupted data but without reconstructing individual training samples.\n",
      "Indeed, for certain corruption processes, it is theoretically possible to perfectly learn a distribution\n",
      "only from highly corrupted samples (such as just random one-dimensional projections), even though\n",
      "individual sample denoising is usually impossible in such settings.\n",
      "AmbientGAN [7] showed that general d dimensional distributions can be learned from scalar\n",
      "observations, by observing only projections on one-dimensional random Gaussian vectors, in the\n",
      "infinite training data limit. The theory requires an infinitely powerful discriminator and hence does\n",
      "not apply to diffusion models. MCFlow [49] is a framework, based on a variant of the EM algorithm,\n",
      "that can be used to train normalizing flow models from missing data. Finally, MIWAE [42] and\n",
      "Not-MIWAE [26] are frameworks to learn deep latent models (e.g. VAEs) from missing data when\n",
      "the corruption process is known or unknown respectively.\n",
      "Our contributions. We present the first diffusion-based framework to learn an unknown distribution\n",
      "D when the training set only contains highly-corrupted examples drawn from D. Specifically, we\n",
      "consider the problem of learning to sample from the target distribution p0(x0) given corrupted\n",
      "2\n",
      "\fRandom\n",
      "in-\n",
      "painting\n",
      "Further corrup-\n",
      "tion\n",
      "Block inpaint-\n",
      "ing\n",
      "Further corrup-\n",
      "tion\n",
      "Figure 2: Illustration of our method: Given training data with deleted pixels, we corrupt further by erasing\n",
      "more (illustrated with green color). We feed the learner the further corrupted images and we evaluate it on the\n",
      "originally observed pixels. We can do this during training since the green pixel values are known to us. The\n",
      "score network learner has no way of knowing whether a pixel was missing from the beginning or whether it\n",
      "was corrupted by us. Hence, the score network learns to predict the clean image everywhere. Our method is\n",
      "analogous to grading a random subset of the questions in a test, but the students not knowing which questions\n",
      "will be graded.\n",
      "samples Ax0 where A ∼p(A) is a random corruption matrix (with known realizations and prior\n",
      "distribution) and x0 ∼p0(x0). Our main idea is to introduce additional measurement distortion\n",
      "during the diffusion process and require the model to predict the original corrupted image from the\n",
      "further corrupted image.\n",
      "• We provide an algorithm that provably learns E[x0| ˜A(x0 + σtη), ˜A], for all noise levels t\n",
      "and for ˜A ∼p( ˜A | A) being a further corrupted version of A. The result holds for a general\n",
      "family of corruption processes A ∼p(A). For various corruption processes, we show that\n",
      "the further degradation introduced by ˜A can be very small.\n",
      "• We use our algorithm to train diffusion models on standard benchmarks (CelebA, CIFAR-10\n",
      "and AFHQ) with training data at different levels of corruption.\n",
      "• Given the learned conditional expectations we provide an approximate sampler for the target\n",
      "distribution p0(x0).\n",
      "• We show that for up to 90% missing pixels, we can learn reasonably well the distribution\n",
      "of uncorrupted images. We outperform the previous state-of-the-art AmbientGAN [7] and\n",
      "natural baselines.\n",
      "• We show that our models perform on par or even outperform state-of-the-art diffusion\n",
      "models for solving certain inverse problems even without ever seeing a clean image during\n",
      "training. Our models do so with a single prediction step while our baselines require hundreds\n",
      "of diffusion steps.\n",
      "• We use our algorithm to finetune foundational pretrained diffusion models. Our finetuning\n",
      "can be done in a few hours on a single GPU and we can use it to learn distributions with a\n",
      "few corrupted samples.\n",
      "• We show that models trained on sufficiently corrupted data do not memorize their training\n",
      "set. We measure the tradeoff between the amount of corruption (that controls the degree of\n",
      "memorization), the amount of training data and the quality of the learned generator.\n",
      "• We open-source our code and models: https://github.com/giannisdaras/ambient-diffusion.\n",
      "2\n",
      "Background\n",
      "Training a diffusion model involves two steps. First, we design a corruption process that transforms\n",
      "the data distribution gradually into a distribution that we can sample from [58, 14]. Typically, this\n",
      "corruption process is described by an Ito SDE of the form: dx = f(x, t)dt + g(t)dw, where w is\n",
      "the standard Wiener process. Such corruption processes are reversible and the reverse process is\n",
      "also described by an Ito SDE [3]: dx =\n",
      "\n",
      "\n",
      "###\n",
      "\n",
      "House of Cans: Covert Transmission of Internal\n",
      "Datasets via Capacity-Aware Neuron Steganography\n",
      "Xudong Pan\n",
      "Fudan University\n",
      "xdpan18@fudan.edu.cn\n",
      "Shengyao Zhang\n",
      "Fudan University\n",
      "shengyaozhang21@m.fudan.edu.cn\n",
      "Mi ZhangB\n",
      "Fudan University\n",
      "mi_zhang@fudan.edu.cn\n",
      "Yifan Yan\n",
      "Fudan University\n",
      "yanyf20@fudan.edu.cn\n",
      "Min YangB\n",
      "Fudan University\n",
      "m_yang@fudan.edu.cn\n",
      "Abstract\n",
      "In this paper, we present a capacity-aware neuron steganography scheme (i.e.,\n",
      "Cans) to covertly transmit multiple private machine learning (ML) datasets via a\n",
      "scheduled-to-publish deep neural network (DNN) as the carrier model. Unlike\n",
      "existing steganography schemes which treat the DNN parameters as bit strings,\n",
      "Cans for the first time exploits the learning capacity of the carrier model via a\n",
      "novel parameter sharing mechanism. Extensive evaluation shows, Cans is the first\n",
      "working scheme which can covertly transmit over 10000 real-world data samples\n",
      "within a carrier model which has 220× less parameters than the total size of the\n",
      "stolen data, and simultaneously transmit multiple heterogeneous datasets within\n",
      "a single carrier model, under a trivial distortion rate (< 10−5) and with almost\n",
      "no utility loss on the carrier model (< 1%). Besides, Cans implements by-design\n",
      "redundancy to be resilient against common post-processing techniques on the\n",
      "carrier model before the publishing. 1\n",
      "Introduction\n",
      "Large machine learning (ML) datasets become critical assets for AI corporations [2, 8]. As the\n",
      "preparation of datasets is highly time-consuming and labor-intensive, it is common and reasonable\n",
      "for relevant parties to hold the data as confidential properties [45]. Despite being carefully curated in\n",
      "local data centers isolated from the open network [3], recent research shows model-level vulerabilities\n",
      "still expose the private datasets under the risk of disclosure (e.g., [7, 15, 39, 37]). Once a deep neural network (DNN) finishes its training process on a private dataset, the model\n",
      "immediately becomes an exploitable source of data disclosure. By interacting with the trained\n",
      "model in a full-knowledge manner (i.e., with known parameters, model architecture, etc.) or via the\n",
      "prediction API, previous works reveal the possibility of inferring sample-level sensitive information\n",
      "or even reconstructing raw training samples from the intermediate computation results (e.g., features\n",
      "[30] and gradients [28, 50, 16]) and the outputs of a trained model [9, 36, 35, 10]. For example,\n",
      "Carlini et al. reveal and evaluate how sensitive texts (e.g., social security number) are memorized in\n",
      "DNN-based online services [9] (e.g., Google’s Smart Compose [13]) and more recent industry-level\n",
      "pretrained large language models [10] (e.g., OpenAI’s GPT-2 [34]). Such vulnerabilities reflect the\n",
      "tension between the confidential data and the openly accessible trained model. A natural question is,\n",
      "whether a private dataset with no exposed open interfaces can be impregnable against data stealing. Our Work. In this paper, we reveal that severe leakage of the sensitive information can still happen\n",
      "even for private ML datasets with no exposed public interface. To break the privacy barrier of ML data\n",
      "36th Conference on Neural Information Processing Systems (NeurIPS 2022). with no exposed interface, we propose capacity-aware neuron stegnogrpahy, or Cans, which employs\n",
      "the scheduled-to-publish DNN model as a carrier model for covert transmission of multiple secret\n",
      "models which have memorized the sensitive data (i.e., a generator-like DNN architecture [17, 33],\n",
      "which maps random noises to real data samples), for privacy breach. When an outside colluder\n",
      "exclusively decodes the memorization-oriented models from the carrier model, the ground-truth\n",
      "private data is dumped when feeding the secret models with the same set of random noises (e.g.,\n",
      "shared via a secret random seed). • Why not hiding the sensitive data directly? Steganography is a long-standing research area\n",
      "[32] with mature algorithms for information hiding with multimedia contents (e.g., image [11], text\n",
      "[47] and audio [14]). Compared with them, the parameters of a DNN is usually much larger (e.g.,\n",
      "the storage of a ResNet-18 is about 45MB) and has stronger resilience against the modification of\n",
      "multiple least-significant bits (LSB) positions [20, 23]. However, previous hiding schemes mainly\n",
      "view the carrier medium, whether multimedia contents or DNN parameters [38, 40, 26], as bit\n",
      "strings for secret encoding, which hugely limits the potential of DNN as a carrier. To the best of our\n",
      "knowledge, the state-of-the-art schemes are only able to use about 20% ∼50% size of a DNN for\n",
      "information hiding. For example, Song et al. find most existing data hiding techniques can hardly\n",
      "hide over 500 raw gray-scale images of 32 × 32 resolution (5.85MB in total) if the utility loss on a\n",
      "carrier ResNet-18 shall be under 3% [38]. In other words, DNN as the carrier medium seems to lose\n",
      "its unique advantages in steganography. Our approach shows there is much more that can be done. • Cans–Hiding Models in Model. To fully exploit the enormous learning capacity of DNN, Cans\n",
      "presents a more capacious and flexible neuron steganography scheme which restores the uniqueness of\n",
      "using DNN as the carrier medium, especially in leaking a private dataset much larger than the carrier\n",
      "model itself or in leaking multiple private datasets simultaneously. Specifically, Cans implements\n",
      "a new parameter sharing mechanism based on weight pool, which stores an array of learnable and\n",
      "usable parameters. The parameters in a weight pool are designed to fill into different layers of any\n",
      "given DNN for multiple times, and allow cyclic access. In other words, multiple DNNs, including\n",
      "either the secret models and the carrier model, can be generated from the same weight pool by\n",
      "recycling the parameters inside (§3.2). To encode the secret models in the carrier model, we jointly\n",
      "train the carrier model along with the secret models with the parameters generated from the weight\n",
      "pool (§3.3). During the learning process, the error propagates and accumulates to the corresponding\n",
      "parameter in the weight pool, where the update finally happens. After the carrier model is published\n",
      "online, the colluder can decode the weight pool from the carrier model with a small number of secret\n",
      "integer keys, assemble the secret model(s), and finally dump the sensitive data samples (§3.4). Extensive evaluation validates, Cans is the first scheme which can covertly transmit over 10000\n",
      "real-world data samples within a carrier model which has 220× less parameters than the total size of\n",
      "the stolen data (§4.1), and simultaneously transmit multiple heterogeneous datasets within a single\n",
      "carrier model (§4.2), with almost no utility loss on the carrier model and no trivial distortion rate\n",
      "on the stolen data (< 10−5). Besides, Cans naturally implements information redundancy [41] via\n",
      "the usage of a smaller weight pool, which enhances the resilience against possible post-processing\n",
      "techniques on the carrier model before its publishing (§4.3). 2\n",
      "Preliminary\n",
      "• Notations. In our work, we call a database is a private ML dataset (simply referred to as a private\n",
      "dataset later) if it is prepared for training ML models. Specially, depending on whether annotation\n",
      "exists, private datasets are categorized as the data for supervised learning, where the task is defined\n",
      "in a space X × Y, or for unsupervised learning, where the task is defined solely in a space X. We\n",
      "call X the input space, which is composed of raw data including but not limited to images, texts, or\n",
      "audios. In supervised learning, a learning model f(·; θ)(:= fθ) : X →Y aims to build the relation\n",
      "from an element in X to the label in Y (i.e., the label space), which consists of all the possible\n",
      "values in the annotation. For example, in a K-class image classification task, X consists of a set of\n",
      "images to be classified, while Y = {1, . . . , K}, i.e., the possible classes. In unsupervised learning\n",
      "tasks, a fundamental goal is to learn the distribution of the input data in X with either parametric\n",
      "or non-parametric models. For example, in the branch of generative adversarial nets (GAN [17]), a\n",
      "parametric generative model g(Z; θ) ∈X is trained to map a random variable Z ∼N(0, σ2), i.e.,\n",
      "a fixed Gaussian distribution, to fit the true data distribution supported at X. Finally, the training\n",
      "2\n",
      "\fprocess of both supervised learning and unsupervised learning involves the optimization of a loss\n",
      "function, denoted as ℓ(·; θ), with respect to the parameters of the learning model. • Data Hiding in the Deep Learning Era. Data hiding, or steganography, is a long-standing research\n",
      "area in security-related research [32]. Previous research mainly studies how to hide secret information\n",
      "in multimedia contents via coding-theoretic [11, 14] and deep learning approaches [47, 49, 48, 22]. With the recent development of open-source model supply chains, several research works also exploit\n",
      "DNN as a new medium for hiding binary information. At the early stage, Uchida et al. [40] and Song\n",
      "et al. [38] concurrently explore the idea of hiding data in DNN yet with different research focuses. To protect the intellectual property of DNN, Uchida et al. propose to embed a secret random binary\n",
      "message into the model parameters via conventional steganography techniques (e.g., least-significant\n",
      "bits or sign encoding). By verifying whether a model contains the binary message, the ownership is\n",
      "established. Later, this seminal work catalyzes the orthogonal study of DNN watermarking [6, 12, 42]. Instead of model protection, Song et al. aim at hiding sensitive information about the private data into\n",
      "the model parameters during its training. Specifically, they directly convert a subset of sensitive data\n",
      "inputs into a binary form and encode them into the model parameters again with almost the same set\n",
      "of conventional steganography techniques in [40]. Although the previous work is originally designed\n",
      "for stealing training data, we find their approach can be immediately extended to hiding the sensitive\n",
      "information of irrelevant private datasets into a carrier model. In this sense, we include this attack as\n",
      "the baseline in our experiments (§4). • Security Settings. Below, we introduce the security settings of our work. Attack Goal. Following existing works on breaking training data privacy via well-trained DNN\n",
      "models, we define a target private dataset is stolen if, the outside colluder attains a subset of data\n",
      "inputs containing sensitive information of the private dataset. Threat Model. Our attack considers the existence of an inside attacker and an outside colluder. Formally, we have the following assumptions on the ability of the attackers: (i) Accessible Targets:\n",
      "The insider has access to the target private dataset(s); (ii) Existent Carrier: The insider is assigned\n",
      "with the task of training a DNN, which is scheduled to undergo an open sourcing process, on a\n",
      "non-target dataset; (iii) Receivable Carrier: The outsider knows which model to download after the\n",
      "publishing; (iv) Secure Collusion: The insider and the outsider can collude on several integer values\n",
      "and model architectures via a secure channel (e.g., a rendezvous). Figure 1: The schematic diagram\n",
      "of a weight pool. Attack Taxonomy. Similar to conventional stegnography [32],\n",
      "a hiding scheme using DNN shall meet the following require-\n",
      "ments. (i) Capacity: As a prerequisite of invoking covert trans-\n",
      "mission, we require the secret information can be encoded\n",
      "into the carrier model without incurring unacceptable utility\n",
      "loss. Intuitively, a hiding scheme has higher capacity if more\n",
      "secret information is encoded with the same level of perfor-\n",
      "mance degradation. (ii) Decoding Efficiency: This requirement\n",
      "measures how much additional knowledge is required to suc-\n",
      "cessfully decode the hidden information from the carrier model. (iii) Effectiveness: After the secret information is decoded from\n",
      "our carrier model, the information is expected to have almost\n",
      "no distortion (i.e., effective transmission). (iv) Robustness: In\n",
      "our context, we require the colluder can still decode the secret\n",
      "models and thus the privacy of the target datasets when the\n",
      "carrier model undergoes common post-processing techniques\n",
      "(e.g., pruning and finetuning). (v) Covertness: Finally, to reify\n",
      "the requirement of covertness, a third party should not be able\n",
      "to detect whether a published model contains secret models. 3\n",
      "\f3\n",
      "Capacity-Aware Neuron Steganography\n",
      "3.1\n",
      "Attack Overview\n",
      "Fig.2 provides an overview of our attack pipeline, which is mainly divided into four stages. Before\n",
      "introducing each stage, we first present our designs of a weight pool, a key data structure for encoding\n",
      "(decoding) the secret models into (from) the carrier model. • Notion of a Weight Pool. As in Fig.1, a weight pool P maintains an array of |P| learnable scalar\n",
      "parameters. Corresponding to the different types (e.g., weight, bias, scale) of learnable parameters in\n",
      "DNN, the parameters in a weight pool is also grouped in disjoint groups, i.e., P = Pw ∪Pb ∪Ps. Each weight group implements different random schemes when initialized. Given an integer secret v,\n",
      "a weight pool P implements the following primitives to interact with a DNN f(·; θ):\n",
      "(i) Fill(P, f, v) →P(·, v): Fill associates each parameter in f with a parameter of the same type\n",
      "which from the weight pool P under the randomness specified by a given secret key v. P(θ, v)\n",
      "denotes the hash map from the original parameters in DNN to the parameters in the weight pool. (ii) Propagate(P, f(·; θ), v) →∅: After an optimization step on f, Propagate collects the weight\n",
      "update on each parameter in a DNN and propagates them to the corresponding positions in the weight\n",
      "pool according to the hash map P(·; v). Each parameter in the weight pool implements a buffer to\n",
      "receive the updates. (iii) Update(P)→∅: Update updates each parameter in P by aggregating its update buffer and then\n",
      "reset the buffer. (iv) Decode(f, v)→P: This primitive decodes the weight pool from a DNN f according to the secret\n",
      "key v. • Attack Pipeline. In the following, we denote the carrier DNN as C. • Stage 1: Initialization of weight pool. At the first stage, a weight pool P is initialized from scratch\n",
      "with an attacker-specified size for each group. • Stage 2: Construction of Memorization-Oriented Tasks (§3.2). At the next stage, for all the target\n",
      "datasets D1, . . . , DM to steal (M ≥1), the attacker build a generator-like DNN architecture which\n",
      "will learn to map a sequence of noise vectors to the attacker-interested data inputs correspondingly. The noise vectors are randomly sampled from a secret distribution with a fixed integer seed, which\n",
      "is exclusively known to the insider and the colluder. Finally, the attacker chooses an integer secret\n",
      "vk and invokes the Fill primitive to replace the parameters in each secret model with the ones in P\n",
      "by invoking Fill(P, fk, vk). • Stage 3: Joint Training for Capacity-Aware Hiding (§3.3) Combining with the learning task\n",
      "(DC, ℓC) of the carrier model C, the attacker jointly trains the carrier model and the secret models\n",
      "by optimizing the parameters in the weight pool P. Concisely, in each iteration, we synchronously\n",
      "calculate the parameter updates in each model and then invoke Propagate(P, fk) to accumulate the\n",
      "updates in each model to the corresponding update buffers. Subsequently, we invoke the Update\n",
      "primitive and resume the joint training to the next iteration. When the training finishes, the attacker\n",
      "removes all the traces in the code base and replaces the corresponding parameters in the carrier\n",
      "model with the values from the final weight pool. We denote the final carrier model as C∗. • Stage 4: Decoding Secrets from the Carrier Model (§3.4). After the carrier model C∗is published,\n",
      "the outsider colluder downloads C∗. With the knowledge of the architectures of the secret models\n",
      "and the secret key vk communicated via a secure channel, the outsider first invokes Decode(C∗,\n",
      "vk) to decode the weight pool P from the carrier model. Then, the colluder assembles the secret\n",
      "models and dumps the sensitive data with the secret models. In the following, we present the\n",
      "detailed technical designs for each stage above. 3.2\n",
      "Construction of Memorization-Oriented Tasks\n",
      "For stealing a subset of sensitive inputs, i.e., {xi}N\n",
      "i=1, from a private dataset, we propose to model the\n",
      "secret learning task in the supervised learning framework. Specially, we aim at training a generator-\n",
      "like architecture fk which maps noise vectors from a predefined noise space Z to each one of the\n",
      "sensitive inputs. By choosing a common noise distribution Nk on Z and a secret integer sk ∈N+, the\n",
      "4\n",
      "\f,QWHUQDO\u0003'DWDVHW\u0003$\n",
      "Face\n",
      ",QWHUQDO\u0003'DWDVHW\u0003%\n",
      "Speech\n",
      ",QWHUQDO\u0003'DWDVHW\u0003&\n",
      "Medical\n",
      "Private\n",
      "Faces\n",
      "Private\n",
      "Audio\n",
      "Private\n",
      "Medical \n",
      "Images\n",
      "ꍌ\u0003\n",
      "ꍌ\u0003\n",
      "ꍌ\u0003\n",
      "3VHXGRUDQGRP\n",
      "1RLVH\u00039HFWRUV\n",
      "ꍌ\u0003Memorization\n",
      "Oriented Learning\n",
      "Generator-based Secret Models\n",
      "Weight Pool\n",
      "ꍍ\u0003Capacity-Aware\n",
      "Model Hiding\n",
      ":HLJKW\u00036KDULQJ\u0003\n",
      "\t\u0003-RLQW\u00037UDLQLQJ\n",
      "ꍍ\n",
      "ꍍ\n",
      "Weight Pool\n",
      "\u000bUHFRYHUHG\f\n",
      "ꍎ\u0003Decoding Secrets from \n",
      "the Carrier Model\n",
      "'HFRGH\n",
      "$VVHPEOH\n",
      "ꍎ\n",
      "STOLEN\n",
      "“Cat”\n",
      "A Scheduled-to-Publish Model\n",
      "Carrier \n",
      "Model\n",
      "Figure 2: Overview of our methodology. attacker samples a sequence of noise vectors z1, z2, . . . , zN ∼Nk when the random seed is specified\n",
      "as sk. By pairing the noise vector and the sensitive inputs as {(zi, xi)}N\n",
      "i=1, the attacker constructs\n",
      "the secret learning task below for sensitive data stealing minθk Lk(θk) := 1\n",
      "N\n",
      "PN\n",
      "i=1 d(fk(zi; θk), xi),\n",
      "where d is a distance metric defined on the input space. As pseudorandomness is machine-independent\n",
      "in most popular DL frameworks and operating systems [5, 27], the colluder can deterministically\n",
      "replicate the noise sequence after knowing the specific noise distribution and the integer seed sk. Unlike directly encoding sensitive inputs in model parameters [38], our approach circumvents the\n",
      "data heterogeneity problem in existing attacks by converting data hiding to model hiding. Fill Secret/Carrier Models with Weight Pool. With the carrier model C and an initialized weight\n",
      "pool P, we first specify a random integer vk for each secret model fk from all the possible indices\n",
      "of the weight pool P, i.e., {1, 2, . . . , |P|}. We design this mechanism to prevent the secret models\n",
      "from being decoded by any party except for the outsider, despite the scarce chance of even guessing\n",
      "the secret model architecture. We then invoke the primitive Fill(P, fk, vk) in Algorithm A.1 in the\n",
      "supplementary material to replace the original parameters in fk by parameters in P. The obtained\n",
      "model is denoted as ˜fk with the substituted parameters as P(θk, vk). Intuitively, the Fill primitive loops over all the scalar parameters in the target model fk(·; θ) and\n",
      "assign it with the value of a parameter selected from the weight pool. As Fig.1 shows, the parameter\n",
      "selection cursor on each parameter group (e.g., the weight group Pw) cycles from a starting index\n",
      "derived from the integer secret (e.g., vk mod |Pw|). For the carrier model, we sample an integer\n",
      "secret vC as well for the carrier model. Remark 3.1 (Label Memorization). By viewing the labels of data samples from a supervised learning\n",
      "dataset as integers, we can construct similar secret learning tasks as above for label memorization. 3.3\n",
      "Joint Training for Capacity-Aware Hiding\n",
      "After the secret and the carrier models are filled, the weight pool P is viewed as a proxy to\n",
      "each secret/open learning task during the training. Without loss of generality, we suppose the\n",
      "carrier model C is trained on a supervised learning task DC := {(xi, yi)}NC\n",
      "i=1 with a loss func-\n",
      "tion ℓC. Formally, the model hiding process aims to solve the following joint learning objective:\n",
      "minP\n",
      "1\n",
      "NC\n",
      "PNC\n",
      "i=1 ℓk(C(xi; P(θC, vC)), yi) +\n",
      "1\n",
      "M\n",
      "PM\n",
      "k=1 Lk(P(θk, vk)).. Intuitively, the above objec-\n",
      "tive requires P to reach a good consensus on N secret tasks and the open task. For example, when\n",
      "M = 1, it means that the sets of local optimum for fC and f1 should intersect with one another to\n",
      "guarantee a near-optimal weight pool is attainable. For the first time, we empirically propose a joint\n",
      "training algorithm which solves the learning objective above to construct a near-optimal weight pool. Each secret model assembled from the optimized weight pool exhibits similar utility compared with\n",
      "an identical model which is independently trained (§4.2). Searching for the Optimized Weight Pool. To solve the joint learning objective, our proposed attack\n",
      "executes the following training iteration recurrently. Denote the weight pool at the t-th iteration as\n",
      "Pt. Concisely, at the t-th iteration, we iterate over all the M secret tasks and the normal task to\n",
      "conduct the following key procedures: (i) For the k-th secret tasks, we first invoke Fill(Pt, fk, vk)\n",
      "to instantiate fk with the current values of the weight pool. (ii) Then, we forward a training batch\n",
      "via the model fk(·; Pt(θk, vk)), back-propagate the loss Lk approximated on the training batch, and\n",
      "5\n",
      "\fconduct one optimization step on the parameters of fk with an optimizer (e.g., Adam [21]). (iii)\n",
      "Finally, we collect the weight update on each parameter and follow the mapping relation in P(·, vk)\n",
      "to propagate the update to the corresponding weight pool parameter. The above procedures are also\n",
      "conducted on the carrier model. The above procedures describe the Propagate primitive on each secret/open task (L4-15 in Algorithm\n",
      "A.2). The final step in one training iteration is to invoke the Update primitive on the weight pool\n",
      "(L16-18 in Algorithm A.2). Technically, for each parameter in Pt, we maintain a buffer to store the\n",
      "weight updates from each task. The update buffer is aggregated to obtain the global update value\n",
      "on the corresponding scalar parameter in Pt. In our experiments, we find aggregation by average is\n",
      "already sufficient to achieve effective attacks. After the parameter is updated, we clear the update\n",
      "buffers and resume the next training iteration. To wind up the hiding phase, the attacker clears up any traces of malicious training code and irrelevant\n",
      "intermediate outcomes, memorizes the secret keys (i.e., the random seeds {sk} for generating noises,\n",
      "the starting indices {vk} and the architecture name for each task, the size of each group in the weight\n",
      "pool) via a secure medium, and instantiates the carrier model by Fill(P ∗, fC, vC). We denote the\n",
      "final weight pool as P ∗. 3.4\n",
      "Decoding the Secrets from the Carrier Model\n",
      "Recovering the Weight Pool. After the carrier model is published online with open access, the\n",
      "attacker immediately notifies the outside colluder to download the carrier model and decode the\n",
      "secret models from the carrier model. Specifically, after colluding on the secret keys with the attacker\n",
      "via a secure channel (e.g., an in-person rendezvous), the outsider first decodes the weight pool based\n",
      "on the colluded knowledge of the weight pool sizes and the starting index vC, i.e., by the primitive\n",
      "Decode(C, vC). Specifically, the colluder first collects the parameters of different groups from the\n",
      "carrier model. Then, the attacker slices, e.g., the weight parameters into segments of length |Pw|. The\n",
      "last segment may need additional zero padding to hold the same length. Finally, the attacker conducts\n",
      "a fusion operation on the N decoded segments, right-shift the fusion result by vk mod |Pw|, and\n",
      "permute it with a permutation inverse to the one in Fill to recover the final Pw in the weight pool. Similar operations are conducted on the bias and the scale groups. We introduce the fusion mechanism\n",
      "on the N decoded segments to implement resilience against post-processing on the carrier model\n",
      "(§4.3). For example, when the colluder finds the carrier model is pruned, the fusion mechanism\n",
      "selects the non-zero value from each weight pool copy to restore the pruned values. More details on\n",
      "the decoding procedure are provided in Algorithm A.3 of in the supplementary material. Assembling the Secrets. Finally, the colluder recovers the secret models f1, f2, . . . , fM by invoking\n",
      "the Fill primitive with the decoded weight pool in the previous part. For the private dataset on which\n",
      "the adversarial purpose is functionality stealing, the attack objective is attained. Otherwise, for the\n",
      "k-th target dataset, the attacker uses the securely communicated knowledge of the fixed distribution\n",
      "and the integer random seed to replicate the set of random noise vectors z1, . . . , zN. Finally, he/she\n",
      "dumps each fk(zi) for approximately recovering the sensitive input xi. 4\n",
      "Evaluation Results\n",
      "Datasets and Scenarios. We evaluate the performance of Cans on three real-world public datasets\n",
      "covering the scenarios of object classification, face recognition and speech recognition. Table 1\n",
      "summarizes the scenarios and the statistics. Below, we concisely introduce the dataset information\n",
      "and the construction of the memorization-based secret tasks. Table 1: Datasets and scenarios. ( ↑/↓indicates the metric is the higher/lower the better)\n",
      "Dataset\n",
      "# of Samples\n",
      "Bytes per Sample\n",
      "Limits of Existing Schemes\n",
      "Reconstruction Metric\n",
      "SpeechCommand\n",
      "100,503\n",
      "62.5KB\n",
      "Nmax = 699\n",
      "Mean Square Error (MSE, ↓)\n",
      "FaceScrub\n",
      "107,818\n",
      "588KB\n",
      "Nmax = 76\n",
      "SSIM (↑)/MSE (↓)\n",
      "CIFAR-10\n",
      "60,000\n",
      "12KB\n",
      "Nmax = 3638\n",
      "SSIM [43] (↑)/MSE (↓)\n",
      "• CIFAR-10 [24]: This dataset contains 60, 000 images of daily objects (e.g., cat, trunk and ship). Each image is encoded in RGB and has the shape of 32 × 32. 6\n",
      "\f• FaceScrub [29]: This dataset contains 107, 818 face images of 530 male and female celebrities\n",
      "retrieved from the Internet. Each image is encoded in RGB and has the shape of 224 × 224. • SpeechCommand (i.e., Speech) [44]: This dataset contains 35 different voice commands spoken\n",
      "by multiple subjects, which is composed of over 100,000 audio files of 1 second length with a\n",
      "sampling frequency of 16kHz. Each audio is encoded in a one-dimensional matrix of 16, 000. In each secret task, we set the dimension of the pseudorandom noise vectors as 100 and the secret\n",
      "model as an off-the-shelf generator-like architecture which is detailed in the supplementary materials. Specification of the Carrier Model. We consider a standard ResNet-18 [19] as the carrier model,\n",
      "and the training on the CIFAR-10 [24] dataset as the open task. The total number of parameters in a\n",
      "ResNet-18 is about 11.7 million, 42.63MB in bytes. Baselines. We consider the following classical steganography schemes which are first adapted to the\n",
      "context of DNN by Song et al. [38]. • Least-Significant-Bits-based Hiding (LSB): With LSB, the attacker hides the secret information in\n",
      "the last K bits of the floating-point representation of each parameter in a DNN. Therefore, for a\n",
      "DNN carrier with L parameters, the maximal hiding capacity of the LSB scheme is LK/8 bytes. In the example of ResNet-18, if the attacker uses the last 16 bits (which incurs almost no ∆Perf),\n",
      "the ResNet-18 provides a capacity of about 38 samples from FaceScrub. This scheme provably\n",
      "incurs no distortion on the hidden secrets. • Sign-based Hiding (Sign): With Sign, the attacker hides the secret information in the sign bit of\n",
      "each parameter. Therefore, a DNN carrier with L parameters provides hiding capacity of L/8 bytes\n",
      "in this scheme. • Covariance-based Hiding (Covariance): With Covariance, the attacker hides the secret informa-\n",
      "tion by maximizing the Pearson covariance coefficients between the secret values as floating-point\n",
      "values and the model parameters. Therefore, a DNN carrier with L parameters provides hiding\n",
      "capacity of 4L bytes in this scheme. It is worth to note, the latter two schemes are learning-based\n",
      "and have no guarantee on the distortion which may be incurred on the hidden secrets. Remark 4.1. For fair comparisons, our experiments adopt the same encoding schemes for each type\n",
      "of datasets when invoking Cans and the baseline methods on DNN. Evaluation Metrics. We measure the effectiveness of neuron steganography schemes with the\n",
      "following metrics: (i) Reconstruction Error: This metric measures the pairwise difference between\n",
      "the stolen and ground-truth samples. For each data type, the specific metric for the reconstruction\n",
      "error is listed in Table 1. (ii) Performance Difference (∆Perf): ∆Perf measures the difference\n",
      "between the carrier model encoded with secret models and an independently trained carrier model. A lower ∆Perf means model hiding causes more performance overhead to the carrier model or the\n",
      "secret model, which implies the carrier model has lower capacity. Moreover, a lower carrier model\n",
      "∆Perf means model hiding is less covert. (iii) Hiding Capacity: As in [38], we measure the hiding\n",
      "capacity by the byte size of data samples which can be hidden in the carrier model without incurring\n",
      "nontrivial ∆Perf (e.g., < 1% on our specified carrier model) or perceptible reconstruction errors (e.g.,\n",
      "10−2 in MSE for images [31]). 4.1\n",
      "Effectiveness of Cans\n",
      "• Stealing a Single Dataset. First, we present evaluation results on stealing data samples from each\n",
      "single dataset with Cans and the baselines. Fig.3 reports the performance of Cans and the baselines\n",
      "when the number of stolen examples, i.e., N, increases from 8 and 1024 on Facescrub. As Fig.3(c)\n",
      "shows, Cans can steal over 1000 face images with an SSIM uniformly higher than 0.97 and with less\n",
      "than 1% accuracy loss on the carrier model. This substantially surpasses the performance of existing\n",
      "data hiding techniques. On the one hand, in the upper part of Fig.3(a), the corresponding result for\n",
      "Sign, Covariance and LSB is unable to be derived when N reaches 64, 128 and 128 respectively. It is\n",
      "mainly because, when the number of target inputs hits such a size, either the information capacity\n",
      "in the sign bit, the covariance, or the bits can no longer afford the required capacity for hiding all\n",
      "the images (i.e., about 147MB, 3× of a ResNet-18), which directly inhibits the baselines from being\n",
      "executed. In contrast, based on parameter sharing instead of directly modifying the parameter for data\n",
      "hiding, Cans has no hard upper limit on its hiding capacity and is more flexible to more general data\n",
      "7\n",
      "\fFigure 3: (a)(b) Comparison between our approach and the baselines in terms of SSIM and the\n",
      "accuracy of the carrier model on CIFAR-10 and FaceScrub. (c)-(e) Sampled results with different\n",
      "data stealing approaches on the three datasets. stealing scenarios. On the other hand, our proposed attack achieves the optimal SSIM (with the MSE\n",
      "of reconstruction constantly smaller than 10−5) compared with both the Sign and the Covariance\n",
      "encoding. Although the SSIM of LSB remains 1, it drastically hurts the performance of the carrier\n",
      "model when the number of inputs reaches 64. In this case, the LSB encoding has to totally modify the\n",
      "last 24 bits of all the FP32 parameters in ResNet-18 to afford the required capacity, which degrades\n",
      "the carrier model to a totally random model. In comparison, the carrier model in Cans preserves most\n",
      "of its utility. Table 2 additionally report the effectiveness of Cans in stealing audio data. Table 2: Effectiveness of Cans in\n",
      "stealing audio data from Speech. # Stolen Samples\n",
      "MSE\n",
      "∆Perf\n",
      "512\n",
      "2.7 × 10−4\n",
      "−0.43%\n",
      "1024\n",
      "3.5 × 10−4\n",
      "−0.19%\n",
      "2048\n",
      "4.2 × 10−4\n",
      "−0.41%\n",
      "4096\n",
      "4.8 × 10−4\n",
      "−0.35%\n",
      "Table 3: Effectiveness of Cans in stealing multiple heteroge-\n",
      "neous datasets simultaneously. # Stolen Samples\n",
      "Size\n",
      "Distortion\n",
      "∆Perf\n",
      "CIFAR-10\n",
      "4096\n",
      "48MB\n",
      "0.856 (SSIM)\n",
      "−0.84%\n",
      "FaceScrub\n",
      "128\n",
      "73.5MB\n",
      "0.992 (SSIM)\n",
      "Speech\n",
      "1024\n",
      "62.5MB\n",
      "4.1 × 10−4 (MSE)\n",
      "In Total\n",
      "5248\n",
      "184MB = 4.31× Size of ResNet-18\n",
      "• Stealing Heterogeneous Datasets. Next, we evaluate the scenario of stealing secret samples from\n",
      "multiple heterogeneous datasets. Specifically, we construct memorization-oriented secret models for\n",
      "the three datasets and hide the secret models simultaneously in the ResNet-18 carrier model with the\n",
      "aid of Cans. In our experiments, the size of stolen samples from each single dataset already exceed\n",
      "the size of the carrier model, which inhibits almost all existing approaches from working in this\n",
      "scenario. In contrast, Table 3 show the average distortion with Cans on each single dataset remains\n",
      "very close to the cases when stealing each dataset alone, while the decrease in the performance of the\n",
      "carrier model is controlled under 1%. For example, on Speech, the average MSE of the recovered\n",
      "1024 audio segments is 4.1 × 10−4 in a 7 × 10−5 margin of the performance when stealing the\n",
      "Speech data alone. • Visualization of Stolen Samples. For better intuition, we also qualitatively compare the results\n",
      "of our attack with Covariance and Sign encoding in Fig.3(c)-(e). For example, as is shown, our\n",
      "reconstructed images and audios are almost perceptually indistinguishable with the ground-truth ones,\n",
      "which conforms to our quantitative results in SSIM and MSE. 4.2\n",
      "Exploring the Capacity Limits of Cans\n",
      "Next, we explore the capacity limits of Cans when the number of stolen examples N increases on\n",
      "the FaceScrub dataset. As Fig.4(a) shows, there is almost no distortion on the decoded images (i.e.,\n",
      "SSIM remains close to 1.0) when N increase from 8 to 1024, while the ∆Perf of the carrier model\n",
      "is controlled below 1%. As the size of the stolen images further doubles from 1024 to 32768 (i.e.,\n",
      "440× of the size of the carrier model), we notice the SSIM gradually decreases from near 1.0 to\n",
      "about 0.6, during which the quality of the decoded images does not show clear deterioration.\n"
     ]
    }
   ],
   "source": [
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_embeddings = client.embeddings.create(input=\"CRoSS: Diffusion Model\", model='text-embedding-3-small').data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['distances'] = df['embedding'].apply(lambda x: cosine_similarity(x, q_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted = df.sort_values('distances', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>embedding</th>\n",
       "      <th>distances</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>W$2\u00131\\n\u001fCË\\n)CJ\\nYK|\\n7\u0003u\\n¢\u0018¶F¡\\n\u001fU 6Ó\\n...</td>\n",
       "      <td>7645.0</td>\n",
       "      <td>[0.008818204514682293, -0.009038213640451431, ...</td>\n",
       "      <td>0.078187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22493</th>\n",
       "      <td>HAN\\nJoMoLD\\nVALOR\\nVALOR++\\nMethods\\n0\\n10\\n2...</td>\n",
       "      <td>7977.0</td>\n",
       "      <td>[0.006200759205967188, -0.01920808106660843, -...</td>\n",
       "      <td>0.078344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>\u0012\\nß\\né\\nã\\nð\\n\u001d\\nï\\n\u0012\\nä\\nã\\nð\\ná\\n\u0003\\nß\\nü\\nß...</td>\n",
       "      <td>6141.0</td>\n",
       "      <td>[0.020025920122861862, 0.008722320199012756, -...</td>\n",
       "      <td>0.078858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20837</th>\n",
       "      <td>REALTIME QA: What’s the Answer Right Now?\\nJun...</td>\n",
       "      <td>7920.0</td>\n",
       "      <td>[0.0003461475716903806, 0.02710149809718132, 0...</td>\n",
       "      <td>0.079549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20168</th>\n",
       "      <td>Makes 4 servings (about 1 ½ cups each) Recipe ...</td>\n",
       "      <td>7857.0</td>\n",
       "      <td>[0.019085267558693886, 0.018926342949271202, -...</td>\n",
       "      <td>0.080477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  n_tokens  \\\n",
       "1154   W$2\u00131\\n\u001fCË\\n)CJ\\nYK|\\n7\u0003u\\n¢\u0018¶F¡\\n\u001fU 6Ó\\n...    7645.0   \n",
       "22493  HAN\\nJoMoLD\\nVALOR\\nVALOR++\\nMethods\\n0\\n10\\n2...    7977.0   \n",
       "940    \u0012\\nß\\né\\nã\\nð\\n\n",
       "\\nï\\n\u0012\\nä\\nã\\nð\\ná\\n\u0003\\nß\\nü\\nß...    6141.0   \n",
       "20837  REALTIME QA: What’s the Answer Right Now?\\nJun...    7920.0   \n",
       "20168  Makes 4 servings (about 1 ½ cups each) Recipe ...    7857.0   \n",
       "\n",
       "                                               embedding  distances  \n",
       "1154   [0.008818204514682293, -0.009038213640451431, ...   0.078187  \n",
       "22493  [0.006200759205967188, -0.01920808106660843, -...   0.078344  \n",
       "940    [0.020025920122861862, 0.008722320199012756, -...   0.078858  \n",
       "20837  [0.0003461475716903806, 0.02710149809718132, 0...   0.079549  \n",
       "20168  [0.019085267558693886, 0.018926342949271202, -...   0.080477  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    question=\"Can you summarize the main findings of 'CRoSS: Diffusion Model Makes Controllable, Robust and Secure Image Steganography'?\",\n",
    "    max_len=8000,\n",
    "    debug=False,\n",
    "    max_tokens=150,\n",
    "    stop_sequence=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Answer a question based on the most similar context from the dataframe texts\n",
    "    \"\"\"\n",
    "    context = create_context(\n",
    "        question,\n",
    "        max_len=max_len\n",
    "    )\n",
    "    # If debug, print the raw model response\n",
    "    if debug:\n",
    "        print(\"Context:\\n\" + context)\n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "    try:\n",
    "        # Create a chat completion using the question and context\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Answer the question based on the context below, and if the question can't be answered based on the context, say \\\"I don't know\\\"\\n\\n\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Question: {question}\\n\\n---\\n\\nContext: {context}\"}\n",
    "            ]\n",
    "            # temperature=0,\n",
    "            # max_tokens=max_tokens,\n",
    "            # top_p=1,\n",
    "            # frequency_penalty=0,\n",
    "            # presence_penalty=0,\n",
    "            # stop=stop_sequence,\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "answer = answer_question()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main findings of the paper \"CRoSS: Diffusion Model Makes Controllable, Robust and Secure Image Steganography\" are as follows:\n",
      "\n",
      "1. **Introduction of CRoSS Framework**: The authors propose a novel image steganography framework called CRoSS, which utilizes diffusion models to enhance controllability, robustness, and security without requiring additional training.\n",
      "\n",
      "2. **Limitations of Existing Methods**: Current image steganography techniques often face issues such as information leakage, poor robustness against degradation, and limited controllability. These issues motivate the need for new approaches.\n",
      "\n",
      "3. **Advantages of Diffusion Models**: The framework leverages two key properties of diffusion models: their ability to translate images without training and their robustness against noise. This makes diffusion models suitable for steganography tasks.\n",
      "\n",
      "4. **Unification of Goals**: CRoSS aims to achieve security (ensuring the hidden image cannot be leaked), controllability (allowing the user to control the generated container image), and robustness (maintaining fidelity during degradation).\n",
      "\n",
      "5. **Empirical Validation**: Extensive experiments demonstrate that CRoSS significantly outperforms existing cover-based and coverless steganography methods in terms of security, controllability, and robustness. \n",
      "\n",
      "6. **Use of Pre-trained Models**: The framework exploits existing tools from the Stable Diffusion community, such as LoRAs and ControlNets, which enhance the framework’s functionalities.\n",
      "\n",
      "7. **Performance Metrics**: The experimental results show that CRoSS provides high quality for the revealed images, maintains a low detection rate against steganalysis methods, and exhibits superior performance under various degradations compared to other methods.\n",
      "\n",
      "Overall, CRoSS represents a significant advancement in image steganography by effectively combining the strengths of diffusion models to address long-standing limitations in the field.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "academic-paper-explorer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
